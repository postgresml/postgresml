{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1 { font-size: 3em; font-weight: 700; margin-bottom: -1rem; max-width: 80em; } .md-typeset p.subtitle { font-weight: 100; margin: 2em; max-width: 80em; } .md-typeset img { margin: 0; border-radius: 10px; } .md-grid { max-width: 100em; } .md-content video, .md-content img { max-width: 90%; margin: 2em 5%; } article.md-content__inner.md-typeset a.md-content__button.md-icon { display: none; } h1.title { font-size: 2rem; } Microsecond inference with the most capable feature store Easily train and deploy online models using only SQL, with an open source extension for PostgreSQL. Try PostgresML Free Docs Pure SQL Solution \u00b6 SELECT pgml . train ( 'My First PostgresML Project' , task => 'regression' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' ); Learn more about Training SELECT pgml . deploy ( 'My First PostgresML Project' , strategy => 'best_score' , algorithm => 'xgboost' ); Learn more about Deployments SELECT target , pgml . predict ( 'My First PostgresML Project' , image ) AS prediction FROM pgml . digits ORDER BY prediction DESC ; Learn more about Predictions What's in the box \u00b6 All your favorite algorithms Whether you need a simple linear regression, or extreme gradient boosting, we've included support for all classification and regression algorithms in Scikit Learn , XGBoost , LightGBM and pre-trained deep learning models from Hugging Face . Algorithms Hyperparameter search Use either grid or random searches with cross validation on your training set to discover the most important knobs to tweak on your favorite algorithm. Hyperparameter Search Blazing fast With core implementation and bindings written in Rust, use XGBoost, LightGBM and Linfa algorithms at blazing speed with minimal memory utilization and no garbage collection. Benchmarks Online and offline support Predictions are served via a standard Postgres connection to ensure that your core apps can always access both your data and your models in real time. Pure SQL workflows also enable batch predictions to cache results in native Postgres tables for lookup. Predictions Fast vector operations Vector operations make working with learned emebeddings a snap, for things like nearest neighbor searches or other similarity comparisons. Rust and BLAS optimized for maximum performance. Vector Operations Managed model deployments Models can be periodically retrained and automatically promoted to production depending on their key metric. Rollback capability is provided to ensure that you're always able to serve the highest quality predictions, along with historical logs of all deployments for long term study. Deployments The performance of Postgres Since your data never leaves the database, you retain the speed, reliability and security you expect in your foundational stateful services. Leverage your existing infrastructure and the data distribution strategies native to PostgreSQL to deliver new capabilities. Distributed Training Instant visualizations Run standard analysis on your datasets to detect outliers, bimodal distributions, feature correlation, and other common data visualizations on your datasets. Everything is cataloged in the dashboard for easy reference. Dashboard Open source We're building on the shoulders of giants. These machine learning libraries and Postgres have received extensive academic and industry use, and we'll continue their tradition to build with the community. MIT License Try PostgresML Free","title":"PostgresML"},{"location":"#pure-sql-solution","text":"SELECT pgml . train ( 'My First PostgresML Project' , task => 'regression' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' ); Learn more about Training SELECT pgml . deploy ( 'My First PostgresML Project' , strategy => 'best_score' , algorithm => 'xgboost' ); Learn more about Deployments SELECT target , pgml . predict ( 'My First PostgresML Project' , image ) AS prediction FROM pgml . digits ORDER BY prediction DESC ; Learn more about Predictions","title":"Pure SQL Solution"},{"location":"#whats-in-the-box","text":"All your favorite algorithms Whether you need a simple linear regression, or extreme gradient boosting, we've included support for all classification and regression algorithms in Scikit Learn , XGBoost , LightGBM and pre-trained deep learning models from Hugging Face . Algorithms Hyperparameter search Use either grid or random searches with cross validation on your training set to discover the most important knobs to tweak on your favorite algorithm. Hyperparameter Search Blazing fast With core implementation and bindings written in Rust, use XGBoost, LightGBM and Linfa algorithms at blazing speed with minimal memory utilization and no garbage collection. Benchmarks Online and offline support Predictions are served via a standard Postgres connection to ensure that your core apps can always access both your data and your models in real time. Pure SQL workflows also enable batch predictions to cache results in native Postgres tables for lookup. Predictions Fast vector operations Vector operations make working with learned emebeddings a snap, for things like nearest neighbor searches or other similarity comparisons. Rust and BLAS optimized for maximum performance. Vector Operations Managed model deployments Models can be periodically retrained and automatically promoted to production depending on their key metric. Rollback capability is provided to ensure that you're always able to serve the highest quality predictions, along with historical logs of all deployments for long term study. Deployments The performance of Postgres Since your data never leaves the database, you retain the speed, reliability and security you expect in your foundational stateful services. Leverage your existing infrastructure and the data distribution strategies native to PostgreSQL to deliver new capabilities. Distributed Training Instant visualizations Run standard analysis on your datasets to detect outliers, bimodal distributions, feature correlation, and other common data visualizations on your datasets. Everything is cataloged in the dashboard for easy reference. Dashboard Open source We're building on the shoulders of giants. These machine learning libraries and Postgres have received extensive academic and industry use, and we'll continue their tradition to build with the community. MIT License Try PostgresML Free","title":"What's in the box"},{"location":"about/faq/","text":"FAQ \u00b6 How far can this scale? Petabyte-sized Postgres deployments are documented in production since at least 2008, and recent patches have enabled working beyond exabyte and up to the yotabyte scale. Machine learning models can be horizontally scaled using standard Postgres replicas. How reliable can this be? Postgres is widely considered mission critical, and some of the most reliable technology in any modern stack. PostgresML allows an infrastructure organization to leverage pre-existing best practices to deploy machine learning into production with less risk and effort than other systems. For example, model backup and recovery happens automatically alongside normal Postgres data backup. How good are the models? Model quality is often a trade-off between compute resources and incremental quality improvements. Sometimes a few thousands training examples and an off the shelf algorithm can deliver significant business value after a few seconds of training. PostgresML allows stakeholders to choose several different algorithms to get the most bang for the buck, or invest in more computationally intensive techniques as necessary. In addition, PostgresML can automatically apply best practices for data cleaning ) like imputing missing values by default and normalizing features to prevent common problems in production. PostgresML doesn't help with reformulating a business problem into a machine learning problem. Like most things in life, the ultimate in quality will be a concerted effort of experts working over time. PostgresML is intended to establish successful patterns for those experts to collaborate around while leveraging the expertise of open source and research communities. Is PostgresML fast? Collocating the compute with the data inside the database removes one of the most common latency bottlenecks in the ML stack, which is the (de)serialization of data between stores and services across the wire. Modern versions of Postgres also support automatic query parrellization across multiple workers to further minimize latency in large batch workloads. Finally, PostgresML will utilize GPU compute if both the algorithm and hardware support it, although it is currently rare in practice for production databases to have GPUs. We're working on benchmarks .","title":"FAQ"},{"location":"about/faq/#faq","text":"How far can this scale? Petabyte-sized Postgres deployments are documented in production since at least 2008, and recent patches have enabled working beyond exabyte and up to the yotabyte scale. Machine learning models can be horizontally scaled using standard Postgres replicas. How reliable can this be? Postgres is widely considered mission critical, and some of the most reliable technology in any modern stack. PostgresML allows an infrastructure organization to leverage pre-existing best practices to deploy machine learning into production with less risk and effort than other systems. For example, model backup and recovery happens automatically alongside normal Postgres data backup. How good are the models? Model quality is often a trade-off between compute resources and incremental quality improvements. Sometimes a few thousands training examples and an off the shelf algorithm can deliver significant business value after a few seconds of training. PostgresML allows stakeholders to choose several different algorithms to get the most bang for the buck, or invest in more computationally intensive techniques as necessary. In addition, PostgresML can automatically apply best practices for data cleaning ) like imputing missing values by default and normalizing features to prevent common problems in production. PostgresML doesn't help with reformulating a business problem into a machine learning problem. Like most things in life, the ultimate in quality will be a concerted effort of experts working over time. PostgresML is intended to establish successful patterns for those experts to collaborate around while leveraging the expertise of open source and research communities. Is PostgresML fast? Collocating the compute with the data inside the database removes one of the most common latency bottlenecks in the ML stack, which is the (de)serialization of data between stores and services across the wire. Modern versions of Postgres also support automatic query parrellization across multiple workers to further minimize latency in large batch workloads. Finally, PostgresML will utilize GPU compute if both the algorithm and hardware support it, although it is currently rare in practice for production databases to have GPUs. We're working on benchmarks .","title":"FAQ"},{"location":"about/license/","text":"Copyright \u00a9 2022 PostgresML Team Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/motivation/","text":"Motivation \u00b6 Deploying machine learning models into existing applications is not straight forward. It involves operating new services, which need to be written in specialized languages with libraries outside of the experience of many software engineers. Those services tend to be architected around specialized datastores and hardware that requires additional management and know how. Data access needs to be secure across production and development environments without impeding productivity. This complexity pushes risks and costs beyond acceptable trade off limits for many otherwise valuable use cases. PostgresML makes ML simple by moving the code to your data, rather than copying the data all over the place. You train models using simple SQL commands, and you get the predictions in your apps via a mechanism you're already using: a query over a standard Postgres connection. Our goal is that anyone with a basic understanding of SQL should be able to build, deploy and maintain machine learning models in production, while receiving the benefits of a high performance machine learning platform. Ultimately, PostgresML aims to be the easiest, safest and fastest way to gain value from machine learning.","title":"Motivation"},{"location":"about/motivation/#motivation","text":"Deploying machine learning models into existing applications is not straight forward. It involves operating new services, which need to be written in specialized languages with libraries outside of the experience of many software engineers. Those services tend to be architected around specialized datastores and hardware that requires additional management and know how. Data access needs to be secure across production and development environments without impeding productivity. This complexity pushes risks and costs beyond acceptable trade off limits for many otherwise valuable use cases. PostgresML makes ML simple by moving the code to your data, rather than copying the data all over the place. You train models using simple SQL commands, and you get the predictions in your apps via a mechanism you're already using: a query over a standard Postgres connection. Our goal is that anyone with a basic understanding of SQL should be able to build, deploy and maintain machine learning models in production, while receiving the benefits of a high performance machine learning platform. Ultimately, PostgresML aims to be the easiest, safest and fastest way to gain value from machine learning.","title":"Motivation"},{"location":"about/roadmap/","text":"Road map \u00b6 This project is currently a proof of concept. Some important features, which we are currently thinking about or working on, are listed below. Production deployment \u00b6 Many companies that use PostgreSQL in production do so using managed services like AWS RDS, Digital Ocean, Azure, etc. Those services do not allow running custom extensions, so we have to run PostgresML directly on VMs, e.g. EC2, droplets, etc. The idea here is to replicate production data directly from Postgres and make it available in real-time to PostgresML. We're considering solutions like logical replication for small to mid-size databases, and Debezium for multi-TB deployments. Model management dashboard \u00b6 A good looking and useful UI goes a long way. A dashboard similar to existing solutions like MLFlow or AWS SageMaker will make the experience of working with PostgresML as pleasant as possible. Data explorer \u00b6 A data explorer allows anyone to browse the dataset in production and to find useful tables and features to build effective machine learning models. More algorithms \u00b6 Scikit-Learn is a good start, but we're also thinking about including Tensorflow, Pytorch, and many more useful models. Scheduled training \u00b6 In applications where data changes often, it's useful to retrain the models automatically on a schedule, e.g. every day, every week, etc.","title":"Roadmap"},{"location":"about/roadmap/#road-map","text":"This project is currently a proof of concept. Some important features, which we are currently thinking about or working on, are listed below.","title":"Road map"},{"location":"about/roadmap/#production-deployment","text":"Many companies that use PostgreSQL in production do so using managed services like AWS RDS, Digital Ocean, Azure, etc. Those services do not allow running custom extensions, so we have to run PostgresML directly on VMs, e.g. EC2, droplets, etc. The idea here is to replicate production data directly from Postgres and make it available in real-time to PostgresML. We're considering solutions like logical replication for small to mid-size databases, and Debezium for multi-TB deployments.","title":"Production deployment"},{"location":"about/roadmap/#model-management-dashboard","text":"A good looking and useful UI goes a long way. A dashboard similar to existing solutions like MLFlow or AWS SageMaker will make the experience of working with PostgresML as pleasant as possible.","title":"Model management dashboard"},{"location":"about/roadmap/#data-explorer","text":"A data explorer allows anyone to browse the dataset in production and to find useful tables and features to build effective machine learning models.","title":"Data explorer"},{"location":"about/roadmap/#more-algorithms","text":"Scikit-Learn is a good start, but we're also thinking about including Tensorflow, Pytorch, and many more useful models.","title":"More algorithms"},{"location":"about/roadmap/#scheduled-training","text":"In applications where data changes often, it's useful to retrain the models automatically on a schedule, e.g. every day, every week, etc.","title":"Scheduled training"},{"location":"about/team/","text":"ul.team img { border-radius: 50%; } Team \u00b6 Montana Low montanalow Lev Kokotov levkk Jason Dusek solidsnack Santi Adavani santiatpml Daniel Illenberger chillenberger","title":"Team"},{"location":"about/team/#team","text":"","title":"Team"},{"location":"blog/architecture/","text":"","title":"Architecture"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/","text":"Backwards Compatible or Bust: Python Inside Rust Inside Postgres \u00b6 Lev Kokotov October 3, 2022 Some of you may remember the day Python 3 was released. The changes seemed sublte, but they were enough to create chaos: most projects and tools out there written in Python 2 would no longer work under Python 3. The next decade was spent migrating mission-critical infrastructure from print to print() and from str to bytes . Some just gave up and stayed on Python 2. Breaking backwards compatibility to make progress could be good but Python's move was risky. It endured because we loved it more than we disagreed with that change. Most projects won't have that luxury, especially if you're just starting out. For us at PostgresML, backwards compatibility is as important as progress. PostgresML 2.0 is coming out soon and we're rewritten everything in Rust for a 35x performance improvement . The previous version was written in Python, the de facto machine learning environment with the most libraries. Now that we were using Linfa and SmartCore, we could have theoretically went ahead without Python, but we weren't quite ready to let go of all the functionality provided by the Python ecosystem, and I'm sure many of our users weren't either. So what could we do to preserve features, backwards compatibility, and our users' trust? PyO3 to the rescue. Python in Rust \u00b6 PyO3 was written to build Python extensions in Rust. Native extensions are much faster than Python modules so, when speed matters, most things were written in Cython or C. If you've ever tried that, you know the experience isn't very user-friendly or forgiving. Rust, on the other hand, is fast and memory-safe, with compiler hints getting awfully specific (my co-founder thinks it may be becoming a singularity). PyO3 comes with another very important feature: it allows running Python code from inside a Rust program. Sounds too good to be true? We didn't think so at the time. PL/Python has been doing that for years and that's what we used initially to write PostgresML. The path to running Scikit inside Rust seemed clear. The Roadmap \u00b6 Making a massive Python library work under a completely different environment isn't an obvious thing to do. If you dive into Scikit's source code, you would find Python, Cython, C extensions and SciPy. We were going to add that into a shared library which linked into Postgres and implemented its own machine learning algorithms. In order to get this done, we split the work into two distinct steps: Train a model in Rust using Scikit Test for regressions using our 1.0 test suite Hello Python, I am Rust \u00b6 First thing we needed to do was to make sure Scikit can even run under PyO3. So we wrote a small wrapper around all the algorithms we implemented in 1.0 and called it from inside our Rust source code. The wrapper was just 200 lines of code most of which was mapping algorithm names to Scikit's Python classes. Using the wrapper was surprisingly easy: use pyo3 :: prelude :: * ; use pyo3 :: types :: PyTuple ; pub fn sklearn_train () { // Copy the code into the Rust library at build time. let module = include_str! ( concat! ( env! ( \"CARGO_MANIFEST_DIR\" ), \"/src/bindings/sklearn.py\" )); let estimator = Python :: with_gil ( | py | -> Py < PyAny > { // Compile Python let module = PyModule :: from_code ( py , module , \"\" , \"\" ). unwrap (); // ... train the model }); } Our Python code was compiled and ready to go. We trained a model with data coming from Rust arrays, passed into Python using PyO3 automatic conversions, and got back a trained Scikit model. It felt magical. Did it Work? \u00b6 Since we have dozens of ML algorithms in 1.0, we had a pretty decent test suite to make sure all of them worked. My local dev is an Ubuntu 22.04 gaming rig (I still dual-boot though), so I had no issues running the test suite, training all Scikit algorithms on the toy datasets, and getting predictions back in a good amount of time. Drunk on my success, I called the job done, merged the PR, and moved on. Then Montana decided to try my work on his slightly older gaming rig, but instead of getting a trained model, he got this: server closed the connection unexpectedly This probably means the server terminated abnormally before or while processing the request. and after checking the logs, he found an even scarier message: LOG: server process (PID 11352) was terminated by signal 11: Segmentation fault A segmentation fault in Rust? That's supposed to be impossible, but here it was. A segmentation fault happens when a program attempts to read parts of memory that don't exist, either because they were freed, or were never allocated in the first place. That doesn't happen in Rust under normal conditions, but we knew our project was far from normal. More confusingly, the error was coming from inside Scikit. It would have made sense if it was XGBoost or LightGBM, which we wrapped with a bunch of Rust unsafe blocks, but the error was coming from a universally used Python library. Debugging Ten Layers Down \u00b6 Debugging segmentation faults inside compiled executables is hard. Debugging segmentation faults inside shared libraries inside FFI wrappers inside a machine learning library running inside a database... is harder. We've had very few clues: it worked on my Ubuntu 22.04 but didn't on Montana's Ubuntu 20.04. I dual-booted 20.04 to check it out and, surprise, it segfaulted for me too. At this point I was convinced something was terribly wrong and called the \"universal debugger\" to the rescue: I littered Scikit's code with raise Exception(\"I'm here\") to see where it was going and, more importantly, where it couldn't make it because of the segfault. After a few hours, I was inside SciPy, over 10 function calls deep from our wrapper. SciPy implements many useful scientific computing subroutines and one of them happens to solve linear regressions, a very popular machine learning algorithm. SciPy doesn't do it alone but calls out to a BLAS subroutine written to crunch numbers as fast as possible, and that's where I found the segfault. It clicked. Scikit uses SciPy, SciPy uses C-BLAS and we used OpenBLAS for ndarray and our own vector functions, and everything is dynamically linked together at compile time. So which BLAS is SciPy using? It couldn't find the BLAS function it needed and crashed. Static Link or Bust \u00b6 The fix was surprisingly simple: statically link OpenBLAS using the Cargo build script: build.rs fn main () { println! ( \"cargo:rustc-link-lib=static=openblas\" ); } The linker included the code for OpenBLAS into our extension, SciPy was able to find the function it was looking for, and PostgresML 2.0 was working again. Recap \u00b6 In the end, we got what we wanted: Rust machine learning in Postgres was on track Scikit-learn was coming along into PostgresML 2.0 Backwards compatibility with PostgresML 1.0 was preserved and we had a lot of fun working with PyO3 and pushing the limits of what we thought was possible. Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our GitHub .","title":"Backwards Compatible or Bust: Python Inside Rust Inside Postgres"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#backwards-compatible-or-bust-python-inside-rust-inside-postgres","text":"Lev Kokotov October 3, 2022 Some of you may remember the day Python 3 was released. The changes seemed sublte, but they were enough to create chaos: most projects and tools out there written in Python 2 would no longer work under Python 3. The next decade was spent migrating mission-critical infrastructure from print to print() and from str to bytes . Some just gave up and stayed on Python 2. Breaking backwards compatibility to make progress could be good but Python's move was risky. It endured because we loved it more than we disagreed with that change. Most projects won't have that luxury, especially if you're just starting out. For us at PostgresML, backwards compatibility is as important as progress. PostgresML 2.0 is coming out soon and we're rewritten everything in Rust for a 35x performance improvement . The previous version was written in Python, the de facto machine learning environment with the most libraries. Now that we were using Linfa and SmartCore, we could have theoretically went ahead without Python, but we weren't quite ready to let go of all the functionality provided by the Python ecosystem, and I'm sure many of our users weren't either. So what could we do to preserve features, backwards compatibility, and our users' trust? PyO3 to the rescue.","title":"Backwards Compatible or Bust: Python Inside Rust Inside Postgres"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#python-in-rust","text":"PyO3 was written to build Python extensions in Rust. Native extensions are much faster than Python modules so, when speed matters, most things were written in Cython or C. If you've ever tried that, you know the experience isn't very user-friendly or forgiving. Rust, on the other hand, is fast and memory-safe, with compiler hints getting awfully specific (my co-founder thinks it may be becoming a singularity). PyO3 comes with another very important feature: it allows running Python code from inside a Rust program. Sounds too good to be true? We didn't think so at the time. PL/Python has been doing that for years and that's what we used initially to write PostgresML. The path to running Scikit inside Rust seemed clear.","title":"Python in Rust"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#the-roadmap","text":"Making a massive Python library work under a completely different environment isn't an obvious thing to do. If you dive into Scikit's source code, you would find Python, Cython, C extensions and SciPy. We were going to add that into a shared library which linked into Postgres and implemented its own machine learning algorithms. In order to get this done, we split the work into two distinct steps: Train a model in Rust using Scikit Test for regressions using our 1.0 test suite","title":"The Roadmap"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#hello-python-i-am-rust","text":"First thing we needed to do was to make sure Scikit can even run under PyO3. So we wrote a small wrapper around all the algorithms we implemented in 1.0 and called it from inside our Rust source code. The wrapper was just 200 lines of code most of which was mapping algorithm names to Scikit's Python classes. Using the wrapper was surprisingly easy: use pyo3 :: prelude :: * ; use pyo3 :: types :: PyTuple ; pub fn sklearn_train () { // Copy the code into the Rust library at build time. let module = include_str! ( concat! ( env! ( \"CARGO_MANIFEST_DIR\" ), \"/src/bindings/sklearn.py\" )); let estimator = Python :: with_gil ( | py | -> Py < PyAny > { // Compile Python let module = PyModule :: from_code ( py , module , \"\" , \"\" ). unwrap (); // ... train the model }); } Our Python code was compiled and ready to go. We trained a model with data coming from Rust arrays, passed into Python using PyO3 automatic conversions, and got back a trained Scikit model. It felt magical.","title":"Hello Python, I am Rust"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#did-it-work","text":"Since we have dozens of ML algorithms in 1.0, we had a pretty decent test suite to make sure all of them worked. My local dev is an Ubuntu 22.04 gaming rig (I still dual-boot though), so I had no issues running the test suite, training all Scikit algorithms on the toy datasets, and getting predictions back in a good amount of time. Drunk on my success, I called the job done, merged the PR, and moved on. Then Montana decided to try my work on his slightly older gaming rig, but instead of getting a trained model, he got this: server closed the connection unexpectedly This probably means the server terminated abnormally before or while processing the request. and after checking the logs, he found an even scarier message: LOG: server process (PID 11352) was terminated by signal 11: Segmentation fault A segmentation fault in Rust? That's supposed to be impossible, but here it was. A segmentation fault happens when a program attempts to read parts of memory that don't exist, either because they were freed, or were never allocated in the first place. That doesn't happen in Rust under normal conditions, but we knew our project was far from normal. More confusingly, the error was coming from inside Scikit. It would have made sense if it was XGBoost or LightGBM, which we wrapped with a bunch of Rust unsafe blocks, but the error was coming from a universally used Python library.","title":"Did it Work?"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#debugging-ten-layers-down","text":"Debugging segmentation faults inside compiled executables is hard. Debugging segmentation faults inside shared libraries inside FFI wrappers inside a machine learning library running inside a database... is harder. We've had very few clues: it worked on my Ubuntu 22.04 but didn't on Montana's Ubuntu 20.04. I dual-booted 20.04 to check it out and, surprise, it segfaulted for me too. At this point I was convinced something was terribly wrong and called the \"universal debugger\" to the rescue: I littered Scikit's code with raise Exception(\"I'm here\") to see where it was going and, more importantly, where it couldn't make it because of the segfault. After a few hours, I was inside SciPy, over 10 function calls deep from our wrapper. SciPy implements many useful scientific computing subroutines and one of them happens to solve linear regressions, a very popular machine learning algorithm. SciPy doesn't do it alone but calls out to a BLAS subroutine written to crunch numbers as fast as possible, and that's where I found the segfault. It clicked. Scikit uses SciPy, SciPy uses C-BLAS and we used OpenBLAS for ndarray and our own vector functions, and everything is dynamically linked together at compile time. So which BLAS is SciPy using? It couldn't find the BLAS function it needed and crashed.","title":"Debugging Ten Layers Down"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#static-link-or-bust","text":"The fix was surprisingly simple: statically link OpenBLAS using the Cargo build script: build.rs fn main () { println! ( \"cargo:rustc-link-lib=static=openblas\" ); } The linker included the code for OpenBLAS into our extension, SciPy was able to find the function it was looking for, and PostgresML 2.0 was working again.","title":"Static Link or Bust"},{"location":"blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres/#recap","text":"In the end, we got what we wanted: Rust machine learning in Postgres was on track Scikit-learn was coming along into PostgresML 2.0 Backwards compatibility with PostgresML 1.0 was preserved and we had a lot of fun working with PyO3 and pushing the limits of what we thought was possible. Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our GitHub .","title":"Recap"},{"location":"blog/data-is-living-and-relational/","text":"img.float-right { margin: 0 16px !important; max-width: 50% !important; float: right; } img.float-left { margin: 0 16px !important; max-width: 60% !important; float: left; } img.center { margin: 16px 12.5%; max-width: 75%; } Data is Living and Relational \u00b6 Montana Low August 25, 2022 A common problem with data science and machine learning tutorials is the published and studied datasets are often nothing like what you\u2019ll find in industry. width height area 1 1 1 2 1 2 2 2 4 They are: usually denormalized into a single tabular form, e.g. a CSV file often relatively tiny to medium amounts of data, not big data always static, with new rows never added sometimes pretreated to clean or simplify the data As Data Science transitions from academia into industry, these norms influence organizations and applications. Professional Data Scientists need teams of Data Engineers to move data from production databases into data warehouses and denormalized schemas, which are more familiar and ideally easier to work with. Large offline batch jobs are a typical integration point between Data Scientists and their Engineering counterparts, who primarily deal with online systems. As the systems grow more complex, additional specialized Machine Learning Engineers are required to optimize performance and scalability bottlenecks between databases, warehouses, models and applications. This eventually leads to expensive maintenance and terminal complexity: new improvements to the system become exponentially more difficult. Ultimately, previously working models start getting replaced by simpler solutions, so the business can continue to iterate. This is not a new phenomenon, see the fate of the Netflix Prize. Announcing the PostgresML Gym \ud83c\udf89 \u00b6 Instead of starting from the academic perspective that data is dead, PostgresML embraces the living and dynamic nature of data produced by modern organizations. It's relational and growing in multiple dimensions. Relational data: is normalized for real time performance and correctness considerations has new rows added and updated constantly, which form incomplete features for a prediction Meanwhile, denormalized datasets: may grow to billions of rows, where single updates multiply into mass rewrites often span multiple iterations of the schema, with software bugs leaving behind outliers We think it\u2019s worth attempting to move the machine learning process and modern data architectures beyond the status quo. To that end, we\u2019re building the PostgresML Gym, a free offering, to provide a test bed for real world ML experimentation, in a Postgres database. Your personal Gym will include the PostgresML dashboard, several tutorial notebooks to get you started, and access to your own personal PostgreSQL database, supercharged with our machine learning extension. Try the PostgresML Gym Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work.","title":"Data is Living and Relational"},{"location":"blog/data-is-living-and-relational/#data-is-living-and-relational","text":"Montana Low August 25, 2022 A common problem with data science and machine learning tutorials is the published and studied datasets are often nothing like what you\u2019ll find in industry. width height area 1 1 1 2 1 2 2 2 4 They are: usually denormalized into a single tabular form, e.g. a CSV file often relatively tiny to medium amounts of data, not big data always static, with new rows never added sometimes pretreated to clean or simplify the data As Data Science transitions from academia into industry, these norms influence organizations and applications. Professional Data Scientists need teams of Data Engineers to move data from production databases into data warehouses and denormalized schemas, which are more familiar and ideally easier to work with. Large offline batch jobs are a typical integration point between Data Scientists and their Engineering counterparts, who primarily deal with online systems. As the systems grow more complex, additional specialized Machine Learning Engineers are required to optimize performance and scalability bottlenecks between databases, warehouses, models and applications. This eventually leads to expensive maintenance and terminal complexity: new improvements to the system become exponentially more difficult. Ultimately, previously working models start getting replaced by simpler solutions, so the business can continue to iterate. This is not a new phenomenon, see the fate of the Netflix Prize.","title":"Data is Living and Relational"},{"location":"blog/data-is-living-and-relational/#announcing-the-postgresml-gym","text":"Instead of starting from the academic perspective that data is dead, PostgresML embraces the living and dynamic nature of data produced by modern organizations. It's relational and growing in multiple dimensions. Relational data: is normalized for real time performance and correctness considerations has new rows added and updated constantly, which form incomplete features for a prediction Meanwhile, denormalized datasets: may grow to billions of rows, where single updates multiply into mass rewrites often span multiple iterations of the schema, with software bugs leaving behind outliers We think it\u2019s worth attempting to move the machine learning process and modern data architectures beyond the status quo. To that end, we\u2019re building the PostgresML Gym, a free offering, to provide a test bed for real world ML experimentation, in a Postgres database. Your personal Gym will include the PostgresML dashboard, several tutorial notebooks to get you started, and access to your own personal PostgreSQL database, supercharged with our machine learning extension. Try the PostgresML Gym Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work.","title":"Announcing the PostgresML Gym \ud83c\udf89"},{"location":"blog/oxidizing-machine-learning/","text":"Oxidizing Machine Learning \u00b6 Lev Kokotov September 7, 2022 Machine learning in Python can be hard to deploy at scale. We all love Python, but it's no secret that its overhead is large: Load data from large CSV files Do some post-processing with NumPy Move and join data into a Pandas dataframe Load data into the algorithm Each step incurs at least one copy of the data in memory; 4x storage and compute cost for training a model sounds inefficient, but when you add Python's memory allocation, the price tag increases exponentially. Even if you could find the money to pay for the compute needed, fitting the dataset we want into the RAM we have becomes difficult. The status quo needs a shake up, and along came Rust. The State of ML in Rust \u00b6 Doing machine learning in anything but Python sounds wild, but if one looks under the hood, ML algorithms are mostly written in C++: libtorch (Torch), XGBoost, large parts of Tensorflow, libsvm (Support Vector Machines), and the list goes on. A linear regression can be (and is) written in about 10 lines of for-loops. It then should come to no surprise that the Rust ML community is alive, and doing well: SmartCore 1 is rivaling Scikit for commodity algorithms XGBoost bindings 2 work great for gradient boosted trees Torch bindings 3 are first class for building any kind of neural network Tensorflow bindings 4 are also in the mix, although parts of them are still Python (e.g. Keras) If you start missing NumPy, don't worry, the Rust version 5 has got you covered, and the list of available tools keeps growing. When you only need 4 bytes to represent a floating point instead of Python's 26 bytes 6 , suddenly you can do more. XGBoost, Rustified \u00b6 Let's do a quick example to illustrate our point. XGBoost is a popular decision tree algorithm which uses gradient boosting, a fancy optimization technique, to train algorithms on data that could confuse simpler linear models. It comes with a Python interface, which calls into its C++ primitives, but now, it has a Rust interface as well. Cargo.toml [dependencies] xgboost = \"0.1\" src/main.rs use xgboost :: { parameters , Booster , DMatrix }; fn main () { // Data is read directly into the C++ data structure let train = DMatrix :: load ( \"train.txt\" ). unwrap (); let test = DMatrix :: load ( \"test.txt\" ). unwrap (); // Task (regression or classification) let learning_params = parameters :: learning :: LearningTaskParametersBuilder :: default () . objective ( parameters :: learning :: Objective :: BinaryLogistic ) . build () . unwrap (); // Tree parameters (e.g. depth) let tree_params = parameters :: tree :: TreeBoosterParametersBuilder :: default () . max_depth ( 2 ) . eta ( 1.0 ) . build () . unwrap (); // Gradient boosting parameters let booster_params = parameters :: BoosterParametersBuilder :: default () . booster_type ( parameters :: BoosterType :: Tree ( tree_params )) . learning_params ( learning_params ) . build () . unwrap (); // Train on train data, test accuracy on test data let evaluation_sets = & [( & train , \"train\" ), ( & test , \"test\" )]; // Final algorithm configuration let params = parameters :: TrainingParametersBuilder :: default () . dtrain ( & train ) . boost_rounds ( 2 ) // n_estimators . booster_params ( booster_params ) . evaluation_sets ( Some ( evaluation_sets )) . build () . unwrap (); // Train the model let model = Booster :: train ( & params ). unwrap (); // Save and load later in any language that has XGBoost bindings model . save ( \"/tmp/xgboost_model.bin\" ). unwrap (); } Example created from rust-xgboost 7 documentation and my own experiments. That's it! You just trained an XGBoost model in Rust, in just a few lines of efficient and ergonomic code. Unlike Python, Rust compiles and verifies your code, so you'll know that it's likely to work before you even run it. When it can take several hours to train a model, it's great to know that you don't have a syntax error on your last line. SmartCore \u21a9 XGBoost bindings \u21a9 Torch bindings \u21a9 Tensorflow bindings \u21a9 rust-ndarray \u21a9 Python floating points \u21a9 rust-xgboost \u21a9","title":"Oxidizing Machine Learning"},{"location":"blog/oxidizing-machine-learning/#oxidizing-machine-learning","text":"Lev Kokotov September 7, 2022 Machine learning in Python can be hard to deploy at scale. We all love Python, but it's no secret that its overhead is large: Load data from large CSV files Do some post-processing with NumPy Move and join data into a Pandas dataframe Load data into the algorithm Each step incurs at least one copy of the data in memory; 4x storage and compute cost for training a model sounds inefficient, but when you add Python's memory allocation, the price tag increases exponentially. Even if you could find the money to pay for the compute needed, fitting the dataset we want into the RAM we have becomes difficult. The status quo needs a shake up, and along came Rust.","title":"Oxidizing Machine Learning"},{"location":"blog/oxidizing-machine-learning/#the-state-of-ml-in-rust","text":"Doing machine learning in anything but Python sounds wild, but if one looks under the hood, ML algorithms are mostly written in C++: libtorch (Torch), XGBoost, large parts of Tensorflow, libsvm (Support Vector Machines), and the list goes on. A linear regression can be (and is) written in about 10 lines of for-loops. It then should come to no surprise that the Rust ML community is alive, and doing well: SmartCore 1 is rivaling Scikit for commodity algorithms XGBoost bindings 2 work great for gradient boosted trees Torch bindings 3 are first class for building any kind of neural network Tensorflow bindings 4 are also in the mix, although parts of them are still Python (e.g. Keras) If you start missing NumPy, don't worry, the Rust version 5 has got you covered, and the list of available tools keeps growing. When you only need 4 bytes to represent a floating point instead of Python's 26 bytes 6 , suddenly you can do more.","title":"The State of ML in Rust"},{"location":"blog/oxidizing-machine-learning/#xgboost-rustified","text":"Let's do a quick example to illustrate our point. XGBoost is a popular decision tree algorithm which uses gradient boosting, a fancy optimization technique, to train algorithms on data that could confuse simpler linear models. It comes with a Python interface, which calls into its C++ primitives, but now, it has a Rust interface as well. Cargo.toml [dependencies] xgboost = \"0.1\" src/main.rs use xgboost :: { parameters , Booster , DMatrix }; fn main () { // Data is read directly into the C++ data structure let train = DMatrix :: load ( \"train.txt\" ). unwrap (); let test = DMatrix :: load ( \"test.txt\" ). unwrap (); // Task (regression or classification) let learning_params = parameters :: learning :: LearningTaskParametersBuilder :: default () . objective ( parameters :: learning :: Objective :: BinaryLogistic ) . build () . unwrap (); // Tree parameters (e.g. depth) let tree_params = parameters :: tree :: TreeBoosterParametersBuilder :: default () . max_depth ( 2 ) . eta ( 1.0 ) . build () . unwrap (); // Gradient boosting parameters let booster_params = parameters :: BoosterParametersBuilder :: default () . booster_type ( parameters :: BoosterType :: Tree ( tree_params )) . learning_params ( learning_params ) . build () . unwrap (); // Train on train data, test accuracy on test data let evaluation_sets = & [( & train , \"train\" ), ( & test , \"test\" )]; // Final algorithm configuration let params = parameters :: TrainingParametersBuilder :: default () . dtrain ( & train ) . boost_rounds ( 2 ) // n_estimators . booster_params ( booster_params ) . evaluation_sets ( Some ( evaluation_sets )) . build () . unwrap (); // Train the model let model = Booster :: train ( & params ). unwrap (); // Save and load later in any language that has XGBoost bindings model . save ( \"/tmp/xgboost_model.bin\" ). unwrap (); } Example created from rust-xgboost 7 documentation and my own experiments. That's it! You just trained an XGBoost model in Rust, in just a few lines of efficient and ergonomic code. Unlike Python, Rust compiles and verifies your code, so you'll know that it's likely to work before you even run it. When it can take several hours to train a model, it's great to know that you don't have a syntax error on your last line. SmartCore \u21a9 XGBoost bindings \u21a9 Torch bindings \u21a9 Tensorflow bindings \u21a9 rust-ndarray \u21a9 Python floating points \u21a9 rust-xgboost \u21a9","title":"XGBoost, Rustified"},{"location":"blog/postgres-full-text-search-is-awesome/","text":"Postgres Full Text Search is Awesome! Montana Low August 31, 2022 Normalized data is a powerful tool leveraged by 10x engineering organizations. If you haven't read Postgres Full Text Search is Good Enough! you should, unless you're willing to take that statement at face value, without the code samples to prove it. We'll go beyond that claim in this post, but to reiterate the main points, Postgres supports: Stemming Ranking / Boost Multiple languages Fuzzy search for misspelling Accent support This is good enough for most of the use cases out there, without introducing any additional concerns to your application. But, if you've ever tried to deliver relevant search results at scale, you'll realize that you need a lot more than these fundamentals. ElasticSearch has all kinds of best in class features, like a modified version of BM25 that is state of the art (developed in the 1970's), which is one of the many features you need beyond the Term Frequency (TF) based ranking that Postgres uses... but, the ElasticSearch approach is a dead end for 2 reasons: Trying to improve search relevance with statistics like TF-IDF and BM25 is like trying to make a flying car. What you want is a helicopter instead. Computing Inverse Document Frequency (IDF) for BM25 brutalizes your search indexing performance, which leads to a host of follow on issues via distributed computation , for the originally dubious reason. What we were promised Academics have spent decades inventing many algorithms that use orders of magnitude more compute eking out marginally better results that often aren't worth it in practice. Not to generally disparage academia, their work has consistently improved our world, but we need to pay attention to tradeoffs. SQL is another acronym similarly pioneered in the 1970's. One difference between SQL and BM25 is that everyone has heard of the former before reading this blog post, for good reason. If you actually want to meaningfully improve search results, you generally need to add new data sources. Relevance is much more often revealed by the way other things relate to the document, rather than the content of the document itself. Google proved the point 23 years ago. Pagerank doesn't rely on the page content itself as much as it uses metadata from links to the pages . We live in a connected world and it's the interplay among things that reveal their relevance, whether that is links for websites, sales for products, shares for social posts... It's the greater context around the document that matters. If you want to improve your search results, don't rely on expensive O(n*m) word frequency statistics. Get new sources of data instead. It's the relational nature of relevance that underpins why a relational database forms the ideal search engine. Postgres made the right call to avoid the costs required to compute Inverse Document Frequency in their search indexing, given its meager benefit. Instead, it offers the most feature-complete relational data platform. Elasticsearch will tell you , that you can't join data in a naively distributed system at read time, because it is prohibitively expensive. Instead you'll have to join the data eagerly at indexing time, which is even more prohibitively expensive. That's good for their business since you're the one paying for it, and it will scale until you're bankrupt. What you really should do, is leave the data normalized inside Postgres, which will allow you to join additional, related data at query time. It will take multiple orders of magnitude less compute to index and search a normalized corpus, meaning you'll have a lot longer (potentially forever) before you need to distribute your workload, and then maybe you can do that intelligently instead of naively. Instead of spending your time building and maintaining pipelines to shuffle updates between systems, you can work on new sources of data to really improve relevance. With PostgresML, you can now skip straight to full on machine learning when you have the related data. You can load your feature store into the same database as your search corpus. Each data source can live in its own independent table, with its own update cadence, rather than having to reindex and denormalize entire documents back to ElasticSearch, or worse, large portions of the entire corpus, when a single thing changes. With a single SQL query, you can do multiple passes of re-ranking, pruning and personalization to refine a search relevance score. basic term relevance embedding similarities XGBoost or LightGBM inference These queries can execute in milliseconds on large production-sized corpora with Postgres's multiple indexing strategies. You can do all of this without adding any new infrastructure to your stack. The following full blown example is for demonstration purposes only of a 3 rd generation search engine. You can test it for real in the PostgresML Gym to build up a complete understanding. Try the PostgresML Gym search.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 WITH query AS ( -- construct a query context with arguments that would typically be -- passed in from the application layer SELECT -- a keyword query for \"my\" OR \"search\" OR \"terms\" tsquery ( 'my | search | terms' ) AS keywords , -- a user_id for personalization later on 123456 AS user_id ), first_pass AS ( SELECT * , -- calculate the term frequency of keywords in the document ts_rank ( documents . full_text , keywords ) AS term_frequency -- our basic corpus is stored in the documents table FROM documents -- that match the query keywords defined above WHERE documents . full_text @@ query . keywords -- ranked by term frequency ORDER BY term_frequency DESC -- prune to a reasonably large candidate population LIMIT 10000 ), second_pass AS ( SELECT * , -- create a second pass score of cosine_similarity across embeddings pgml . cosine_similarity ( document_embeddings . vector , user_embeddings . vector ) AS similarity_score FROM first_pass -- grab more data from outside the documents JOIN document_embeddings ON document_embeddings . document_id = documents . id JOIN user_embeddings ON user_embeddings . user_id = query . user_id -- of course we be re-ranking ORDER BY similarity_score DESC -- further prune results to top performers for more expensive ranking LIMIT 1000 ), third_pass AS ( SELECT * , -- create a final score using xgboost pgml . predict ( 'search relevance model' , ARRAY [ session_level_features . * ]) AS final_score FROM second_pass JOIN session_level_features ON session_level_features . user_id = query . user_id ) SELECT * FROM third_pass ORDER BY final_score DESC LIMIT 100 ; If you'd like to play through an interactive notebook to generate models for search relevance in a Postgres database, try it in the Gym. An exercise for the curious reader, would be to combine all three scores above into a single algebraic function for ranking, and then into a fourth learned model... Try the PostgresML Gym Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work.","title":"Postgres Full Text Search is Awesome"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/","text":"PostgresML is 8-40x faster than Python HTTP microservices \u00b6 Lev Kokotov October 18, 2022 Machine learning architectures can be some of the most complex, expensive and difficult arenas in modern systems. The number of technologies and the amount of required hardware compete for tightening headcount, hosting, and latency budgets. Unfortunately, the trend in the industry is only getting worse along these lines, with increased usage of state-of-the-art architectures that center around data warehouses, microservices and NoSQL databases. PostgresML is a simpler alternative to that ever-growing complexity. In this post, we explore some additional performance benefits of a more elegant architecture and discover that PostgresML outperforms traditional Python microservices by a factor of 8 in local tests and by a factor of 40 on AWS EC2. Candidate architectures \u00b6 To consider Python microservices with every possible advantage, our first benchmark is run with Python and Redis located on the same machine. Our goal is to avoid any additional network latency, which puts it on a more even footing with PostgresML. Our second test takes place on AWS EC2, with Redis and Gunicorn separated by a network; this benchmark proves to be relatively devastating. The full source code for both benchmarks is available on Github . PostgresML \u00b6 PostgresML architecture is composed of: A PostgreSQL server with PostgresML v2.0 pgbench SQL client Python \u00b6 Python architecture is composed of: A Flask/Gunicorn server accepting and returning JSON CSV file with the training data Redis feature store with the inference dataset, serialized with JSON ab HTTP client ML \u00b6 Both architectures host the same XGBoost model, running predictions against the same dataset. See Methodology for more details. Results \u00b6 Throughput \u00b6 Throughput is defined as the number of XGBoost predictions the architecture can serve per second. In this benchmark, PostgresML outperformed Python and Redis, running on the same machine, by a factor of 8 . In Python, most of the bottleneck comes from having to fetch and deserialize Redis data. Since the features are externally stored, they need to be passed through Python and into XGBoost. XGBoost itself is written in C++, and it's Python library only provides a convenient interface. The prediction coming out of XGBoost has to go through Python again, serialized as JSON, and sent via HTTP to the client. This is pretty much the bare minimum amount of work you can do for an inference microservice. PostgresML, on the other hand, collocates data and compute. It fetches data from a Postgres table, which already comes in a standard floating point format, and the Rust inference layer forwards it to XGBoost via a pointer. An interesting thing happened when the benchmark hit 20 clients: PostgresML throughput starts to quickly decrease. This may be surprising to some, but to Postgres enthusiasts it's a known issue: Postgres isn't very good at handling more concurrent active connections than CPU threads. To mitigate this, we introduced PgBouncer (a Postgres proxy and pooler) in front of the database, and the throughput increased back up, and continued to hold as we went to 100 clients. It's worth noting that the benchmarking machine had only 16 available CPU threads (8 cores). If more cores were available, the bottleneck would only occur with more clients. The general recommendation for Postgres servers it to open around 2 connections per available CPU core, although newer versions of PostgreSQL have been incrementally chipping away at this limitation. Why throughput is important \u00b6 Throughput allows you to do more with less. If you're able to serve 30,000 queries per second using a single machine, but only using 1,000 today, you're unlikely to need an upgrade anytime soon. On the other hand, if the system can only serve 5,000 requests, an expensive and possibly stressful upgrade is in your near future. Latency \u00b6 Latency is defined as the time it takes to return a single XGBoost prediction. Since most systems have limited resources, throughput directly impacts latency (and vice versa). If there are many active requests, clients waiting in the queue take longer to be serviced, and overall system latency increases. In this benchmark, PostgresML outperformed Python by a factor of 8 as well. You'll note the same issue happens at 20 clients, and the same mitigation using PgBouncer reduces its impact. Meanwhile, Python's latency continues to increase substantially. Latency is a good metric to use when describing the performance of an architecture. In other words, if I were to use this service, I would get a prediction back in at most this long, irrespective of how many other clients are using it. Why latency is important \u00b6 Latency is important in machine learning services because they are often running as an addition to the main application, and sometimes have to be accessed multiple times during the same HTTP request. Let's take the example of an e-commerce website. A typical storefront wants to show many personalization models concurrently. Examples of such models could include \"buy it again\" recommendations for recurring purchases (binary classification), or \"popular items in your area\" (geographic clustering of purchase histories) or \"customers like you bought this item\" (nearest neighbour model). All of these models are important because they have been proven, over time, to be very successful at driving purchases. If inference latency is high, the models start to compete for very expensive real estate, front page and checkout, and the business has to drop some of them or, more likely, suffer from slow page loads. Nobody likes a slow app when they are trying to order groceries or dinner. Memory utilization \u00b6 Python is known for using more memory than more optimized languages and, in this case, it uses 7 times more than PostgresML. PostgresML is a Postgres extension, and it shares RAM with the database server. Postgres is very efficient at fetching and allocating only the memory it needs: it reuses shared_buffers and OS page cache to store rows for inference, and requires very little to no memory allocation to serve queries. Meanwhile, Python must allocate memory for each feature it receives from Redis and for each HTTP response it returns. This benchmark did not measure Redis memory utilization, which is an additional and often substantial cost of running traditional machine learning microservices. Training \u00b6 Since Python often uses Pandas to load and preprocess data, it is notably more memory hungry. Before even passing the data into XGBoost, we were already at 8GB RSS (resident set size); during actual fitting, memory utilization went to almost 12GB. This test is another best case scenario for Python, since the data has already been preprocessed, and was merely passed on to the algorithm. Meanwhile, PostresML enjoys sharing RAM with the Postgres server and only allocates the memory needed by XGBoost. The dataset size was significant, but we managed to train the same model using only 5GB of RAM. PostgresML therefore allows training models on datasets at least twice as large as Python, all the while using identical hardware. Why memory utilization is important \u00b6 This is another example of doing more with less. Most machine learning algorithms, outside of FAANG and research universities, require the dataset to fit into the memory of a single machine. Distributed training is not where we want it to be, and there is still so much value to be extracted from simple linear regressions. Using less RAM allows to train larger and better models on larger and more complete datasets. If you happen to suffer from large machine learning compute bills, using less RAM can be a pleasant surprise at the end of your fiscal year. What about UltraJSON/MessagePack/Serializer X? \u00b6 We spent a lot of time talking about serialization, so it makes sense to look at prior work in that field. JSON is the most user-friendly format, but it's certainly not the fastest. MessagePack and Ultra JSON, for example, are sometimes faster and more efficient at reading and storing binary information. So, would using them in this benchmark be better, instead of Python's built-in json module? The answer is: not really. Time to (de)serialize is important, but ultimately needing (de)serialization in the first place is the bottleneck. Taking data out of a remote system (e.g. a feature store like Redis), sending it over a network socket, parsing it into a Python object (which requires memory allocation), only to convert it again to a binary type for XGBoost, is causing unnecessary delays in the system. PostgresML does one in-memory copy of features from Postgres. No network, no (de)serialization, no unnecessary latency. What about the real world? \u00b6 Testing over localhost is convenient, but it's not the most realistic benchmark. In production deployments, the client and the server are on different machines, and in the case of the Python + Redis architecture, the feature store is yet another network hop away. To demonstrate this, we spun up 3 EC2 instances and ran the benchmark again. This time, PostgresML outperformed Python and Redis by a factor of 40 . Network gap between Redis and Gunicorn made things worse...a lot worse. Fetching data from a remote feature store added milliseconds to the request the Python architecture could not spare. The additional latency compounded, and in a system that has finite resources, caused contention. Most Gunicorn threads were simply waiting on the network, and thousands of requests were stuck in the queue. PostgresML didn't have this issue, because the features and the Rust inference layer live on the same system. This architectural choice removes network latency and (de)serialization from the equation. You'll note the concurrency issue we discussed earlier hit Postgres at 20 connections, and we used PgBouncer again to save the day. Scaling Postgres, once you know how to do it, isn't as difficult as it sounds. Methodology \u00b6 Hardware \u00b6 Both the client and the server in the first benchmark were located on the same machine. Redis was local as well. The machine is an 8 core, 16 threads AMD Ryzen 7 5800X with 32GB RAM, 1TB NVMe SSD running Ubuntu 22.04. AWS EC2 benchmarks were done with one c5.4xlarge instance hosting Gunicorn and PostgresML, and two c5.large instances hosting the client and Redis, respectively. They were located in the same VPC. Configuration \u00b6 Gunicorn was running with 5 workers and 2 threads per worker. Postgres was using 1, 5 and 20 connections for 1, 5 and 20 clients, respectively. PgBouncer was given a default_pool_size of 10, so a maximum of 10 Postgres connections were used for 20 and 100 clients. XGBoost was allowed to use 2 threads during inference, and all available CPU cores (16 threads) during training. Both ab and pgbench use all available resources, but are very lightweight; the requests were a single JSON object and a single query respectively. Both of the clients use persistent connections, ab by using HTTP Keep-Alives, and pgbench by keeping the Postgres connection open for the duration of the benchmark. ML \u00b6 Data \u00b6 We used the Flight Status Prediction dataset from Kaggle. After some post-processing, it ended up being about 2 GB of floating point features. We didn't use all columns because some of them are redundant, e.g. airport name and airport identifier, which refer to the same thing. Model \u00b6 Our XGBoost model was trained with default hyperparameters and 25 estimators (also known as boosting rounds). Data used for training and inference is available here . Data stored in the Redis feature store is available here . It's only a subset because it was taking hours to load the entire dataset into Redis with a single Python process (28 million rows). Meanwhile, Postgres COPY only took about a minute. PostgresML model is trained with: SELECT * FROM pgml . train ( project_name => 'r2' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); It had terrible accuracy (as did the Python version), probably because we were missing any kind of weather information, the latter most likely causing delays at airports. Source code \u00b6 Benchmark source code can be found on Github . Feedback \u00b6 Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our Github .","title":"PostgresML is 8-40x faster than Python HTTP microservices"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#postgresml-is-8-40x-faster-than-python-http-microservices","text":"Lev Kokotov October 18, 2022 Machine learning architectures can be some of the most complex, expensive and difficult arenas in modern systems. The number of technologies and the amount of required hardware compete for tightening headcount, hosting, and latency budgets. Unfortunately, the trend in the industry is only getting worse along these lines, with increased usage of state-of-the-art architectures that center around data warehouses, microservices and NoSQL databases. PostgresML is a simpler alternative to that ever-growing complexity. In this post, we explore some additional performance benefits of a more elegant architecture and discover that PostgresML outperforms traditional Python microservices by a factor of 8 in local tests and by a factor of 40 on AWS EC2.","title":"PostgresML is 8-40x faster than Python HTTP microservices"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#candidate-architectures","text":"To consider Python microservices with every possible advantage, our first benchmark is run with Python and Redis located on the same machine. Our goal is to avoid any additional network latency, which puts it on a more even footing with PostgresML. Our second test takes place on AWS EC2, with Redis and Gunicorn separated by a network; this benchmark proves to be relatively devastating. The full source code for both benchmarks is available on Github .","title":"Candidate architectures"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#postgresml","text":"PostgresML architecture is composed of: A PostgreSQL server with PostgresML v2.0 pgbench SQL client","title":"PostgresML"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#python","text":"Python architecture is composed of: A Flask/Gunicorn server accepting and returning JSON CSV file with the training data Redis feature store with the inference dataset, serialized with JSON ab HTTP client","title":"Python"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#ml","text":"Both architectures host the same XGBoost model, running predictions against the same dataset. See Methodology for more details.","title":"ML"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#results","text":"","title":"Results"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#throughput","text":"Throughput is defined as the number of XGBoost predictions the architecture can serve per second. In this benchmark, PostgresML outperformed Python and Redis, running on the same machine, by a factor of 8 . In Python, most of the bottleneck comes from having to fetch and deserialize Redis data. Since the features are externally stored, they need to be passed through Python and into XGBoost. XGBoost itself is written in C++, and it's Python library only provides a convenient interface. The prediction coming out of XGBoost has to go through Python again, serialized as JSON, and sent via HTTP to the client. This is pretty much the bare minimum amount of work you can do for an inference microservice. PostgresML, on the other hand, collocates data and compute. It fetches data from a Postgres table, which already comes in a standard floating point format, and the Rust inference layer forwards it to XGBoost via a pointer. An interesting thing happened when the benchmark hit 20 clients: PostgresML throughput starts to quickly decrease. This may be surprising to some, but to Postgres enthusiasts it's a known issue: Postgres isn't very good at handling more concurrent active connections than CPU threads. To mitigate this, we introduced PgBouncer (a Postgres proxy and pooler) in front of the database, and the throughput increased back up, and continued to hold as we went to 100 clients. It's worth noting that the benchmarking machine had only 16 available CPU threads (8 cores). If more cores were available, the bottleneck would only occur with more clients. The general recommendation for Postgres servers it to open around 2 connections per available CPU core, although newer versions of PostgreSQL have been incrementally chipping away at this limitation.","title":"Throughput"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#why-throughput-is-important","text":"Throughput allows you to do more with less. If you're able to serve 30,000 queries per second using a single machine, but only using 1,000 today, you're unlikely to need an upgrade anytime soon. On the other hand, if the system can only serve 5,000 requests, an expensive and possibly stressful upgrade is in your near future.","title":"Why throughput is important"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#latency","text":"Latency is defined as the time it takes to return a single XGBoost prediction. Since most systems have limited resources, throughput directly impacts latency (and vice versa). If there are many active requests, clients waiting in the queue take longer to be serviced, and overall system latency increases. In this benchmark, PostgresML outperformed Python by a factor of 8 as well. You'll note the same issue happens at 20 clients, and the same mitigation using PgBouncer reduces its impact. Meanwhile, Python's latency continues to increase substantially. Latency is a good metric to use when describing the performance of an architecture. In other words, if I were to use this service, I would get a prediction back in at most this long, irrespective of how many other clients are using it.","title":"Latency"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#why-latency-is-important","text":"Latency is important in machine learning services because they are often running as an addition to the main application, and sometimes have to be accessed multiple times during the same HTTP request. Let's take the example of an e-commerce website. A typical storefront wants to show many personalization models concurrently. Examples of such models could include \"buy it again\" recommendations for recurring purchases (binary classification), or \"popular items in your area\" (geographic clustering of purchase histories) or \"customers like you bought this item\" (nearest neighbour model). All of these models are important because they have been proven, over time, to be very successful at driving purchases. If inference latency is high, the models start to compete for very expensive real estate, front page and checkout, and the business has to drop some of them or, more likely, suffer from slow page loads. Nobody likes a slow app when they are trying to order groceries or dinner.","title":"Why latency is important"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#memory-utilization","text":"Python is known for using more memory than more optimized languages and, in this case, it uses 7 times more than PostgresML. PostgresML is a Postgres extension, and it shares RAM with the database server. Postgres is very efficient at fetching and allocating only the memory it needs: it reuses shared_buffers and OS page cache to store rows for inference, and requires very little to no memory allocation to serve queries. Meanwhile, Python must allocate memory for each feature it receives from Redis and for each HTTP response it returns. This benchmark did not measure Redis memory utilization, which is an additional and often substantial cost of running traditional machine learning microservices.","title":"Memory utilization"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#training","text":"Since Python often uses Pandas to load and preprocess data, it is notably more memory hungry. Before even passing the data into XGBoost, we were already at 8GB RSS (resident set size); during actual fitting, memory utilization went to almost 12GB. This test is another best case scenario for Python, since the data has already been preprocessed, and was merely passed on to the algorithm. Meanwhile, PostresML enjoys sharing RAM with the Postgres server and only allocates the memory needed by XGBoost. The dataset size was significant, but we managed to train the same model using only 5GB of RAM. PostgresML therefore allows training models on datasets at least twice as large as Python, all the while using identical hardware.","title":"Training"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#why-memory-utilization-is-important","text":"This is another example of doing more with less. Most machine learning algorithms, outside of FAANG and research universities, require the dataset to fit into the memory of a single machine. Distributed training is not where we want it to be, and there is still so much value to be extracted from simple linear regressions. Using less RAM allows to train larger and better models on larger and more complete datasets. If you happen to suffer from large machine learning compute bills, using less RAM can be a pleasant surprise at the end of your fiscal year.","title":"Why memory utilization is important"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#what-about-ultrajsonmessagepackserializer-x","text":"We spent a lot of time talking about serialization, so it makes sense to look at prior work in that field. JSON is the most user-friendly format, but it's certainly not the fastest. MessagePack and Ultra JSON, for example, are sometimes faster and more efficient at reading and storing binary information. So, would using them in this benchmark be better, instead of Python's built-in json module? The answer is: not really. Time to (de)serialize is important, but ultimately needing (de)serialization in the first place is the bottleneck. Taking data out of a remote system (e.g. a feature store like Redis), sending it over a network socket, parsing it into a Python object (which requires memory allocation), only to convert it again to a binary type for XGBoost, is causing unnecessary delays in the system. PostgresML does one in-memory copy of features from Postgres. No network, no (de)serialization, no unnecessary latency.","title":"What about UltraJSON/MessagePack/Serializer X?"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#what-about-the-real-world","text":"Testing over localhost is convenient, but it's not the most realistic benchmark. In production deployments, the client and the server are on different machines, and in the case of the Python + Redis architecture, the feature store is yet another network hop away. To demonstrate this, we spun up 3 EC2 instances and ran the benchmark again. This time, PostgresML outperformed Python and Redis by a factor of 40 . Network gap between Redis and Gunicorn made things worse...a lot worse. Fetching data from a remote feature store added milliseconds to the request the Python architecture could not spare. The additional latency compounded, and in a system that has finite resources, caused contention. Most Gunicorn threads were simply waiting on the network, and thousands of requests were stuck in the queue. PostgresML didn't have this issue, because the features and the Rust inference layer live on the same system. This architectural choice removes network latency and (de)serialization from the equation. You'll note the concurrency issue we discussed earlier hit Postgres at 20 connections, and we used PgBouncer again to save the day. Scaling Postgres, once you know how to do it, isn't as difficult as it sounds.","title":"What about the real world?"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#methodology","text":"","title":"Methodology"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#hardware","text":"Both the client and the server in the first benchmark were located on the same machine. Redis was local as well. The machine is an 8 core, 16 threads AMD Ryzen 7 5800X with 32GB RAM, 1TB NVMe SSD running Ubuntu 22.04. AWS EC2 benchmarks were done with one c5.4xlarge instance hosting Gunicorn and PostgresML, and two c5.large instances hosting the client and Redis, respectively. They were located in the same VPC.","title":"Hardware"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#configuration","text":"Gunicorn was running with 5 workers and 2 threads per worker. Postgres was using 1, 5 and 20 connections for 1, 5 and 20 clients, respectively. PgBouncer was given a default_pool_size of 10, so a maximum of 10 Postgres connections were used for 20 and 100 clients. XGBoost was allowed to use 2 threads during inference, and all available CPU cores (16 threads) during training. Both ab and pgbench use all available resources, but are very lightweight; the requests were a single JSON object and a single query respectively. Both of the clients use persistent connections, ab by using HTTP Keep-Alives, and pgbench by keeping the Postgres connection open for the duration of the benchmark.","title":"Configuration"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#ml_1","text":"","title":"ML"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#data","text":"We used the Flight Status Prediction dataset from Kaggle. After some post-processing, it ended up being about 2 GB of floating point features. We didn't use all columns because some of them are redundant, e.g. airport name and airport identifier, which refer to the same thing.","title":"Data"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#model","text":"Our XGBoost model was trained with default hyperparameters and 25 estimators (also known as boosting rounds). Data used for training and inference is available here . Data stored in the Redis feature store is available here . It's only a subset because it was taking hours to load the entire dataset into Redis with a single Python process (28 million rows). Meanwhile, Postgres COPY only took about a minute. PostgresML model is trained with: SELECT * FROM pgml . train ( project_name => 'r2' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); It had terrible accuracy (as did the Python version), probably because we were missing any kind of weather information, the latter most likely causing delays at airports.","title":"Model"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#source-code","text":"Benchmark source code can be found on Github .","title":"Source code"},{"location":"blog/postgresml-is-8x-faster-than-python-http-microservices/#feedback","text":"Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our Github .","title":"Feedback"},{"location":"blog/postgresml-is-moving-to-rust-for-our-2.0-release/","text":"PostgresML is Moving to Rust for our 2.0 Release \u00b6 Montana Low September 19, 2022 PostgresML is a fairly young project. We recently released v1.0 and now we're considering what we want to accomplish for v2.0. In addition to simplifying the workflow for building models, we'd like to address runtime speed, memory consumption and the overall reliability we've seen is needed for machine learning deployments running at scale. Python is generally touted as fast enough for machine learning, and is the de facto industry standard with tons of popular libraries, implementing all the latest and greatest algorithms. Many of these algorithms (Torch, Tensorflow, XGboost, NumPy) have been optimized in C, but not all of them. For example, most of the linear algorithms in scikit-learn are written in pure Python, although they do use NumPy, which is a convenient optimization. It also uses Cython in a few performance critical places. This ecosystem has allowed PostgresML to offer a ton of functionality with minimal duplication of effort. Ambition Starts With a Simple Benchmark \u00b6 Rust mascot image by opensource.com To illustrate our motivation, we'll create a test set of 10,000 random embeddings with 128 dimensions, and store them in a table. Our first benchmark will simulate semantic ranking, by computing the dot product against every member of the test set, sorting the results and returning the top match. generate_embeddings.sql 1 2 3 4 5 -- Generate 10,000 embeddings with 128 dimensions as FLOAT4[] type. CREATE TABLE embeddings AS SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 1280000 ) i GROUP BY i % 10000 ; Spoiler alert: idiomatic Rust is about 10x faster than native SQL, embedded PL/pgSQL, and pure Python. Rust comes close to the hand-optimized assembly version of the Basic Linear Algebra Subroutines (BLAS) implementation. NumPy is supposed to provide optimizations in cases like this, but it's actually the worst performer. Data movement from Postgres to PL/Python is pretty good; it's even faster than the pure SQL equivalent, but adding the extra conversion from Python list to Numpy array takes almost as much time as everything else. Machine Learning systems that move relatively large quantities of data around can become dominated by these extraneous operations, rather than the ML algorithms that actually generate value. SQL PL/pgSQL Python NumPy Rust BLAS define_sql.sql 1 2 3 4 5 6 7 CREATE OR REPLACE FUNCTION dot_product_sql ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE sql IMMUTABLE STRICT PARALLEL SAFE AS $$ SELECT SUM ( multiplied . values ) FROM ( SELECT UNNEST ( a ) * UNNEST ( b ) AS values ) AS multiplied ; $$ ; test_sql.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_sql ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_plpgsql.sql 1 2 3 4 5 6 7 8 9 CREATE OR REPLACE FUNCTION dot_product_plpgsql ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE plpgsql IMMUTABLE STRICT PARALLEL SAFE AS $$ BEGIN RETURN SUM ( multiplied . values ) FROM ( SELECT UNNEST ( a ) * UNNEST ( b ) AS values ) AS multiplied ; END $$ ; test_plpgsql.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_plpgsql ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_python.sql 1 2 3 4 5 6 CREATE OR REPLACE FUNCTION dot_product_python ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE plpython3u IMMUTABLE STRICT PARALLEL SAFE AS $$ return sum ([ a * b for a , b in zip ( a , b )]) $$ ; test_python.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_python ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_numpy.sql 1 2 3 4 5 6 7 CREATE OR REPLACE FUNCTION dot_product_numpy ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE plpython3u IMMUTABLE STRICT PARALLEL SAFE AS $$ import numpy return numpy . dot ( a , b ) $$ ; test_numpy.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_numpy ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_rust.rs 1 2 3 4 5 6 7 8 9 #[pg_extern(immutable, strict, parallel_safe)] fn dot_product_rust ( vector : Vec < f32 > , other : Vec < f32 > ) -> f32 { vector . as_slice () . iter () . zip ( other . as_slice (). iter ()) . map ( | ( a , b ) | ( a * b )) . sum () } test_rust.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT pgml . dot_product_rust ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_blas.rs 1 2 3 4 5 6 7 8 9 10 11 12 #[pg_extern(immutable, strict, parallel_safe)] fn dot_product_blas ( vector : Vec < f32 > , other : Vec < f32 > ) -> f32 { unsafe { blas :: sdot ( vector . len (). try_into (). unwrap (), vector . as_slice (), 1 , other . as_slice (), 1 , ) } } test_blas.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT pgml . dot_product_blas ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; We're building with the Rust pgrx crate that makes our development cycle even nicer than the one we use to manage Python. It really streamlines creating an extension in Rust, so all we have to worry about is writing our functions. It took about an hour to port all of our vector operations to Rust with BLAS support, and another week to port all the \"business logic\" for maintaining model training and deployment. We've even gained some new capabilities for caching models across connections (independent processes), now that we have access to Postgres shared memory, without having to worry about Python's GIL and GC. This is the dream of Apache's Arrow project, realized for our applications, without having to change the world, just our implementations. \ud83e\udd29 Single-copy end-to-end machine learning, with parallel processing and shared data access. What about XGBoost and friends? \u00b6 ML isn't just about basic math and a little bit of business logic. It's about all those complicated algorithms beyond linear regression for gradient boosting and deep learning. The good news is that most of these libraries are implemented in C/C++, and just have Python bindings. There are also bindings for Rust ( lightgbm , xgboost , tensorflow , torch ). Layers of abstraction must remain a good value. The results are somewhat staggering. We didn't spend any time intentionally optimizing Rust over Python. Most of the time spent was just trying to get things to compile. \ud83d\ude05 It's hard to believe the difference is this big, but those fringe operations outside of the core machine learning algorithms really do dominate, requiring up to 35x more time in Python during inference. The difference between classification and regression speeds here are related to the dataset size. The scikit learn handwritten image classification dataset effectively has 64 features (pixels) vs the diabetes regression dataset having only 10 features. The more data we're dealing with, the bigger the improvement we see in Rust . We're even giving Python some leeway by warming up the runtime on the connection before the test, which typically takes a second or two to interpret all of PostgresML's dependencies. Since Rust is a compiled language, there is no longer a need to warmup the connection. This language comparison uses in-process data access. Python based machine learning microservices that communicate with other services over HTTP with JSON or gRPC interfaces will look even worse in comparison, especially if they are stateless and rely on yet another database to provide their data over yet another wire. Preserving Backward Compatibility \u00b6 train.sql 1 2 3 4 5 6 7 SELECT pgml . train ( project_name => 'Handwritten Digit Classifier' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' ); train.sql 1 2 SELECT pgml . predict ( 'Handwritten Digit Classifier' , image ) FROM pgml . digits ; The API is identical between v1.0 and v2.0. We take breaking changes seriously and we're not going to break existing deployments just because we're rewriting the whole project. The only reason we're bumping the major version is because we feel like this is a dramatic change, but we intend to preserve a full compatibility layer with models trained on v1.0 in Python. However, this does mean that to get the full performance benefits, you'll need to retrain models after upgrading. Ensuring High Quality Rust Implementations \u00b6 Besides backwards compatibility, we're building a Python compatibility layer to guarantee we can preserve the full Python model training APIs, when Rust APIs are not at parity in terms of functionality, quality or performance. We started this journey thinking that the older vanilla Python algorithms in Scikit would be the best candidates for replacement in Rust, but that is only partly true. There are high quality efforts in linfa and smartcore that also show 10-30x speedup over Scikit, but they still lack some of the deeper functionality like joint regression, some of the more obscure algorithms and hyperparameters, and some of the error handling that has been hardened into Scikit with mass adoption. We see similar speed up in prediction time for the Rust implementations of classic algorithms. The Rust implementations also produce high quality predictions against test sets, although there is not perfect parity in the implementations where different optimizations have been chosen by default. Interestingly, the training times for some of the simplest algorithms are worse in the Rust implementation. Until we can guarantee each Rust algorithm is an upgrade in every way, we'll continue to use the Python compatibility layer on a case by case basis to avoid any unpleasant surprises. We believe that machine learning in Rust is mature enough to add significant value now. We'll be using the same underlying C/C++ libraries, and it's worth contributing to the Rust ML ecosystem to bring it up to full feature parity. Our v2.0 release will include a benchmark suite for the full API we support via all Python libraries, so that we can track our progress toward pure Rust implementations over time. Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our GitHub . Try the PostgresML Gym","title":"PostgresML is Moving to Rust for our 2.0 Release"},{"location":"blog/postgresml-is-moving-to-rust-for-our-2.0-release/#postgresml-is-moving-to-rust-for-our-20-release","text":"Montana Low September 19, 2022 PostgresML is a fairly young project. We recently released v1.0 and now we're considering what we want to accomplish for v2.0. In addition to simplifying the workflow for building models, we'd like to address runtime speed, memory consumption and the overall reliability we've seen is needed for machine learning deployments running at scale. Python is generally touted as fast enough for machine learning, and is the de facto industry standard with tons of popular libraries, implementing all the latest and greatest algorithms. Many of these algorithms (Torch, Tensorflow, XGboost, NumPy) have been optimized in C, but not all of them. For example, most of the linear algorithms in scikit-learn are written in pure Python, although they do use NumPy, which is a convenient optimization. It also uses Cython in a few performance critical places. This ecosystem has allowed PostgresML to offer a ton of functionality with minimal duplication of effort.","title":"PostgresML is Moving to Rust for our 2.0 Release"},{"location":"blog/postgresml-is-moving-to-rust-for-our-2.0-release/#ambition-starts-with-a-simple-benchmark","text":"Rust mascot image by opensource.com To illustrate our motivation, we'll create a test set of 10,000 random embeddings with 128 dimensions, and store them in a table. Our first benchmark will simulate semantic ranking, by computing the dot product against every member of the test set, sorting the results and returning the top match. generate_embeddings.sql 1 2 3 4 5 -- Generate 10,000 embeddings with 128 dimensions as FLOAT4[] type. CREATE TABLE embeddings AS SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 1280000 ) i GROUP BY i % 10000 ; Spoiler alert: idiomatic Rust is about 10x faster than native SQL, embedded PL/pgSQL, and pure Python. Rust comes close to the hand-optimized assembly version of the Basic Linear Algebra Subroutines (BLAS) implementation. NumPy is supposed to provide optimizations in cases like this, but it's actually the worst performer. Data movement from Postgres to PL/Python is pretty good; it's even faster than the pure SQL equivalent, but adding the extra conversion from Python list to Numpy array takes almost as much time as everything else. Machine Learning systems that move relatively large quantities of data around can become dominated by these extraneous operations, rather than the ML algorithms that actually generate value. SQL PL/pgSQL Python NumPy Rust BLAS define_sql.sql 1 2 3 4 5 6 7 CREATE OR REPLACE FUNCTION dot_product_sql ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE sql IMMUTABLE STRICT PARALLEL SAFE AS $$ SELECT SUM ( multiplied . values ) FROM ( SELECT UNNEST ( a ) * UNNEST ( b ) AS values ) AS multiplied ; $$ ; test_sql.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_sql ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_plpgsql.sql 1 2 3 4 5 6 7 8 9 CREATE OR REPLACE FUNCTION dot_product_plpgsql ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE plpgsql IMMUTABLE STRICT PARALLEL SAFE AS $$ BEGIN RETURN SUM ( multiplied . values ) FROM ( SELECT UNNEST ( a ) * UNNEST ( b ) AS values ) AS multiplied ; END $$ ; test_plpgsql.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_plpgsql ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_python.sql 1 2 3 4 5 6 CREATE OR REPLACE FUNCTION dot_product_python ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE plpython3u IMMUTABLE STRICT PARALLEL SAFE AS $$ return sum ([ a * b for a , b in zip ( a , b )]) $$ ; test_python.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_python ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_numpy.sql 1 2 3 4 5 6 7 CREATE OR REPLACE FUNCTION dot_product_numpy ( a FLOAT4 [], b FLOAT4 []) RETURNS FLOAT4 LANGUAGE plpython3u IMMUTABLE STRICT PARALLEL SAFE AS $$ import numpy return numpy . dot ( a , b ) $$ ; test_numpy.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT dot_product_numpy ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_rust.rs 1 2 3 4 5 6 7 8 9 #[pg_extern(immutable, strict, parallel_safe)] fn dot_product_rust ( vector : Vec < f32 > , other : Vec < f32 > ) -> f32 { vector . as_slice () . iter () . zip ( other . as_slice (). iter ()) . map ( | ( a , b ) | ( a * b )) . sum () } test_rust.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT pgml . dot_product_rust ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; define_blas.rs 1 2 3 4 5 6 7 8 9 10 11 12 #[pg_extern(immutable, strict, parallel_safe)] fn dot_product_blas ( vector : Vec < f32 > , other : Vec < f32 > ) -> f32 { unsafe { blas :: sdot ( vector . len (). try_into (). unwrap (), vector . as_slice (), 1 , other . as_slice (), 1 , ) } } test_blas.sql 1 2 3 4 5 6 7 8 WITH test AS ( SELECT ARRAY_AGG ( random ()):: FLOAT4 [] AS vector FROM generate_series ( 1 , 128 ) i ) SELECT pgml . dot_product_blas ( embeddings . vector , test . vector ) AS dot_product FROM embeddings , test ORDER BY 1 LIMIT 1 ; We're building with the Rust pgrx crate that makes our development cycle even nicer than the one we use to manage Python. It really streamlines creating an extension in Rust, so all we have to worry about is writing our functions. It took about an hour to port all of our vector operations to Rust with BLAS support, and another week to port all the \"business logic\" for maintaining model training and deployment. We've even gained some new capabilities for caching models across connections (independent processes), now that we have access to Postgres shared memory, without having to worry about Python's GIL and GC. This is the dream of Apache's Arrow project, realized for our applications, without having to change the world, just our implementations. \ud83e\udd29 Single-copy end-to-end machine learning, with parallel processing and shared data access.","title":"Ambition Starts With a Simple Benchmark"},{"location":"blog/postgresml-is-moving-to-rust-for-our-2.0-release/#what-about-xgboost-and-friends","text":"ML isn't just about basic math and a little bit of business logic. It's about all those complicated algorithms beyond linear regression for gradient boosting and deep learning. The good news is that most of these libraries are implemented in C/C++, and just have Python bindings. There are also bindings for Rust ( lightgbm , xgboost , tensorflow , torch ). Layers of abstraction must remain a good value. The results are somewhat staggering. We didn't spend any time intentionally optimizing Rust over Python. Most of the time spent was just trying to get things to compile. \ud83d\ude05 It's hard to believe the difference is this big, but those fringe operations outside of the core machine learning algorithms really do dominate, requiring up to 35x more time in Python during inference. The difference between classification and regression speeds here are related to the dataset size. The scikit learn handwritten image classification dataset effectively has 64 features (pixels) vs the diabetes regression dataset having only 10 features. The more data we're dealing with, the bigger the improvement we see in Rust . We're even giving Python some leeway by warming up the runtime on the connection before the test, which typically takes a second or two to interpret all of PostgresML's dependencies. Since Rust is a compiled language, there is no longer a need to warmup the connection. This language comparison uses in-process data access. Python based machine learning microservices that communicate with other services over HTTP with JSON or gRPC interfaces will look even worse in comparison, especially if they are stateless and rely on yet another database to provide their data over yet another wire.","title":"What about XGBoost and friends?"},{"location":"blog/postgresml-is-moving-to-rust-for-our-2.0-release/#preserving-backward-compatibility","text":"train.sql 1 2 3 4 5 6 7 SELECT pgml . train ( project_name => 'Handwritten Digit Classifier' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' ); train.sql 1 2 SELECT pgml . predict ( 'Handwritten Digit Classifier' , image ) FROM pgml . digits ; The API is identical between v1.0 and v2.0. We take breaking changes seriously and we're not going to break existing deployments just because we're rewriting the whole project. The only reason we're bumping the major version is because we feel like this is a dramatic change, but we intend to preserve a full compatibility layer with models trained on v1.0 in Python. However, this does mean that to get the full performance benefits, you'll need to retrain models after upgrading.","title":"Preserving Backward Compatibility"},{"location":"blog/postgresml-is-moving-to-rust-for-our-2.0-release/#ensuring-high-quality-rust-implementations","text":"Besides backwards compatibility, we're building a Python compatibility layer to guarantee we can preserve the full Python model training APIs, when Rust APIs are not at parity in terms of functionality, quality or performance. We started this journey thinking that the older vanilla Python algorithms in Scikit would be the best candidates for replacement in Rust, but that is only partly true. There are high quality efforts in linfa and smartcore that also show 10-30x speedup over Scikit, but they still lack some of the deeper functionality like joint regression, some of the more obscure algorithms and hyperparameters, and some of the error handling that has been hardened into Scikit with mass adoption. We see similar speed up in prediction time for the Rust implementations of classic algorithms. The Rust implementations also produce high quality predictions against test sets, although there is not perfect parity in the implementations where different optimizations have been chosen by default. Interestingly, the training times for some of the simplest algorithms are worse in the Rust implementation. Until we can guarantee each Rust algorithm is an upgrade in every way, we'll continue to use the Python compatibility layer on a case by case basis to avoid any unpleasant surprises. We believe that machine learning in Rust is mature enough to add significant value now. We'll be using the same underlying C/C++ libraries, and it's worth contributing to the Rust ML ecosystem to bring it up to full feature parity. Our v2.0 release will include a benchmark suite for the full API we support via all Python libraries, so that we can track our progress toward pure Rust implementations over time. Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our GitHub . Try the PostgresML Gym","title":"Ensuring High Quality Rust Implementations"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/","text":"Scaling PostgresML to 1 Million Requests per Second \u00b6 Lev Kokotov November 7, 2022 The question \"Does it Scale?\" has become somewhat of a meme in software engineering. There is a good reason for it though, because most businesses plan for success. If your app, online store, or SaaS becomes popular, you want to be sure that the system powering it can serve all your new customers. At PostgresML, we are very concerned with scale. Our engineering background took us through scaling PostgreSQL to 100 TB+, so we're certain that it scales, but could we scale machine learning alongside it? In this post, we'll discuss how we horizontally scale PostgresML to achieve more than 1 million XGBoost predictions per second on commodity hardware. If you missed our previous post and are wondering why someone would combine machine learning and Postgres, take a look at our PostgresML vs. Python benchmark . Architecture Overview \u00b6 If you're familiar with how one runs PostgreSQL at scale, you can skip straight to the results . Part of our thesis, and the reason why we chose Postgres as our host for machine learning, is that scaling machine learning inference is very similar to scaling read queries in a typical database cluster. Inference speed varies based on the model complexity (e.g. n_estimators for XGBoost) and the size of the dataset (how many features the model uses), which is analogous to query complexity and table size in the database world and, as we'll demonstrate further on, scaling the latter is mostly a solved problem. System Architecture Component Description Clients Regular Postgres clients ELB Elastic Network Load Balancer PgCat A Postgres pooler with built-in load balancing, failover, and sharding Replica Regular Postgres replicas Primary Regular Postgres primary Our architecture has four components that may need to scale up or down based on load: Clients Load balancer PgCat pooler Postgres replicas We intentionally don't discuss scaling the primary in this post, because sharding, which is the most effective way to do so, is a fascinating subject that deserves its own series of posts. Spoiler alert: we sharded Postgres without any problems. Clients \u00b6 Clients are regular Postgres connections coming from web apps, job queues, or pretty much anywhere that needs data. They can be long-living or ephemeral and they typically grow in number as the application scales. Most modern deployments use containers which are added as load on the app increases, and removed as the load decreases. This is called dynamic horizontal scaling, and it's an effective way to adapt to changing traffic patterns experienced by most businesses. Load Balancer \u00b6 The load balancer is a way to spread traffic across horizontally scalable components, by routing new connections to targets in a round robin (or random) fashion. It's typically a very large box (or a fast router), but even those need to be scaled if traffic suddenly increases. Since we're running our system on AWS, this is already taken care of, for a reasonably small fee, by using an Elastic Load Balancer. PgCat \u00b6 Meow. All your Postgres belong to me. If you've used Postgres in the past, you know that it can't handle many concurrent connections. For large deployments, it's necessary to run something we call a pooler. A pooler routes thousands of clients to only a few dozen server connections by time-sharing when a client can use a server. Because most queries are very quick, this is a very effective way to run Postgres at scale. There are many poolers available presently, the most notable being PgBouncer, which has been around for a very long time, and is trusted by many large organizations. Unfortunately, it hasn't evolved much with the growing needs of highly available Postgres deployments, so we wrote our own which added important functionality we needed: Load balancing of read queries Failover in case a read replica is broken Sharding (this feature is still being developed) In this benchmark, we used its load balancing feature to evenly distribute XGBoost predictions across our Postgres replicas. Postgres Replicas \u00b6 Scaling Postgres reads is pretty straight forward. If more read queries are coming in, we add a replica to serve the increased load. If the load is decreasing, we remove a replica to save money. The data is replicated from the primary, so all replicas are identical, and all of them can serve any query, or in our case, an XGBoost prediction. PgCat can dynamically add and remove replicas from its config without disconnecting clients, so we can add and remove replicas as needed, without downtime. Parallelizing XGBoost \u00b6 Scaling XGBoost predictions is a little bit more interesting. XGBoost cannot serve predictions concurrently because of internal data structure locks. This is common to many other machine learning algorithms as well, because making predictions can temporarily modify internal components of the model. PostgresML bypasses that limitation because of how Postgres itself handles concurrency: PostgresML concurrency PostgreSQL uses the fork/multiprocessing architecture to serve multiple clients concurrently: each new client connection becomes an independent OS process. During connection startup, PostgresML loads all models inside the process' memory space. This means that each connection has its own copy of the XGBoost model and PostgresML ends up serving multiple XGBoost predictions at the same time without any lock contention. Results \u00b6 We ran over a 100 different benchmarks, by changing the number of clients, poolers, replicas, and XGBoost predictions we requested. The benchmarks were meant to test the limits of each configuration, and what remediations were needed in each scenario. Our raw data is available below . One of the tests we ran used 1,000 clients, which were connected to 1, 2, and 5 replicas. The results were exactly what we expected. Linear Scaling \u00b6 Both latency and throughput, the standard measurements of system performance, scale mostly linearly with the number of replicas. Linear scaling is the north star of all horizontally scalable systems, and most are not able to achieve it because of increasing complexity that comes with synchronization. Our architecture shares nothing and requires no synchronization. The replicas don't talk to each other and the poolers don't either. Every component has the knowledge it needs (through configuration) to do its job, and they do it well. The most impressive result is serving close to a million predictions with an average latency of less than 1ms. You might notice though that 950160.7 isn't quite one million, and that's true. We couldn't reach one million with 1000 clients, so we increased to 2000 and got our magic number: 1,021,692.7 req/sec , with an average latency of 1.7ms . Batching Predictions \u00b6 Batching is a proven method to optimize performance. If you need to get several data points, batch the requests into one query, and it will run faster than making individual requests. We should precede this result by stating that PostgresML does not yet have a batch prediction API as such. Our pgml.predict() function can predict multiple points, but we haven't implemented a query pattern to pass multiple rows to that function at the same time. Once we do, based on our tests, we should see a substantial increase in batch prediction performance. Regardless of that limitation, we still managed to get better results by batching queries together since Postgres needed to do less query parsing and searching, and we saved on network round trip time as well. If batching did not work at all, we would see a linear increase in latency and a linear decrease in throughput. That did not happen; instead, we got a 1.5x improvement by batching 5 predictions together, and a 1.2x improvement by batching 20. A modest success, but a success nonetheless. Graceful Degradation and Queuing \u00b6 All systems, at some point in their lifetime, will come under more load than they were designed for; what happens then is an important feature (or bug) of their design. Horizontal scaling is never immediate: it takes a bit of time to spin up additional hardware to handle the load. It can take a second, or a minute, depending on availability, but in both cases, existing resources need to serve traffic the best way they can. We were hoping to test PostgresML to its breaking point, but we couldn't quite get there. As the load (number of clients) increased beyond provisioned capacity, the only thing we saw was a gradual increase in latency. Throughput remained roughly the same. This gradual latency increase was caused by simple queuing: the replicas couldn't serve requests concurrently, so the requests had to patiently wait in the poolers. \"What's taking so long over there!?\" Among many others, this is a very important feature of any proxy: it's a FIFO queue (first in, first out). If the system is underutilized, queue size is 0 and all requests are served as quickly as physically possible. If the system is overutilized, the queue size increases, holds as the number of requests stabilizes, and decreases back to 0 as the system is scaled up to accommodate new traffic. Queueing overall is not desirable, but it's a feature, not a bug. While autoscaling spins up an additional replica, the app continues to work, although a few milliseconds slower, which is a good trade off for not overspending on hardware. As the demand on PostgresML increases, the system gracefully handles the load. If the number of replicas stays the same, latency slowly increases, all the while remaining well below acceptable ranges. Throughput holds as well, as increasing number of clients evenly split available resources. If we increase the number of replicas, latency decreases and throughput increases, as the number of clients increases in parallel. We get the best result with 5 replicas, but this number is variable and can be changed as needs for latency compete with cost. What's Next \u00b6 Horizontal scaling and high availability are fascinating topics in software engineering. Needing to serve 1 million predictions per second is rare, but having the ability to do that, and more if desired, is an important aspect for any new system. The next challenge for us is to scale writes horizontally. In the database world, this means sharding the database into multiple separate machines using a hashing function, and automatically routing both reads and writes to the right shards. There are many possible solutions on the market for this already, e.g. Citus and Foreign Data Wrappers, but none are as horizontally scalable as we like, although we will incorporate them into our architecture until we build the one we really want. For that purpose, we're building our own open source Postgres proxy which we discussed earlier in the article. As we progress further in our journey, we'll be adding more features and performance improvements. By combining PgCat with PostgresML, we are aiming to build the next generation of machine learning infrastructure that can power anything from tiny startups to unicorns and massive enterprises, without the data ever leaving our favorite database. Methodology \u00b6 ML \u00b6 This time, we used an XGBoost model with 100 trees: SELECT * FROM pgml . train ( 'flights' , task => 'regression' , relation_name => 'flights_mat_3' , y_column_name => 'depdelayminutes' , algorithm => 'xgboost' , hyperparams => '{\"n_estimators\": 100 }' , runtime => 'rust' ); and fetched our predictions the usual way: SELECT pgml . predict ( 'flights' , ARRAY [ year , quarter , month , distance , dayofweek , dayofmonth , flight_number_operating_airline , originairportid , destairportid , flight_number_marketing_airline , departure ] ) AS prediction FROM flights_mat_3 LIMIT :limit ; where :limit is the batch size of 1, 5, and 20. Model \u00b6 The model is roughly the same as the one we used in our previous post , with just one extra feature added, which improved R 2 a little bit. Hardware \u00b6 Client \u00b6 The client was a c5n.4xlarge box on EC2. We chose the c5n class to have the 100 GBit NIC, since we wanted it to saturate our network as much as possible. Thousands of clients were simulated using pgbench . PgCat Pooler \u00b6 PgCat, written in asynchronous Rust, was running on c5.xlarge machines (4 vCPUs, 8GB RAM) with 4 Tokio workers. We used between 1 and 35 machines, and scaled them in increments of 5-20 at a time. The pooler did a decent amount of work around parsing queries, making sure they are read-only SELECT s, and routing them, at random, to replicas. If any replica was down for any reason, it would route around it to remaining machines. Postgres Replicas \u00b6 Postgres replicas were running on c5.9xlarge machines with 36 vCPUs and 72 GB of RAM. The hot dataset fits entirely in memory. The servers were intentionally saturated to maximum capacity before scaling up to test queuing and graceful degradation of performance. Raw Results \u00b6 Raw latency data is available here and raw throughput data is available here . Call to Early Adopters \u00b6 PostgresML and PgCat are free and open source. If your organization can benefit from simplified and fast machine learning, get in touch! We can help deploy PostgresML internally, and collaborate on new and existing features. Join our Discord or email us! Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our Github .","title":"Scaling PostgresML to 1 Million Requests per Second"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#scaling-postgresml-to-1-million-requests-per-second","text":"Lev Kokotov November 7, 2022 The question \"Does it Scale?\" has become somewhat of a meme in software engineering. There is a good reason for it though, because most businesses plan for success. If your app, online store, or SaaS becomes popular, you want to be sure that the system powering it can serve all your new customers. At PostgresML, we are very concerned with scale. Our engineering background took us through scaling PostgreSQL to 100 TB+, so we're certain that it scales, but could we scale machine learning alongside it? In this post, we'll discuss how we horizontally scale PostgresML to achieve more than 1 million XGBoost predictions per second on commodity hardware. If you missed our previous post and are wondering why someone would combine machine learning and Postgres, take a look at our PostgresML vs. Python benchmark .","title":"Scaling PostgresML to 1 Million Requests per Second"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#architecture-overview","text":"If you're familiar with how one runs PostgreSQL at scale, you can skip straight to the results . Part of our thesis, and the reason why we chose Postgres as our host for machine learning, is that scaling machine learning inference is very similar to scaling read queries in a typical database cluster. Inference speed varies based on the model complexity (e.g. n_estimators for XGBoost) and the size of the dataset (how many features the model uses), which is analogous to query complexity and table size in the database world and, as we'll demonstrate further on, scaling the latter is mostly a solved problem. System Architecture Component Description Clients Regular Postgres clients ELB Elastic Network Load Balancer PgCat A Postgres pooler with built-in load balancing, failover, and sharding Replica Regular Postgres replicas Primary Regular Postgres primary Our architecture has four components that may need to scale up or down based on load: Clients Load balancer PgCat pooler Postgres replicas We intentionally don't discuss scaling the primary in this post, because sharding, which is the most effective way to do so, is a fascinating subject that deserves its own series of posts. Spoiler alert: we sharded Postgres without any problems.","title":"Architecture Overview"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#clients","text":"Clients are regular Postgres connections coming from web apps, job queues, or pretty much anywhere that needs data. They can be long-living or ephemeral and they typically grow in number as the application scales. Most modern deployments use containers which are added as load on the app increases, and removed as the load decreases. This is called dynamic horizontal scaling, and it's an effective way to adapt to changing traffic patterns experienced by most businesses.","title":"Clients"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#load-balancer","text":"The load balancer is a way to spread traffic across horizontally scalable components, by routing new connections to targets in a round robin (or random) fashion. It's typically a very large box (or a fast router), but even those need to be scaled if traffic suddenly increases. Since we're running our system on AWS, this is already taken care of, for a reasonably small fee, by using an Elastic Load Balancer.","title":"Load Balancer"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#pgcat","text":"Meow. All your Postgres belong to me. If you've used Postgres in the past, you know that it can't handle many concurrent connections. For large deployments, it's necessary to run something we call a pooler. A pooler routes thousands of clients to only a few dozen server connections by time-sharing when a client can use a server. Because most queries are very quick, this is a very effective way to run Postgres at scale. There are many poolers available presently, the most notable being PgBouncer, which has been around for a very long time, and is trusted by many large organizations. Unfortunately, it hasn't evolved much with the growing needs of highly available Postgres deployments, so we wrote our own which added important functionality we needed: Load balancing of read queries Failover in case a read replica is broken Sharding (this feature is still being developed) In this benchmark, we used its load balancing feature to evenly distribute XGBoost predictions across our Postgres replicas.","title":"PgCat"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#postgres-replicas","text":"Scaling Postgres reads is pretty straight forward. If more read queries are coming in, we add a replica to serve the increased load. If the load is decreasing, we remove a replica to save money. The data is replicated from the primary, so all replicas are identical, and all of them can serve any query, or in our case, an XGBoost prediction. PgCat can dynamically add and remove replicas from its config without disconnecting clients, so we can add and remove replicas as needed, without downtime.","title":"Postgres Replicas"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#parallelizing-xgboost","text":"Scaling XGBoost predictions is a little bit more interesting. XGBoost cannot serve predictions concurrently because of internal data structure locks. This is common to many other machine learning algorithms as well, because making predictions can temporarily modify internal components of the model. PostgresML bypasses that limitation because of how Postgres itself handles concurrency: PostgresML concurrency PostgreSQL uses the fork/multiprocessing architecture to serve multiple clients concurrently: each new client connection becomes an independent OS process. During connection startup, PostgresML loads all models inside the process' memory space. This means that each connection has its own copy of the XGBoost model and PostgresML ends up serving multiple XGBoost predictions at the same time without any lock contention.","title":"Parallelizing XGBoost"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#results","text":"We ran over a 100 different benchmarks, by changing the number of clients, poolers, replicas, and XGBoost predictions we requested. The benchmarks were meant to test the limits of each configuration, and what remediations were needed in each scenario. Our raw data is available below . One of the tests we ran used 1,000 clients, which were connected to 1, 2, and 5 replicas. The results were exactly what we expected.","title":"Results"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#linear-scaling","text":"Both latency and throughput, the standard measurements of system performance, scale mostly linearly with the number of replicas. Linear scaling is the north star of all horizontally scalable systems, and most are not able to achieve it because of increasing complexity that comes with synchronization. Our architecture shares nothing and requires no synchronization. The replicas don't talk to each other and the poolers don't either. Every component has the knowledge it needs (through configuration) to do its job, and they do it well. The most impressive result is serving close to a million predictions with an average latency of less than 1ms. You might notice though that 950160.7 isn't quite one million, and that's true. We couldn't reach one million with 1000 clients, so we increased to 2000 and got our magic number: 1,021,692.7 req/sec , with an average latency of 1.7ms .","title":"Linear Scaling"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#batching-predictions","text":"Batching is a proven method to optimize performance. If you need to get several data points, batch the requests into one query, and it will run faster than making individual requests. We should precede this result by stating that PostgresML does not yet have a batch prediction API as such. Our pgml.predict() function can predict multiple points, but we haven't implemented a query pattern to pass multiple rows to that function at the same time. Once we do, based on our tests, we should see a substantial increase in batch prediction performance. Regardless of that limitation, we still managed to get better results by batching queries together since Postgres needed to do less query parsing and searching, and we saved on network round trip time as well. If batching did not work at all, we would see a linear increase in latency and a linear decrease in throughput. That did not happen; instead, we got a 1.5x improvement by batching 5 predictions together, and a 1.2x improvement by batching 20. A modest success, but a success nonetheless.","title":"Batching Predictions"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#graceful-degradation-and-queuing","text":"All systems, at some point in their lifetime, will come under more load than they were designed for; what happens then is an important feature (or bug) of their design. Horizontal scaling is never immediate: it takes a bit of time to spin up additional hardware to handle the load. It can take a second, or a minute, depending on availability, but in both cases, existing resources need to serve traffic the best way they can. We were hoping to test PostgresML to its breaking point, but we couldn't quite get there. As the load (number of clients) increased beyond provisioned capacity, the only thing we saw was a gradual increase in latency. Throughput remained roughly the same. This gradual latency increase was caused by simple queuing: the replicas couldn't serve requests concurrently, so the requests had to patiently wait in the poolers. \"What's taking so long over there!?\" Among many others, this is a very important feature of any proxy: it's a FIFO queue (first in, first out). If the system is underutilized, queue size is 0 and all requests are served as quickly as physically possible. If the system is overutilized, the queue size increases, holds as the number of requests stabilizes, and decreases back to 0 as the system is scaled up to accommodate new traffic. Queueing overall is not desirable, but it's a feature, not a bug. While autoscaling spins up an additional replica, the app continues to work, although a few milliseconds slower, which is a good trade off for not overspending on hardware. As the demand on PostgresML increases, the system gracefully handles the load. If the number of replicas stays the same, latency slowly increases, all the while remaining well below acceptable ranges. Throughput holds as well, as increasing number of clients evenly split available resources. If we increase the number of replicas, latency decreases and throughput increases, as the number of clients increases in parallel. We get the best result with 5 replicas, but this number is variable and can be changed as needs for latency compete with cost.","title":"Graceful Degradation and Queuing"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#whats-next","text":"Horizontal scaling and high availability are fascinating topics in software engineering. Needing to serve 1 million predictions per second is rare, but having the ability to do that, and more if desired, is an important aspect for any new system. The next challenge for us is to scale writes horizontally. In the database world, this means sharding the database into multiple separate machines using a hashing function, and automatically routing both reads and writes to the right shards. There are many possible solutions on the market for this already, e.g. Citus and Foreign Data Wrappers, but none are as horizontally scalable as we like, although we will incorporate them into our architecture until we build the one we really want. For that purpose, we're building our own open source Postgres proxy which we discussed earlier in the article. As we progress further in our journey, we'll be adding more features and performance improvements. By combining PgCat with PostgresML, we are aiming to build the next generation of machine learning infrastructure that can power anything from tiny startups to unicorns and massive enterprises, without the data ever leaving our favorite database.","title":"What's Next"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#methodology","text":"","title":"Methodology"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#ml","text":"This time, we used an XGBoost model with 100 trees: SELECT * FROM pgml . train ( 'flights' , task => 'regression' , relation_name => 'flights_mat_3' , y_column_name => 'depdelayminutes' , algorithm => 'xgboost' , hyperparams => '{\"n_estimators\": 100 }' , runtime => 'rust' ); and fetched our predictions the usual way: SELECT pgml . predict ( 'flights' , ARRAY [ year , quarter , month , distance , dayofweek , dayofmonth , flight_number_operating_airline , originairportid , destairportid , flight_number_marketing_airline , departure ] ) AS prediction FROM flights_mat_3 LIMIT :limit ; where :limit is the batch size of 1, 5, and 20.","title":"ML"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#model","text":"The model is roughly the same as the one we used in our previous post , with just one extra feature added, which improved R 2 a little bit.","title":"Model"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#hardware","text":"","title":"Hardware"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#client","text":"The client was a c5n.4xlarge box on EC2. We chose the c5n class to have the 100 GBit NIC, since we wanted it to saturate our network as much as possible. Thousands of clients were simulated using pgbench .","title":"Client"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#pgcat-pooler","text":"PgCat, written in asynchronous Rust, was running on c5.xlarge machines (4 vCPUs, 8GB RAM) with 4 Tokio workers. We used between 1 and 35 machines, and scaled them in increments of 5-20 at a time. The pooler did a decent amount of work around parsing queries, making sure they are read-only SELECT s, and routing them, at random, to replicas. If any replica was down for any reason, it would route around it to remaining machines.","title":"PgCat Pooler"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#postgres-replicas_1","text":"Postgres replicas were running on c5.9xlarge machines with 36 vCPUs and 72 GB of RAM. The hot dataset fits entirely in memory. The servers were intentionally saturated to maximum capacity before scaling up to test queuing and graceful degradation of performance.","title":"Postgres Replicas"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#raw-results","text":"Raw latency data is available here and raw throughput data is available here .","title":"Raw Results"},{"location":"blog/scaling-postgresml-to-one-million-requests-per-second/#call-to-early-adopters","text":"PostgresML and PgCat are free and open source. If your organization can benefit from simplified and fast machine learning, get in touch! We can help deploy PostgresML internally, and collaborate on new and existing features. Join our Discord or email us! Many thanks and \u2764\ufe0f to all those who are supporting this endeavor. We\u2019d love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. You can show your support by starring us on our Github .","title":"Call to Early Adopters"},{"location":"blog/which-database-that-is-the-question/","text":"Which Database, That is the Question \u00b6 Lev Kokotov September 1, 2022 Choosing a database for your product sounds like a hard problem. These days, we engineers have an abundance of choice, which makes this decision harder than it should be. Let's look at a few options. Redis \u00b6 Redis is not really a database. It's a key-value store that keeps your data in memory. If Redis accidentally restarts, due to power failure for example, you'll lose some or all of your keys, depending on configuration. Don't get me wrong, I love Redis; it's fast, it has cool data structures like sets and HyperLogLog, and it can even horizontally scale most of its features in cluster mode. For this and many of its other properties, it is the key-value store of choice for high throughput systems like ML feature stores, job queues, Twitter and Twitch 1 . None of those systems however expect your data to be safe. In fact, if it's gone, your product should be able to go on like nothing really happened. For those deployments, machine learning and other features it powers, are treated as just a nice to have. ScyllaDB (and friends) \u00b6 Scylla is the new kid on the block, at least as far as databases go. It's been around for 6 years, but it's making headlines with large deployments like Discord 2 and Expedia 3 . It takes the idea that key-value stores can be fast, and if you have a power outage, your data remains safe and replicated across availability zones of your favorite cloud. To top it all off, it uses Cassandra's SQL syntax and client/server protocol, so you might think that it can actually power your business-critical systems. At its heart though Scylla is still a key-value store. We can put things in, but getting them back out in a way that makes sense will still prove to be a challenge. It does have secondary indexes, so if you want to find your users by email instead of by primary key one day, you still might be able to, it'll just be slower. Ultimately though, with no join support or foreign keys, Scylla tables, much like Redis keys, are isolated from each other. So finding out how many of your customers in San Francisco have ordered your best selling shoes will require an expensive data warehouse instead of a GROUP BY city ORDER BY COUNT(*) . You might think DynamoDB, MongoDB, and all other SQL look-alikes 6 are better, but they are all forgetting one important fact. Denormalized Data is DOA \u00b6 Relationships are the foundation of everything, ranging from personal well-being to having a successful business. Most problems we'll run into involve understanding how entities work together. Which users logged in today? That's a relationship between users, logins and time. How many users bought our top selling product? How much did that product cost to deliver? Those are relationships between prices, products, date ranges, users, and orders. If we denormalize this data, by either flattening it into a key-value store or just storing it in independent tables in different databases, we lose the ability to query it in interesting ways, and if we lose that, we stop understanding our business. PostgreSQL \u00b6 Okay, that was a bit of a spoiler. When looking at our options, one has to wonder, why can't we have our cake and eat it too? That's a bad analogy though, because we're not asking for that much and we certainly can have it. When it comes to reliability, there is no better option. PostgreSQL does not lose data. In fact, it has several layers of failure checks 4 to ensure that bytes in equals bytes out. When installed on modern SSDs, PostgreSQL can serve 100k+ write transactions per second without breaking a sweat, and push 1GB/second write throughput. When it comes to reads, it can serve datasets going into petabytes and is horizontally scalable into millions of reads per second. That's better than web scale 5 . Most importantly though, Postgres allows you to understand your data and your business. With just a few joins, you can connect users to orders to chargebacks and to your website visits. You don't need a data warehouse, Spark, Cassandra, large pipelines to make them all work together or data validation scripts. You can read, write and understand straight from the source. In Comes Machine Learning \u00b6 Understanding your business is good, but what if you could improve it too? Most are tempted to throw spaghetti against the wall (and that's okay), but machine learning allows for a more scientific approach. Traditionally, ML has been tough to use with modern data architectures: using key-value databases makes data virtually inaccessible in bulk. With PostgresML though, you can train an XGBoost model directly on your orders table with a single SQL query: SELECT pgml . train ( 'Orders Likely To Be Returned' , -- name of your model 'regression' , -- objective (regression or classification) 'public.orders' , -- table 'refunded' , -- label (what are we predicting) 'xgboost' -- algorithm ); SELECT pgml . predict ( 'Orders Likely To Be Returned' , ARRAY [ orders . * ]) AS refund_likelihood , orders . * FROM orders ORDER BY refund_likelyhood DESC LIMIT 100 ; Checkmate. Check out our free PostgresML tutorials if you haven't already, and become a machine learning engineer with just a few lines of SQL. Enterprise Redis Twitch Case Study \u21a9 Discord Chooses ScyllaDB as Its Core Storage Layer \u21a9 Expedia Group: Our Migration Journey to ScyllaDB \u21a9 PostgreSQL WAL \u21a9 Web scale \u21a9 SQL to MongoDB Mapping Chart \u21a9","title":"Which Database, That is the Question"},{"location":"blog/which-database-that-is-the-question/#which-database-that-is-the-question","text":"Lev Kokotov September 1, 2022 Choosing a database for your product sounds like a hard problem. These days, we engineers have an abundance of choice, which makes this decision harder than it should be. Let's look at a few options.","title":"Which Database, That is the Question"},{"location":"blog/which-database-that-is-the-question/#redis","text":"Redis is not really a database. It's a key-value store that keeps your data in memory. If Redis accidentally restarts, due to power failure for example, you'll lose some or all of your keys, depending on configuration. Don't get me wrong, I love Redis; it's fast, it has cool data structures like sets and HyperLogLog, and it can even horizontally scale most of its features in cluster mode. For this and many of its other properties, it is the key-value store of choice for high throughput systems like ML feature stores, job queues, Twitter and Twitch 1 . None of those systems however expect your data to be safe. In fact, if it's gone, your product should be able to go on like nothing really happened. For those deployments, machine learning and other features it powers, are treated as just a nice to have.","title":"Redis"},{"location":"blog/which-database-that-is-the-question/#scylladb-and-friends","text":"Scylla is the new kid on the block, at least as far as databases go. It's been around for 6 years, but it's making headlines with large deployments like Discord 2 and Expedia 3 . It takes the idea that key-value stores can be fast, and if you have a power outage, your data remains safe and replicated across availability zones of your favorite cloud. To top it all off, it uses Cassandra's SQL syntax and client/server protocol, so you might think that it can actually power your business-critical systems. At its heart though Scylla is still a key-value store. We can put things in, but getting them back out in a way that makes sense will still prove to be a challenge. It does have secondary indexes, so if you want to find your users by email instead of by primary key one day, you still might be able to, it'll just be slower. Ultimately though, with no join support or foreign keys, Scylla tables, much like Redis keys, are isolated from each other. So finding out how many of your customers in San Francisco have ordered your best selling shoes will require an expensive data warehouse instead of a GROUP BY city ORDER BY COUNT(*) . You might think DynamoDB, MongoDB, and all other SQL look-alikes 6 are better, but they are all forgetting one important fact.","title":"ScyllaDB (and friends)"},{"location":"blog/which-database-that-is-the-question/#denormalized-data-is-doa","text":"Relationships are the foundation of everything, ranging from personal well-being to having a successful business. Most problems we'll run into involve understanding how entities work together. Which users logged in today? That's a relationship between users, logins and time. How many users bought our top selling product? How much did that product cost to deliver? Those are relationships between prices, products, date ranges, users, and orders. If we denormalize this data, by either flattening it into a key-value store or just storing it in independent tables in different databases, we lose the ability to query it in interesting ways, and if we lose that, we stop understanding our business.","title":"Denormalized Data is DOA"},{"location":"blog/which-database-that-is-the-question/#postgresql","text":"Okay, that was a bit of a spoiler. When looking at our options, one has to wonder, why can't we have our cake and eat it too? That's a bad analogy though, because we're not asking for that much and we certainly can have it. When it comes to reliability, there is no better option. PostgreSQL does not lose data. In fact, it has several layers of failure checks 4 to ensure that bytes in equals bytes out. When installed on modern SSDs, PostgreSQL can serve 100k+ write transactions per second without breaking a sweat, and push 1GB/second write throughput. When it comes to reads, it can serve datasets going into petabytes and is horizontally scalable into millions of reads per second. That's better than web scale 5 . Most importantly though, Postgres allows you to understand your data and your business. With just a few joins, you can connect users to orders to chargebacks and to your website visits. You don't need a data warehouse, Spark, Cassandra, large pipelines to make them all work together or data validation scripts. You can read, write and understand straight from the source.","title":"PostgreSQL"},{"location":"blog/which-database-that-is-the-question/#in-comes-machine-learning","text":"Understanding your business is good, but what if you could improve it too? Most are tempted to throw spaghetti against the wall (and that's okay), but machine learning allows for a more scientific approach. Traditionally, ML has been tough to use with modern data architectures: using key-value databases makes data virtually inaccessible in bulk. With PostgresML though, you can train an XGBoost model directly on your orders table with a single SQL query: SELECT pgml . train ( 'Orders Likely To Be Returned' , -- name of your model 'regression' , -- objective (regression or classification) 'public.orders' , -- table 'refunded' , -- label (what are we predicting) 'xgboost' -- algorithm ); SELECT pgml . predict ( 'Orders Likely To Be Returned' , ARRAY [ orders . * ]) AS refund_likelihood , orders . * FROM orders ORDER BY refund_likelyhood DESC LIMIT 100 ; Checkmate. Check out our free PostgresML tutorials if you haven't already, and become a machine learning engineer with just a few lines of SQL. Enterprise Redis Twitch Case Study \u21a9 Discord Chooses ScyllaDB as Its Core Storage Layer \u21a9 Expedia Group: Our Migration Journey to ScyllaDB \u21a9 PostgreSQL WAL \u21a9 Web scale \u21a9 SQL to MongoDB Mapping Chart \u21a9","title":"In Comes Machine Learning"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/","text":"Python vs. PostgresML \u00b6 PostgresML \u00b6 cargo pgrx run --release Schema \u00b6 CREATE TABLE public . flights_delay_mat ( depdelayminutes real , year real NOT NULL , quarter real NOT NULL , month real NOT NULL , distance real NOT NULL , dayofweek real NOT NULL , dayofmonth real NOT NULL , flight_number_operating_airline real NOT NULL , originairportid real NOT NULL , destairportid real NOT NULL , tail_number real NOT NULL ); Data \u00b6 curl -L -o ~/Desktop/flights.csv https://static.postgresml.org/benchmarks/flights.csv \\ copy flights_delay_mat FROM '~/Desktop/flights.csv' CSV HEADER ; CREATE INDEX ON flights_delay_mat USING btree ( originairportid , year , month , dayofmonth ); Train \u00b6 SELECT * FROM pgml . train ( 'r2' , 'regression' , 'flights_delay_mat' , 'depdelayminutes' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); Test \u00b6 pgbench -f pgbench.sql -p 28813 -h 127 .0.0.1 pgml -t 1000 -c 10 -j 10 Python + Redis \u00b6 Setup \u00b6 Inside virtualenv, pip install -r requirements.txt Train \u00b6 python train.py Feature Store \u00b6 Install and start Redis if you don't have it already. curl -L -o ~/Desktop/flights_sub.csv https://static.postgresml.org/benchmarks/flights_sub.csv python load_redis.py Test \u00b6 OMP_NUM_THREADS = 2 gunicorn predict:app -w 5 -t 2 OMP_NUM_THREADS controls XGBoost (it's using OpenMP) parallelization. In a separate tab (install Apache Bench first): ab -n 10000 -c 10 -T application/json -k -p ab.txt http://localhost:8000/","title":"Python vs. PostgresML"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#python-vs-postgresml","text":"","title":"Python vs. PostgresML"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#postgresml","text":"cargo pgrx run --release","title":"PostgresML"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#schema","text":"CREATE TABLE public . flights_delay_mat ( depdelayminutes real , year real NOT NULL , quarter real NOT NULL , month real NOT NULL , distance real NOT NULL , dayofweek real NOT NULL , dayofmonth real NOT NULL , flight_number_operating_airline real NOT NULL , originairportid real NOT NULL , destairportid real NOT NULL , tail_number real NOT NULL );","title":"Schema"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#data","text":"curl -L -o ~/Desktop/flights.csv https://static.postgresml.org/benchmarks/flights.csv \\ copy flights_delay_mat FROM '~/Desktop/flights.csv' CSV HEADER ; CREATE INDEX ON flights_delay_mat USING btree ( originairportid , year , month , dayofmonth );","title":"Data"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#train","text":"SELECT * FROM pgml . train ( 'r2' , 'regression' , 'flights_delay_mat' , 'depdelayminutes' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' );","title":"Train"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#test","text":"pgbench -f pgbench.sql -p 28813 -h 127 .0.0.1 pgml -t 1000 -c 10 -j 10","title":"Test"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#python-redis","text":"","title":"Python + Redis"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#setup","text":"Inside virtualenv, pip install -r requirements.txt","title":"Setup"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#train_1","text":"python train.py","title":"Train"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#feature-store","text":"Install and start Redis if you don't have it already. curl -L -o ~/Desktop/flights_sub.csv https://static.postgresml.org/benchmarks/flights_sub.csv python load_redis.py","title":"Feature Store"},{"location":"blog/benchmarks/python_microservices_vs_postgresml/#test_1","text":"OMP_NUM_THREADS = 2 gunicorn predict:app -w 5 -t 2 OMP_NUM_THREADS controls XGBoost (it's using OpenMP) parallelization. In a separate tab (install Apache Bench first): ab -n 10000 -c 10 -T application/json -k -p ab.txt http://localhost:8000/","title":"Test"},{"location":"developer_guide/overview/","text":"Contributing \u00b6 Thank you for your interest in contributing to PostgresML! We are an open source, MIT licensed project, and we welcome all contributions, including bug fixes, features, documentation, typo fixes, and Github stars. Our project consists of three (3) applications: Postgres extension ( pgml-extension ) Dashboard web app ( pgml-dashboard ) Documentation ( pgml-docs ) The development environment for each differs slightly, but overall we use Python, Rust, and PostgreSQL, so as long as you have all of those installed, the setup should be straight forward. Build Dependencies \u00b6 Install the latest Rust compiler from rust-lang.org . Install a modern version of CMake. Install PostgreSQL development headers and other dependencies: export POSTGRES_VERSION=15 sudo apt-get update && \\ sudo apt-get install -y \\ postgresql-server-dev-${POSTGRES_VERSION} \\ bison \\ build-essential \\ clang \\ cmake \\ flex \\ libclang-dev \\ libopenblas-dev \\ libpython3-dev \\ libreadline-dev \\ libssl-dev \\ pkg-config \\ python3-dev Install the Python dependencies If your system comes with Python 3.6 or lower, you'll need to install libpython3.7-dev or higher. You can get it from ppa:deadsnakes/ppa : sudo add-apt-repository ppa:deadsnakes/ppa && \\ sudo apt update && sudo apt install -y libpython3.7-dev With Python 3.7+ installed, install the package dependencies sudo pip3 install -r pgml-extension/requirements.txt Clone our git repository: git clone https://github.com/postgresml/postgresml && \\ cd postgresml && \\ git submodule update --init --recursive && \\ Postgres extension \u00b6 PostgresML is a Rust extension written with tcdi/pgrx crate. Local development therefore requires the latest Rust compiler and PostgreSQL development headers and libraries. The extension code is located in: cd pgml-extension/ You'll need to install basic dependencies Once there, you can initialize pgrx and get going: Pgrx command line and environments \u00b6 cargo install cargo-pgrx --version \"0.8.2\" --locked && \\ cargo pgrx init # This will take a few minutes Update postgresql.conf \u00b6 pgrx uses Postgres 15 by default. Since pgml is using shared memory, you need to add it to shared_preload_libraries in postgresql.conf which, for pgrx , is located in ~/.pgrx/data-15/postgresql.conf . shared_preload_libraries = 'pgml' # (change requires restart) Run the unit tests cargo pgrx test Run the integration tests: cargo pgrx run --release psql -h localhost -p 28813 -d pgml -f tests/test.sql -P pager Run an interactive psql session cargo pgrx run Create the extension in your database: CREATE EXTENSION pgml; That's it, PostgresML is ready. You can validate the installation by running: SQL Output SELECT pgml . version (); postgres=# select pgml.version(); version ------------------- 2.4.8 (1 row) Basic extension usage: SELECT * FROM pgml . load_dataset ( 'diabetes' ); SELECT * FROM pgml . train ( 'Project name' , 'regression' , 'pgml.diabetes' , 'target' , 'xgboost' ); SELECT target , pgml . predict ( 'Project name' , ARRAY [ age , sex , bmi , bp , s1 , s2 , s3 , s4 , s5 , s6 ]) FROM pgml . diabetes LIMIT 10 ; If you're going to run the dashboard against this database to develop both the extension and By default, the extension is built without CUDA support for XGBoost and LightGBM. You'll need to install CUDA locally to build and enable the cuda feature for cargo. CUDA can be downloaded here . CUDACXX=/usr/local/cuda/bin/nvcc cargo pgrx run --release --features pg15,python,cuda If you ever want to reset the environment, simply spin up the database with cargo pgrx run and drop the extension and metadata tables: DROP EXTENSION IF EXISTS pgml CASCADE ; DROP SCHEMA IF EXISTS pgml CASCADE ; CREATE EXTENSION pgml ; Packaging \u00b6 This requires Docker. Once Docker is installed, you can run: bash build_extension.sh which will produce a .deb file in the current directory (this will take about 20 minutes). The deb file can be installed with apt-get , for example: apt-get install ./postgresql-pgml-12_0.0.4-ubuntu20.04-amd64.deb which will take care of installing its dependencies as well. Make sure to run this as root and not with sudo. Run the dashboard \u00b6 The dashboard is a web app that can be run against any Postgres database with the extension installed. There is a Dockerfile included with the source code if you wish to run it as a container. The dashboard requires a Postgres database with the pgml-extension to generate the core schema. See that subproject for developer setup. We develop and test this web application on Linux, OS X, and Windows using WSL2. Basic installation can be achieved with: Clone the repo (if you haven't already for the extension): cd postgresml/pgml-dashboard Set the DATABASE_URL environment variable, for example to a running interactive cargo pgrx run session started previously: export DATABASE_URL=postgres://localhost:28815/pgml Run migrations sqlx migrate run Run tests: cargo test Incremental and automatic compilation for development cycles is supported with: cargo watch --exec run The dashboard can be packaged for distribution. You'll need to copy the static files along with the target/release directory to your server. Documentation app \u00b6 The documentation app (you're using it right now) is using MkDocs. cd pgml-docs/ Once there, you can set up a virtual environment and get going: python3 -m venv venv source venv/bin/activate pip install -r requirements.txt python -m mkdocs serve General \u00b6 We are a cross-platform team, some of us use WSL and some use Linux or Mac OS. Keeping that in mind, it's good to use common line endings for all files to avoid production errors, e.g. broken Bash scripts. The project is presently using Unix line endings .","title":"Developer Overview"},{"location":"developer_guide/overview/#contributing","text":"Thank you for your interest in contributing to PostgresML! We are an open source, MIT licensed project, and we welcome all contributions, including bug fixes, features, documentation, typo fixes, and Github stars. Our project consists of three (3) applications: Postgres extension ( pgml-extension ) Dashboard web app ( pgml-dashboard ) Documentation ( pgml-docs ) The development environment for each differs slightly, but overall we use Python, Rust, and PostgreSQL, so as long as you have all of those installed, the setup should be straight forward.","title":"Contributing"},{"location":"developer_guide/overview/#build-dependencies","text":"Install the latest Rust compiler from rust-lang.org . Install a modern version of CMake. Install PostgreSQL development headers and other dependencies: export POSTGRES_VERSION=15 sudo apt-get update && \\ sudo apt-get install -y \\ postgresql-server-dev-${POSTGRES_VERSION} \\ bison \\ build-essential \\ clang \\ cmake \\ flex \\ libclang-dev \\ libopenblas-dev \\ libpython3-dev \\ libreadline-dev \\ libssl-dev \\ pkg-config \\ python3-dev Install the Python dependencies If your system comes with Python 3.6 or lower, you'll need to install libpython3.7-dev or higher. You can get it from ppa:deadsnakes/ppa : sudo add-apt-repository ppa:deadsnakes/ppa && \\ sudo apt update && sudo apt install -y libpython3.7-dev With Python 3.7+ installed, install the package dependencies sudo pip3 install -r pgml-extension/requirements.txt Clone our git repository: git clone https://github.com/postgresml/postgresml && \\ cd postgresml && \\ git submodule update --init --recursive && \\","title":"Build Dependencies"},{"location":"developer_guide/overview/#postgres-extension","text":"PostgresML is a Rust extension written with tcdi/pgrx crate. Local development therefore requires the latest Rust compiler and PostgreSQL development headers and libraries. The extension code is located in: cd pgml-extension/ You'll need to install basic dependencies Once there, you can initialize pgrx and get going:","title":"Postgres extension"},{"location":"developer_guide/overview/#pgrx-command-line-and-environments","text":"cargo install cargo-pgrx --version \"0.8.2\" --locked && \\ cargo pgrx init # This will take a few minutes","title":"Pgrx command line and environments"},{"location":"developer_guide/overview/#update-postgresqlconf","text":"pgrx uses Postgres 15 by default. Since pgml is using shared memory, you need to add it to shared_preload_libraries in postgresql.conf which, for pgrx , is located in ~/.pgrx/data-15/postgresql.conf . shared_preload_libraries = 'pgml' # (change requires restart) Run the unit tests cargo pgrx test Run the integration tests: cargo pgrx run --release psql -h localhost -p 28813 -d pgml -f tests/test.sql -P pager Run an interactive psql session cargo pgrx run Create the extension in your database: CREATE EXTENSION pgml; That's it, PostgresML is ready. You can validate the installation by running: SQL Output SELECT pgml . version (); postgres=# select pgml.version(); version ------------------- 2.4.8 (1 row) Basic extension usage: SELECT * FROM pgml . load_dataset ( 'diabetes' ); SELECT * FROM pgml . train ( 'Project name' , 'regression' , 'pgml.diabetes' , 'target' , 'xgboost' ); SELECT target , pgml . predict ( 'Project name' , ARRAY [ age , sex , bmi , bp , s1 , s2 , s3 , s4 , s5 , s6 ]) FROM pgml . diabetes LIMIT 10 ; If you're going to run the dashboard against this database to develop both the extension and By default, the extension is built without CUDA support for XGBoost and LightGBM. You'll need to install CUDA locally to build and enable the cuda feature for cargo. CUDA can be downloaded here . CUDACXX=/usr/local/cuda/bin/nvcc cargo pgrx run --release --features pg15,python,cuda If you ever want to reset the environment, simply spin up the database with cargo pgrx run and drop the extension and metadata tables: DROP EXTENSION IF EXISTS pgml CASCADE ; DROP SCHEMA IF EXISTS pgml CASCADE ; CREATE EXTENSION pgml ;","title":"Update postgresql.conf"},{"location":"developer_guide/overview/#packaging","text":"This requires Docker. Once Docker is installed, you can run: bash build_extension.sh which will produce a .deb file in the current directory (this will take about 20 minutes). The deb file can be installed with apt-get , for example: apt-get install ./postgresql-pgml-12_0.0.4-ubuntu20.04-amd64.deb which will take care of installing its dependencies as well. Make sure to run this as root and not with sudo.","title":"Packaging"},{"location":"developer_guide/overview/#run-the-dashboard","text":"The dashboard is a web app that can be run against any Postgres database with the extension installed. There is a Dockerfile included with the source code if you wish to run it as a container. The dashboard requires a Postgres database with the pgml-extension to generate the core schema. See that subproject for developer setup. We develop and test this web application on Linux, OS X, and Windows using WSL2. Basic installation can be achieved with: Clone the repo (if you haven't already for the extension): cd postgresml/pgml-dashboard Set the DATABASE_URL environment variable, for example to a running interactive cargo pgrx run session started previously: export DATABASE_URL=postgres://localhost:28815/pgml Run migrations sqlx migrate run Run tests: cargo test Incremental and automatic compilation for development cycles is supported with: cargo watch --exec run The dashboard can be packaged for distribution. You'll need to copy the static files along with the target/release directory to your server.","title":"Run the dashboard"},{"location":"developer_guide/overview/#documentation-app","text":"The documentation app (you're using it right now) is using MkDocs. cd pgml-docs/ Once there, you can set up a virtual environment and get going: python3 -m venv venv source venv/bin/activate pip install -r requirements.txt python -m mkdocs serve","title":"Documentation app"},{"location":"developer_guide/overview/#general","text":"We are a cross-platform team, some of us use WSL and some use Linux or Mac OS. Keeping that in mind, it's good to use common line endings for all files to avoid production errors, e.g. broken Bash scripts. The project is presently using Unix line endings .","title":"General"},{"location":"gym/quick_start/","text":".md-content video, .md-content img { max-width: 90%; margin: 2em 5%; } Quick Start \u00b6 PostgresML is easy to get started with. If you haven't already, sign up for our Gym to get a free hosted PostgresML instance you can use to follow this tutorial. You can also run one yourself by following the instructions in our Github repo. Try PostgresML Once you have your PostgresML instance running, we'll be ready to get started. Get data \u00b6 The first part of machine learning is getting your data in a format you can use. That's usually the hardest part, but thankfully we have a few example datasets we can use. To load one of them, navigate to the IDE tab and run this query: SELECT * FROM pgml . load_dataset ( 'diabetes' ); You should see something like this: We have more example Scikit datasets available: iris (classification), digits (classification), wine (regression), To load them into PostgresML, use the same function above with the desired dataset name as parameter. They will become available in the pgml schema as pgml.iris , pgml.digits and pgml.wine respectively. Browse data \u00b6 The SQL editor you just used can run arbitrary queries on the PostgresML instance. For example, if we want to see what dataset we just loaded looks like, we can run: SELECT * FROM pgml . diabetes LIMIT 5 ; The diabetes dataset is a toy (small, not realistic) dataset published by Scikit Learn. It contains ten feature columns and one label column: Column Description Data type age The age of the patient (in years). float sex The sex of the patient (normalized). float bmi The BMI Body Mass index. float bp Average blood pressure of the patient. float s1 Total serum cholesterol. float s2 Low-density lipoproteins. float s3 High-density lipoproteins. float s4 Total cholesterol / HDL. float s5 Possibly log of serum triglycerides level. float s6 Blood sugar level. float target Quantitative measure of disease progression one year after baseline. float This dataset is not realistic because all data is perfectly arranged and normalized, which won't be the case with most real world datasets you'll run into, but it's perfect for our quick tutorial. Alright, we're ready to do some machine learning! First project \u00b6 PostgresML organizes itself into projects. A project is just a name for model(s) trained on a particular dataset. Let's create our first project by training an XGBoost regression model on our diabetes dataset. Using the IDE, run: SELECT * FROM pgml . train ( 'My First Project' , task => 'regression' , relation_name => 'pgml.diabetes' , y_column_name => 'target' , algorithm => 'xgboost' ); You should see this: By executing pmgl.train() we did the following: created a project called \"My First Project\", snapshotted the table pgml.diabetes thus making the experiment reproducible (in case data changes, as it happens in the real world), trained an XGBoost regression model on the data contained in the pgml.diabetes table using the column target as the label, deployed the model into production. We're ready to predict novel data points! Inference \u00b6 Inference is the act of predicting labels that we haven't necessarily used in training. That's the whole point of machine learning really: predict something we haven't seen before. Let's try and predict some new values. Using the IDE, run: SELECT pgml . predict ( 'My First Project' , ARRAY [ 0 . 06 , -- age 0 . 05 , -- sex 0 . 05 , -- bmi - 0 . 0056 , -- bp 0 . 012191 , -- s1 - 0 . 043401 , -- s2 0 . 034309 , -- s3 - 0 . 031938 , -- s4 - 0 . 061988 , --s5 - 0 . 031988 -- s6 ] ) AS prediction ; You should see something like this: The prediction column represents the possible value of the target column given the new features we just passed into the pgml.predict() function. You can just as easily predict multiple points and compare them to the actual labels in the dataset: SELECT pgml . predict ( 'My First Project 2' , ARRAY [ age , sex , bmi , bp , s1 , s3 , s3 , s4 , s5 , s6 ]), target FROM pgml . diabetes LIMIT 10 ; Sometimes the model will be pretty close, but sometimes it will be way off. That's why we'll be training several of them and comparing them next. Browse around \u00b6 By creating our first project, we made the Dashboard a little bit more interesting. This is how the pgml.diabetes snapshot we just created looks like: As you can see, we automatically performed some analysis on the data. Visualizing the data is important to understand how it could potentially behave given different models, and maybe even predict how it could evolve in the future. XGBoost is a good algorithm, but what if there are better ones? Let's try training a few more using the IDE. Run these one at a time: -- Simple linear regression. SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'linear' ); -- The Lasso (much fancier linear regression). SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'lasso' ); If you navigate to the Models tab, you should see all three algorithms you just trained: Huh, apparently XGBoost isn't as good we originally thought! In this case, a simple linear regression did significantly better than all the others. It's hard to know which algorithm will perform best given a dataset; even experienced machine learning engineers get this one wrong. With PostgresML, you needn't worry: you can train all of them and see which one does best for your data. PostgresML will automatically use the best one for inference. Conclusion \u00b6 Congratulations on becoming a Machine Learning engineer. If you thought ML was scary or mysterious, we hope that this small tutorial made it a little bit more approachable. This is the first of many tutorials we'll publish, so stay tuned. Happy machine learning!","title":"Quick start"},{"location":"gym/quick_start/#quick-start","text":"PostgresML is easy to get started with. If you haven't already, sign up for our Gym to get a free hosted PostgresML instance you can use to follow this tutorial. You can also run one yourself by following the instructions in our Github repo. Try PostgresML Once you have your PostgresML instance running, we'll be ready to get started.","title":"Quick Start"},{"location":"gym/quick_start/#get-data","text":"The first part of machine learning is getting your data in a format you can use. That's usually the hardest part, but thankfully we have a few example datasets we can use. To load one of them, navigate to the IDE tab and run this query: SELECT * FROM pgml . load_dataset ( 'diabetes' ); You should see something like this: We have more example Scikit datasets available: iris (classification), digits (classification), wine (regression), To load them into PostgresML, use the same function above with the desired dataset name as parameter. They will become available in the pgml schema as pgml.iris , pgml.digits and pgml.wine respectively.","title":"Get data"},{"location":"gym/quick_start/#browse-data","text":"The SQL editor you just used can run arbitrary queries on the PostgresML instance. For example, if we want to see what dataset we just loaded looks like, we can run: SELECT * FROM pgml . diabetes LIMIT 5 ; The diabetes dataset is a toy (small, not realistic) dataset published by Scikit Learn. It contains ten feature columns and one label column: Column Description Data type age The age of the patient (in years). float sex The sex of the patient (normalized). float bmi The BMI Body Mass index. float bp Average blood pressure of the patient. float s1 Total serum cholesterol. float s2 Low-density lipoproteins. float s3 High-density lipoproteins. float s4 Total cholesterol / HDL. float s5 Possibly log of serum triglycerides level. float s6 Blood sugar level. float target Quantitative measure of disease progression one year after baseline. float This dataset is not realistic because all data is perfectly arranged and normalized, which won't be the case with most real world datasets you'll run into, but it's perfect for our quick tutorial. Alright, we're ready to do some machine learning!","title":"Browse data"},{"location":"gym/quick_start/#first-project","text":"PostgresML organizes itself into projects. A project is just a name for model(s) trained on a particular dataset. Let's create our first project by training an XGBoost regression model on our diabetes dataset. Using the IDE, run: SELECT * FROM pgml . train ( 'My First Project' , task => 'regression' , relation_name => 'pgml.diabetes' , y_column_name => 'target' , algorithm => 'xgboost' ); You should see this: By executing pmgl.train() we did the following: created a project called \"My First Project\", snapshotted the table pgml.diabetes thus making the experiment reproducible (in case data changes, as it happens in the real world), trained an XGBoost regression model on the data contained in the pgml.diabetes table using the column target as the label, deployed the model into production. We're ready to predict novel data points!","title":"First project"},{"location":"gym/quick_start/#inference","text":"Inference is the act of predicting labels that we haven't necessarily used in training. That's the whole point of machine learning really: predict something we haven't seen before. Let's try and predict some new values. Using the IDE, run: SELECT pgml . predict ( 'My First Project' , ARRAY [ 0 . 06 , -- age 0 . 05 , -- sex 0 . 05 , -- bmi - 0 . 0056 , -- bp 0 . 012191 , -- s1 - 0 . 043401 , -- s2 0 . 034309 , -- s3 - 0 . 031938 , -- s4 - 0 . 061988 , --s5 - 0 . 031988 -- s6 ] ) AS prediction ; You should see something like this: The prediction column represents the possible value of the target column given the new features we just passed into the pgml.predict() function. You can just as easily predict multiple points and compare them to the actual labels in the dataset: SELECT pgml . predict ( 'My First Project 2' , ARRAY [ age , sex , bmi , bp , s1 , s3 , s3 , s4 , s5 , s6 ]), target FROM pgml . diabetes LIMIT 10 ; Sometimes the model will be pretty close, but sometimes it will be way off. That's why we'll be training several of them and comparing them next.","title":"Inference"},{"location":"gym/quick_start/#browse-around","text":"By creating our first project, we made the Dashboard a little bit more interesting. This is how the pgml.diabetes snapshot we just created looks like: As you can see, we automatically performed some analysis on the data. Visualizing the data is important to understand how it could potentially behave given different models, and maybe even predict how it could evolve in the future. XGBoost is a good algorithm, but what if there are better ones? Let's try training a few more using the IDE. Run these one at a time: -- Simple linear regression. SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'linear' ); -- The Lasso (much fancier linear regression). SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'lasso' ); If you navigate to the Models tab, you should see all three algorithms you just trained: Huh, apparently XGBoost isn't as good we originally thought! In this case, a simple linear regression did significantly better than all the others. It's hard to know which algorithm will perform best given a dataset; even experienced machine learning engineers get this one wrong. With PostgresML, you needn't worry: you can train all of them and see which one does best for your data. PostgresML will automatically use the best one for inference.","title":"Browse around"},{"location":"gym/quick_start/#conclusion","text":"Congratulations on becoming a Machine Learning engineer. If you thought ML was scary or mysterious, we hope that this small tutorial made it a little bit more approachable. This is the first of many tutorials we'll publish, so stay tuned. Happy machine learning!","title":"Conclusion"},{"location":"user_guides/dashboard/overview/","text":"Dashboard \u00b6 PostgresML comes with a web app to provide visibility into models and datasets in your database. If you're running our Docker container , you can view it running on http://localhost:8000/ . Generate example data \u00b6 The test suite for PostgresML is composed by running the SQL files in the examples directory . You can use these examples to populate your local installation with some test data. The test suite only operates on the pgml schema, and is otherwise isolated from the rest of the PostgresML installation. psql -f pgml-extension/sql/test.sql \\ -P pager \\ postgres://postgres@127.0.0.1:5433/pgml_development Projects \u00b6 Projects organize Models that are all striving toward the same task. They aren't much more than a name to group a collection of models. You can see the currently deployed model for each project indicated by . Models \u00b6 Models are the result of training an algorithm on a snapshot of a dataset. They record metrics depending on their projects task, and are scored accordingly. Some models are the result of a hyperparameter search, and include additional analysis on the range of hyperparameters they are tested against. Snapshots \u00b6 A snapshot is created during training runs to record the data used for further analysis, or to train additional models against identical data. Deployments \u00b6 Every deployment is recorded to track models over time.","title":"Dashboard"},{"location":"user_guides/dashboard/overview/#dashboard","text":"PostgresML comes with a web app to provide visibility into models and datasets in your database. If you're running our Docker container , you can view it running on http://localhost:8000/ .","title":"Dashboard"},{"location":"user_guides/dashboard/overview/#generate-example-data","text":"The test suite for PostgresML is composed by running the SQL files in the examples directory . You can use these examples to populate your local installation with some test data. The test suite only operates on the pgml schema, and is otherwise isolated from the rest of the PostgresML installation. psql -f pgml-extension/sql/test.sql \\ -P pager \\ postgres://postgres@127.0.0.1:5433/pgml_development","title":"Generate example data"},{"location":"user_guides/dashboard/overview/#projects","text":"Projects organize Models that are all striving toward the same task. They aren't much more than a name to group a collection of models. You can see the currently deployed model for each project indicated by .","title":"Projects"},{"location":"user_guides/dashboard/overview/#models","text":"Models are the result of training an algorithm on a snapshot of a dataset. They record metrics depending on their projects task, and are scored accordingly. Some models are the result of a hyperparameter search, and include additional analysis on the range of hyperparameters they are tested against.","title":"Models"},{"location":"user_guides/dashboard/overview/#snapshots","text":"A snapshot is created during training runs to record the data used for further analysis, or to train additional models against identical data.","title":"Snapshots"},{"location":"user_guides/dashboard/overview/#deployments","text":"Every deployment is recorded to track models over time.","title":"Deployments"},{"location":"user_guides/predictions/batch/","text":"Batch Predictions \u00b6 The pgml.predict_batch() function is a performance optimization which allows to return predictions for multiple rows in one function call. It works the same way as pgml.predict() in all other respects. Many machine learning algorithms can benefit from calculating predictions in one operation instead of many, and batch predictions can be 3-6 times faster, for large datasets, than pgml.predict() . API \u00b6 The API for batch predictions is very similar to individual predictions, and only requires two arguments: the project name and the aggregated features used for predictions. pgml.predict_batch() pgml . predict_batch ( project_name TEXT , features REAL [] ) Parameters \u00b6 Parameter Description Example project_name The project name used to train models in pgml.train() . My first PostgresML project features An aggregate of feature vectors used to predict novel data points. array_agg(image) Example SELECT pgml . predict_batch ( 'My First PostgresML Project' , array_agg ( ARRAY [ 0.1 , 2.0 , 5.0 ] ) ) AS prediction FROM pgml . digits Note that we are passing the result of array_agg() to our function because we want Postgres to accumulate all the features first, and only give it to PostgresML in one function call. Collecting Results \u00b6 Batch predictions have to be fetched in a subquery or a CTE because they are using the array_agg() aggregate. To get the results back in an easily usable form, pgml.predict_batch() returns a setof result instead of a normal array, and that can be then built into a table: SQL Output WITH predictions AS ( SELECT pgml . predict_batch ( 'My Classification Project' , array_agg ( image ) ) AS prediction , unnest ( array_agg ( target ) ) AS target FROM pgml . digits WHERE target = 0 ) SELECT prediction , target FROM predictions LIMIT 10 ; prediction | target ------------+-------- 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 (10 rows) Since we're using aggregates, one must take care to place limiting predicates into the WHERE clause of the CTE. For example, we used WHERE target = 0 to batch predict images which are only classified into the 0 class. Joins \u00b6 To perform a join on batch predictions, it's necessary to have a uniquely identifiable join column for each row. As you saw in the example above, one can pass any column through the aggregation by using a combination of unnest() and array_agg() . Example \u00b6 WITH predictions AS ( SELECT -- -- Prediction -- pgml . predict_batch ( 'My Bot Detector' , array_agg ( ARRAY [ account_age , city , last_login ]) ) AS prediction , -- -- The pass-through unique identifier for each row -- unnest ( array_agg ( user_id ) ) AS target FROM users -- -- Filter which rows to pass to pgml.predict_batch() -- WHERE last_login > NOW () - INTERVAL '1 minute' ) SELECT prediction , email , ip_address FROM users INNER JOIN predictions ON users . user_id = predictions . user_id","title":"Batch Predictions"},{"location":"user_guides/predictions/batch/#batch-predictions","text":"The pgml.predict_batch() function is a performance optimization which allows to return predictions for multiple rows in one function call. It works the same way as pgml.predict() in all other respects. Many machine learning algorithms can benefit from calculating predictions in one operation instead of many, and batch predictions can be 3-6 times faster, for large datasets, than pgml.predict() .","title":"Batch Predictions"},{"location":"user_guides/predictions/batch/#api","text":"The API for batch predictions is very similar to individual predictions, and only requires two arguments: the project name and the aggregated features used for predictions. pgml.predict_batch() pgml . predict_batch ( project_name TEXT , features REAL [] )","title":"API"},{"location":"user_guides/predictions/batch/#parameters","text":"Parameter Description Example project_name The project name used to train models in pgml.train() . My first PostgresML project features An aggregate of feature vectors used to predict novel data points. array_agg(image) Example SELECT pgml . predict_batch ( 'My First PostgresML Project' , array_agg ( ARRAY [ 0.1 , 2.0 , 5.0 ] ) ) AS prediction FROM pgml . digits Note that we are passing the result of array_agg() to our function because we want Postgres to accumulate all the features first, and only give it to PostgresML in one function call.","title":"Parameters"},{"location":"user_guides/predictions/batch/#collecting-results","text":"Batch predictions have to be fetched in a subquery or a CTE because they are using the array_agg() aggregate. To get the results back in an easily usable form, pgml.predict_batch() returns a setof result instead of a normal array, and that can be then built into a table: SQL Output WITH predictions AS ( SELECT pgml . predict_batch ( 'My Classification Project' , array_agg ( image ) ) AS prediction , unnest ( array_agg ( target ) ) AS target FROM pgml . digits WHERE target = 0 ) SELECT prediction , target FROM predictions LIMIT 10 ; prediction | target ------------+-------- 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 0 | 0 (10 rows) Since we're using aggregates, one must take care to place limiting predicates into the WHERE clause of the CTE. For example, we used WHERE target = 0 to batch predict images which are only classified into the 0 class.","title":"Collecting Results"},{"location":"user_guides/predictions/batch/#joins","text":"To perform a join on batch predictions, it's necessary to have a uniquely identifiable join column for each row. As you saw in the example above, one can pass any column through the aggregation by using a combination of unnest() and array_agg() .","title":"Joins"},{"location":"user_guides/predictions/batch/#example","text":"WITH predictions AS ( SELECT -- -- Prediction -- pgml . predict_batch ( 'My Bot Detector' , array_agg ( ARRAY [ account_age , city , last_login ]) ) AS prediction , -- -- The pass-through unique identifier for each row -- unnest ( array_agg ( user_id ) ) AS target FROM users -- -- Filter which rows to pass to pgml.predict_batch() -- WHERE last_login > NOW () - INTERVAL '1 minute' ) SELECT prediction , email , ip_address FROM users INNER JOIN predictions ON users . user_id = predictions . user_id","title":"Example"},{"location":"user_guides/predictions/deployments/","text":"Deployments \u00b6 A model is automatically deployed and used for predictions if its key metric ( R 2 for regression, F 1 for classification) is improved during training over the previous version. Alternatively, if you want to manage deploys manually, you can always change which model is currently responsible for making predictions. API \u00b6 pgml.deploy() pgml . deploy ( project_name TEXT , strategy TEXT DEFAULT 'best_score' , algorithm TEXT DEFAULT NULL ) Parameters \u00b6 Parameter Description Example project_name The name of the project used in pgml.train() and pgml.predict() . My First PostgresML Project strategy The deployment strategy to use for this deployment. rollback algorithm Restrict the deployment to a specific algorithm. Useful when training on multiple algorithms and hyperparameters at the same time. xgboost Strategies \u00b6 There are 3 different deployment strategies available: Strategy Description most_recent The most recently trained model for this project is immediately deployed, regardless of metrics. best_score The model that achieved the best key metric score is immediately deployed. rollback The model that was last deployed for this project is immediately redeployed, overriding the currently deployed model. The default deployment behavior allows any algorithm to qualify. It's automatically used during training, but can be manually executed as well: SQL Output SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , strategy => 'best_score' ); project | strategy | algorithm ------------------------------------+------------+----------- Handwritten Digit Image Classifier | best_score | xgboost (1 row) Specific Algorithms \u00b6 Deployment candidates can be restricted to a specific algorithm by including the algorithm parameter. This is useful when you're training multiple algorithms using different hyperparameters and want to restrict the deployment a single algorithm only: SQL Output SELECT * FROM pgml . deploy ( project_name => 'Handwritten Digit Image Classifier' , strategy => 'best_score' , algorithm => 'svm' ); project_name | strategy | algorithm ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | svm (1 row) Rolling Back \u00b6 In case the new model isn't performing well in production, it's easy to rollback to the previous version. A rollback creates a new deployment for the old model. Multiple rollbacks in a row will oscillate between the two most recently deployed models, making rollbacks a safe and reversible operation. Rollback 1 Output Rollback 2 Output 1 2 3 4 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , strategy => 'rollback' ); project | strategy | algorithm ------------------------------------+----------+----------- Handwritten Digit Image Classifier | rollback | linear (1 row) SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , strategy => 'rollback' ); project | strategy | algorithm ------------------------------------+----------+----------- Handwritten Digit Image Classifier | rollback | xgboost (1 row)","title":"Deploying Models"},{"location":"user_guides/predictions/deployments/#deployments","text":"A model is automatically deployed and used for predictions if its key metric ( R 2 for regression, F 1 for classification) is improved during training over the previous version. Alternatively, if you want to manage deploys manually, you can always change which model is currently responsible for making predictions.","title":"Deployments"},{"location":"user_guides/predictions/deployments/#api","text":"pgml.deploy() pgml . deploy ( project_name TEXT , strategy TEXT DEFAULT 'best_score' , algorithm TEXT DEFAULT NULL )","title":"API"},{"location":"user_guides/predictions/deployments/#parameters","text":"Parameter Description Example project_name The name of the project used in pgml.train() and pgml.predict() . My First PostgresML Project strategy The deployment strategy to use for this deployment. rollback algorithm Restrict the deployment to a specific algorithm. Useful when training on multiple algorithms and hyperparameters at the same time. xgboost","title":"Parameters"},{"location":"user_guides/predictions/deployments/#strategies","text":"There are 3 different deployment strategies available: Strategy Description most_recent The most recently trained model for this project is immediately deployed, regardless of metrics. best_score The model that achieved the best key metric score is immediately deployed. rollback The model that was last deployed for this project is immediately redeployed, overriding the currently deployed model. The default deployment behavior allows any algorithm to qualify. It's automatically used during training, but can be manually executed as well: SQL Output SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , strategy => 'best_score' ); project | strategy | algorithm ------------------------------------+------------+----------- Handwritten Digit Image Classifier | best_score | xgboost (1 row)","title":"Strategies"},{"location":"user_guides/predictions/deployments/#specific-algorithms","text":"Deployment candidates can be restricted to a specific algorithm by including the algorithm parameter. This is useful when you're training multiple algorithms using different hyperparameters and want to restrict the deployment a single algorithm only: SQL Output SELECT * FROM pgml . deploy ( project_name => 'Handwritten Digit Image Classifier' , strategy => 'best_score' , algorithm => 'svm' ); project_name | strategy | algorithm ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | svm (1 row)","title":"Specific Algorithms"},{"location":"user_guides/predictions/deployments/#rolling-back","text":"In case the new model isn't performing well in production, it's easy to rollback to the previous version. A rollback creates a new deployment for the old model. Multiple rollbacks in a row will oscillate between the two most recently deployed models, making rollbacks a safe and reversible operation. Rollback 1 Output Rollback 2 Output 1 2 3 4 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , strategy => 'rollback' ); project | strategy | algorithm ------------------------------------+----------+----------- Handwritten Digit Image Classifier | rollback | linear (1 row) SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , strategy => 'rollback' ); project | strategy | algorithm ------------------------------------+----------+----------- Handwritten Digit Image Classifier | rollback | xgboost (1 row)","title":"Rolling Back"},{"location":"user_guides/predictions/overview/","text":"Making Predictions \u00b6 The pgml.predict() function is the key value proposition of PostgresML. It provides online predictions using the best, automatically deployed model for a project. API \u00b6 The API for predictions is very simple and only requires two arguments: the project name and the features used for prediction. pgml.predict() pgml . predict ( project_name TEXT , features REAL [] ) Parameters \u00b6 Parameter Description Example project_name The project name used to train models in pgml.train() . My First PostgresML Project features The feature vector used to predict a novel data point. ARRAY[0.1, 0.45, 1.0] Example SELECT pgml . predict ( 'My Classification Project' , ARRAY [ 0.1 , 2.0 , 5.0 ] ) AS prediction ; where ARRAY[0.1, 2.0, 5.0] is the same type of features used in training, in the same order as in the training data table or view. This score can be used in other regular queries. Example SELECT * , pgml . predict ( 'Buy it Again' , ARRAY [ user . location_id , NOW () - user . created_at , user . total_purchases_in_dollars ] ) AS buying_score FROM users WHERE tenant_id = 5 ORDER BY buying_score LIMIT 25 ; Example \u00b6 If you've already been through the Training Overview , you can see the results of those efforts: SQL Output SELECT target , pgml . predict ( 'Handwritten Digit Image Classifier' , image ) AS prediction FROM pgml . digits LIMIT 10 ; target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 (10 rows) Active Model \u00b6 Since it's so easy to train multiple algorithms with different hyperparameters, sometimes it's a good idea to know which deployed model is used to make predictions. You can find that out by querying the pgml.deployed_models view: SQL Output SELECT * FROM pgml . deployed_models ; id | name | task | algorithm | runtime | deployed_at ----+------------------------------------+----------------+-----------+---------+---------------------------- 4 | Handwritten Digit Image Classifier | classification | xgboost | rust | 2022-10-11 13:06:26.473489 (1 row) PostgresML will automatically deploy a model only if it has better metrics than existing ones, so it's safe to experiment with different algorithms and hyperparameters. Take a look at Deploying Models documentation for more details. Specific Models \u00b6 You may also specify a model_id to predict rather than a project name, to use a particular training run. You can find model ids by querying the pgml.models table. SQL Output SELECT models . id , models . algorithm , models . metrics FROM pgml . models JOIN pgml . projects ON projects . id = models . project_id WHERE projects . name = 'Handwritten Digit Image Classifier' ; id | algorithm | metrics ----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------- 1 | linear | {\"f1\": 0.9190376400947571, \"mcc\": 0.9086633324623108, \"recall\": 0.9205743074417114, \"accuracy\": 0.9175946712493896, \"fit_time\": 0.8388963937759399, \"p recision\": 0.9175060987472534, \"score_time\": 0.019625699147582054} For example: make predictions with model_id = 1 : SQL Output SELECT target , pgml . predict ( 1 , image ) AS prediction FROM pgml . digits LIMIT 10 ; target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 (10 rows)","title":"Prediction Overview"},{"location":"user_guides/predictions/overview/#making-predictions","text":"The pgml.predict() function is the key value proposition of PostgresML. It provides online predictions using the best, automatically deployed model for a project.","title":"Making Predictions"},{"location":"user_guides/predictions/overview/#api","text":"The API for predictions is very simple and only requires two arguments: the project name and the features used for prediction. pgml.predict() pgml . predict ( project_name TEXT , features REAL [] )","title":"API"},{"location":"user_guides/predictions/overview/#parameters","text":"Parameter Description Example project_name The project name used to train models in pgml.train() . My First PostgresML Project features The feature vector used to predict a novel data point. ARRAY[0.1, 0.45, 1.0] Example SELECT pgml . predict ( 'My Classification Project' , ARRAY [ 0.1 , 2.0 , 5.0 ] ) AS prediction ; where ARRAY[0.1, 2.0, 5.0] is the same type of features used in training, in the same order as in the training data table or view. This score can be used in other regular queries. Example SELECT * , pgml . predict ( 'Buy it Again' , ARRAY [ user . location_id , NOW () - user . created_at , user . total_purchases_in_dollars ] ) AS buying_score FROM users WHERE tenant_id = 5 ORDER BY buying_score LIMIT 25 ;","title":"Parameters"},{"location":"user_guides/predictions/overview/#example","text":"If you've already been through the Training Overview , you can see the results of those efforts: SQL Output SELECT target , pgml . predict ( 'Handwritten Digit Image Classifier' , image ) AS prediction FROM pgml . digits LIMIT 10 ; target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 (10 rows)","title":"Example"},{"location":"user_guides/predictions/overview/#active-model","text":"Since it's so easy to train multiple algorithms with different hyperparameters, sometimes it's a good idea to know which deployed model is used to make predictions. You can find that out by querying the pgml.deployed_models view: SQL Output SELECT * FROM pgml . deployed_models ; id | name | task | algorithm | runtime | deployed_at ----+------------------------------------+----------------+-----------+---------+---------------------------- 4 | Handwritten Digit Image Classifier | classification | xgboost | rust | 2022-10-11 13:06:26.473489 (1 row) PostgresML will automatically deploy a model only if it has better metrics than existing ones, so it's safe to experiment with different algorithms and hyperparameters. Take a look at Deploying Models documentation for more details.","title":"Active Model"},{"location":"user_guides/predictions/overview/#specific-models","text":"You may also specify a model_id to predict rather than a project name, to use a particular training run. You can find model ids by querying the pgml.models table. SQL Output SELECT models . id , models . algorithm , models . metrics FROM pgml . models JOIN pgml . projects ON projects . id = models . project_id WHERE projects . name = 'Handwritten Digit Image Classifier' ; id | algorithm | metrics ----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------- 1 | linear | {\"f1\": 0.9190376400947571, \"mcc\": 0.9086633324623108, \"recall\": 0.9205743074417114, \"accuracy\": 0.9175946712493896, \"fit_time\": 0.8388963937759399, \"p recision\": 0.9175060987472534, \"score_time\": 0.019625699147582054} For example: make predictions with model_id = 1 : SQL Output SELECT target , pgml . predict ( 1 , image ) AS prediction FROM pgml . digits LIMIT 10 ; target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 (10 rows)","title":"Specific Models"},{"location":"user_guides/schema/deployments/","text":"Deployments \u00b6 Deployments are an artifact of calls to pgml.deploy() and pgml.train() . See Deployments for ways to create new deployments manually. Schema \u00b6 CREATE TABLE IF NOT EXISTS pgml . deployments ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , model_id BIGINT NOT NULL , strategy pgml . strategy NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ) ON DELETE CASCADE , CONSTRAINT model_id_fk FOREIGN KEY ( model_id ) REFERENCES pgml . models ( id ) ON DELETE CASCADE );","title":"Deployments"},{"location":"user_guides/schema/deployments/#deployments","text":"Deployments are an artifact of calls to pgml.deploy() and pgml.train() . See Deployments for ways to create new deployments manually.","title":"Deployments"},{"location":"user_guides/schema/deployments/#schema","text":"CREATE TABLE IF NOT EXISTS pgml . deployments ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , model_id BIGINT NOT NULL , strategy pgml . strategy NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ) ON DELETE CASCADE , CONSTRAINT model_id_fk FOREIGN KEY ( model_id ) REFERENCES pgml . models ( id ) ON DELETE CASCADE );","title":"Schema"},{"location":"user_guides/schema/models/","text":"Models \u00b6 Models are an artifact of calls to pgml.train() . See Training Overview for ways to create new models. Schema \u00b6 CREATE TABLE IF NOT EXISTS pgml . models ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , snapshot_id BIGINT NOT NULL , num_features INT NOT NULL , algorithm TEXT NOT NULL , runtime pgml . runtime DEFAULT 'python' :: pgml . runtime , hyperparams JSONB NOT NULL , status TEXT NOT NULL , metrics JSONB , search TEXT , search_params JSONB NOT NULL , search_args JSONB NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ) ON DELETE CASCADE , CONSTRAINT snapshot_id_fk FOREIGN KEY ( snapshot_id ) REFERENCES pgml . snapshots ( id ) ON DELETE SET NULL ); CREATE TABLE IF NOT EXISTS pgml . files ( id BIGSERIAL PRIMARY KEY , model_id BIGINT NOT NULL , path TEXT NOT NULL , part INTEGER NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), data BYTEA NOT NULL , CONSTRAINT model_id_fk FOREIGN KEY ( model_id ) REFERENCES pgml . models ( id ) ON DELETE CASCADE ); Files \u00b6 Models are partitioned into parts and stored in the pgml.files table. Most models are relatively small (just a few megabytes), but some neural networks can grow to gigabytes in size, and would therefore exceed the maximum possible size of a column Postgres. Partitioning fixes that limitation and allows us to store models up to 32TB in size (or larger, if we employ table partitioning).","title":"Models"},{"location":"user_guides/schema/models/#models","text":"Models are an artifact of calls to pgml.train() . See Training Overview for ways to create new models.","title":"Models"},{"location":"user_guides/schema/models/#schema","text":"CREATE TABLE IF NOT EXISTS pgml . models ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , snapshot_id BIGINT NOT NULL , num_features INT NOT NULL , algorithm TEXT NOT NULL , runtime pgml . runtime DEFAULT 'python' :: pgml . runtime , hyperparams JSONB NOT NULL , status TEXT NOT NULL , metrics JSONB , search TEXT , search_params JSONB NOT NULL , search_args JSONB NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ) ON DELETE CASCADE , CONSTRAINT snapshot_id_fk FOREIGN KEY ( snapshot_id ) REFERENCES pgml . snapshots ( id ) ON DELETE SET NULL ); CREATE TABLE IF NOT EXISTS pgml . files ( id BIGSERIAL PRIMARY KEY , model_id BIGINT NOT NULL , path TEXT NOT NULL , part INTEGER NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), data BYTEA NOT NULL , CONSTRAINT model_id_fk FOREIGN KEY ( model_id ) REFERENCES pgml . models ( id ) ON DELETE CASCADE );","title":"Schema"},{"location":"user_guides/schema/models/#files","text":"Models are partitioned into parts and stored in the pgml.files table. Most models are relatively small (just a few megabytes), but some neural networks can grow to gigabytes in size, and would therefore exceed the maximum possible size of a column Postgres. Partitioning fixes that limitation and allows us to store models up to 32TB in size (or larger, if we employ table partitioning).","title":"Files"},{"location":"user_guides/schema/projects/","text":"Projects \u00b6 Projects are an artifact of calls to pgml.train() . See Training Overview for ways to create new projects. Schema \u00b6 CREATE TABLE IF NOT EXISTS pgml . projects ( id BIGSERIAL PRIMARY KEY , name TEXT NOT NULL , task pgml . task NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Projects"},{"location":"user_guides/schema/projects/#projects","text":"Projects are an artifact of calls to pgml.train() . See Training Overview for ways to create new projects.","title":"Projects"},{"location":"user_guides/schema/projects/#schema","text":"CREATE TABLE IF NOT EXISTS pgml . projects ( id BIGSERIAL PRIMARY KEY , name TEXT NOT NULL , task pgml . task NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Schema"},{"location":"user_guides/schema/snapshots/","text":"Snapshots \u00b6 Snapshots are an artifact of calls to pgml.train() that specify the relation_name and y_column_name parameters. See Training Overview for ways to create new snapshots. Schema \u00b6 CREATE TABLE IF NOT EXISTS pgml . snapshots ( id BIGSERIAL PRIMARY KEY , relation_name TEXT NOT NULL , y_column_name TEXT [] NOT NULL , test_size FLOAT4 NOT NULL , test_sampling pgml . sampling NOT NULL , status TEXT NOT NULL , columns JSONB , analysis JSONB , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () ); Snapshot Storage \u00b6 Every snapshot has an accompanying table in the pgml schema. For example, the snapshot with the primary key 42 has all data saved in the pgml.snaphot_42 table. If the test_sampling was set to random during training, the rows in the table are ordered using ORDER BY RANDOM() , so that future samples can be consistently and efficiently randomized.","title":"Snapshots"},{"location":"user_guides/schema/snapshots/#snapshots","text":"Snapshots are an artifact of calls to pgml.train() that specify the relation_name and y_column_name parameters. See Training Overview for ways to create new snapshots.","title":"Snapshots"},{"location":"user_guides/schema/snapshots/#schema","text":"CREATE TABLE IF NOT EXISTS pgml . snapshots ( id BIGSERIAL PRIMARY KEY , relation_name TEXT NOT NULL , y_column_name TEXT [] NOT NULL , test_size FLOAT4 NOT NULL , test_sampling pgml . sampling NOT NULL , status TEXT NOT NULL , columns JSONB , analysis JSONB , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Schema"},{"location":"user_guides/schema/snapshots/#snapshot-storage","text":"Every snapshot has an accompanying table in the pgml schema. For example, the snapshot with the primary key 42 has all data saved in the pgml.snaphot_42 table. If the test_sampling was set to random during training, the rows in the table are ordered using ORDER BY RANDOM() , so that future samples can be consistently and efficiently randomized.","title":"Snapshot Storage"},{"location":"user_guides/setup/distributed_training/","text":"Distributed Training \u00b6 Depending on the size of your dataset and its change frequency, you may want to offload training (or inference) to secondary PostgreSQL servers to avoid excessive load on your primary. We've outlined three of the built-in mechanisms to help distribute the load. pg_dump (< 10GB) \u00b6 pg_dump is a standard tool used to export data from a PostgreSQL database. If your dataset is small (e.g. less than 10GB) and changes infrequently, this could be quickest and simplest way to do it. Example # Export data from your production DB pg_dump \\ postgres://username:password@production-database.example.com/production_db \\ --no-owner \\ -t table_one \\ -t table_two > dump.sql # Import the data into PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f dump.sql If you're using our Docker stack, you can import the data there: psql \\ postgres://postgres@localhost:5433/pgml_development \\ -f dump.sql PostgresML tables and functions are located in the pgml schema, so you can safely import your data into PostgresML without conflicts. You can also use pg_dump to copy the pgml schema to other servers which will make the trained models available in a distributed fashion. Foreign Data Wrappers (10GB - 100GB) \u00b6 Foreign Data Wrappers, or FDWs for short, are another good tool for reading or importing data from another PostgreSQL database into PostgresML. Setting up FDWs is a bit more involved than pg_dump but they provide real time access to your production data and are good for small to medium size datasets (e.g. 10GB to 100GB) that change frequently. Official PostgreSQL docs explain FDWs with more detail; we'll document a basic example below. Install the extension \u00b6 PostgreSQL comes with postgres_fdw already available, but the extension needs to be explicitly installed into the database. Connect to your PostgresML database as a superuser and run: CREATE EXTENSION postgres_fdw ; Create foreign server \u00b6 A foreign server is a FDW reference to another PostgreSQL database running somewhere else. In this case, that foreign server is your production database. CREATE SERVER your_production_db FOREIGN DATA WRAPPER postgres_fdw OPTIONS ( host 'production-database.example.com' , port '5432' , dbname 'production_db' ); Create user mapping \u00b6 A user mapping is a relationship between the user you're connecting with to PostgresML and a user that exists on your production database. FDW will use this mapping to talk to your database when it wants to read some data. CREATE USER MAPPING FOR pgml_user SERVER your_production_db OPTIONS ( user 'your_production_db_user' , password 'your_production_db_user_password' ); At this point, when you connect to PostgresML using the example pgml_user and then query data in your production database using FDW, it'll use the user your_production_db_user to connect to your DB and fetch the data. Make sure that your_production_db_user has SELECT permissions on the tables you want to query and the USAGE permissions on the schema. Import the tables \u00b6 The final step is import your production database tables into PostgresML by creating a foreign schema mapping. This mapping will tell PostgresML which tables are available in your database. The quickest way is to import all of them, like so: IMPORT FOREIGN SCHEMA public FROM SERVER your_production_db INTO public ; This will import all tables from your production DB public schema into the public schema in PostgresML. The tables are now available for querying in PostgresML. Usage \u00b6 PostgresML snapshots the data before training on it, so every time you run pgml.train with a relation_name argument, the data will be fetched from the foreign data wrapper and imported into PostgresML. FDWs are reasonably good at fetching only the data specified by the VIEW , so if you place sufficient limits on your dataset in the CREATE VIEW statement, e.g. train on the last two weeks of data, or something similar, FDWs will do its best to fetch only the last two weeks of data in an efficient manner, leaving the rest behind on the primary. Logical replication (100GB - 10TB) \u00b6 Logical replication is a replication mechanism that's been available since PostgreSQL 10. It allows to copy entire tables and schemas from any database into PostgresML and keeping them up-to-date in real time fairly cheaply as the data in production changes. This is suitable for medium to large PostgreSQL deployments (e.g. 100GB - 10TB). Logical replication is designed as a pub/sub system, where your production database is the publisher and PostgresML is the subscriber. As data in your database changes, it is streamed into PostgresML in milliseconds, which is very similar to how Postgres streaming replication works as well. The setup is slightly more involved than Foreign Data Wrappers, and is documented below. All queries must be run as a superuser. WAL \u00b6 First, make sure that your production DB has logical replication enabled. For this, it has to be on PostgreSQL 10 or above and also have wal_level configuration set to logical . SQL Output SHOW wal_level ; wal_level ----------- logical (1 row) If this is not the case, you'll need to change it and restart the server. Publication \u00b6 The publication is created on your production DB and configures which tables are replicated using logical replication. To replicate all tables in your public schema, you can run this: CREATE PUBLICATION all_tables FOR ALL TABLES ; Schema \u00b6 Logical replication does not copy the schema, so it needs to be copied manually in advance; pg_dump is great for this: # Dump the schema from your production DB pg_dump \\ postgres://username:password@production-db.example.com/production_db \\ --schema-only \\ --no-owner > schema.sql # Import the schema in PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f schema.sql Subscription \u00b6 The subscription is created in your PostgresML database. To replicate all the tables we marked in the previous step, run: CREATE SUBSCRIPTION all_tables CONNECTION 'postgres://superuser:password@production-database.example.com/production_db' PUBLICATION all_tables ; As soon as you run this, logical replication will begin. It will start by copying all the data from your production database into PostgresML. That will take a while, depending on database size, network connection and hardware performance. Each table will be copied individually and the process is parallelized. Once the copy is complete, logical replication will synchronize and will replicate the data from your production database into PostgresML in real-time. Schema changes \u00b6 Logical replication has one notable limitation: it does not replicate schema (table) changes. If you change a table in your production DB in an incompatible way, e.g. by adding a column, the replication will break. To remediate this, when you're performing the schema change, make the change first in PostgresML and then in your production database. Native installation (10TB and beyond) \u00b6 For databases that are very large, e.g. 10TB+, we recommend you install the extension directly into your database. This option is available for databases of all sizes, but we recognize that many small to medium databases run on managed services, e.g. RDS, which don't allow this mechanism.","title":"Distributed Training"},{"location":"user_guides/setup/distributed_training/#distributed-training","text":"Depending on the size of your dataset and its change frequency, you may want to offload training (or inference) to secondary PostgreSQL servers to avoid excessive load on your primary. We've outlined three of the built-in mechanisms to help distribute the load.","title":"Distributed Training"},{"location":"user_guides/setup/distributed_training/#pg_dump-10gb","text":"pg_dump is a standard tool used to export data from a PostgreSQL database. If your dataset is small (e.g. less than 10GB) and changes infrequently, this could be quickest and simplest way to do it. Example # Export data from your production DB pg_dump \\ postgres://username:password@production-database.example.com/production_db \\ --no-owner \\ -t table_one \\ -t table_two > dump.sql # Import the data into PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f dump.sql If you're using our Docker stack, you can import the data there: psql \\ postgres://postgres@localhost:5433/pgml_development \\ -f dump.sql PostgresML tables and functions are located in the pgml schema, so you can safely import your data into PostgresML without conflicts. You can also use pg_dump to copy the pgml schema to other servers which will make the trained models available in a distributed fashion.","title":"pg_dump (&lt; 10GB)"},{"location":"user_guides/setup/distributed_training/#foreign-data-wrappers-10gb-100gb","text":"Foreign Data Wrappers, or FDWs for short, are another good tool for reading or importing data from another PostgreSQL database into PostgresML. Setting up FDWs is a bit more involved than pg_dump but they provide real time access to your production data and are good for small to medium size datasets (e.g. 10GB to 100GB) that change frequently. Official PostgreSQL docs explain FDWs with more detail; we'll document a basic example below.","title":"Foreign Data Wrappers (10GB - 100GB)"},{"location":"user_guides/setup/distributed_training/#install-the-extension","text":"PostgreSQL comes with postgres_fdw already available, but the extension needs to be explicitly installed into the database. Connect to your PostgresML database as a superuser and run: CREATE EXTENSION postgres_fdw ;","title":"Install the extension"},{"location":"user_guides/setup/distributed_training/#create-foreign-server","text":"A foreign server is a FDW reference to another PostgreSQL database running somewhere else. In this case, that foreign server is your production database. CREATE SERVER your_production_db FOREIGN DATA WRAPPER postgres_fdw OPTIONS ( host 'production-database.example.com' , port '5432' , dbname 'production_db' );","title":"Create foreign server"},{"location":"user_guides/setup/distributed_training/#create-user-mapping","text":"A user mapping is a relationship between the user you're connecting with to PostgresML and a user that exists on your production database. FDW will use this mapping to talk to your database when it wants to read some data. CREATE USER MAPPING FOR pgml_user SERVER your_production_db OPTIONS ( user 'your_production_db_user' , password 'your_production_db_user_password' ); At this point, when you connect to PostgresML using the example pgml_user and then query data in your production database using FDW, it'll use the user your_production_db_user to connect to your DB and fetch the data. Make sure that your_production_db_user has SELECT permissions on the tables you want to query and the USAGE permissions on the schema.","title":"Create user mapping"},{"location":"user_guides/setup/distributed_training/#import-the-tables","text":"The final step is import your production database tables into PostgresML by creating a foreign schema mapping. This mapping will tell PostgresML which tables are available in your database. The quickest way is to import all of them, like so: IMPORT FOREIGN SCHEMA public FROM SERVER your_production_db INTO public ; This will import all tables from your production DB public schema into the public schema in PostgresML. The tables are now available for querying in PostgresML.","title":"Import the tables"},{"location":"user_guides/setup/distributed_training/#usage","text":"PostgresML snapshots the data before training on it, so every time you run pgml.train with a relation_name argument, the data will be fetched from the foreign data wrapper and imported into PostgresML. FDWs are reasonably good at fetching only the data specified by the VIEW , so if you place sufficient limits on your dataset in the CREATE VIEW statement, e.g. train on the last two weeks of data, or something similar, FDWs will do its best to fetch only the last two weeks of data in an efficient manner, leaving the rest behind on the primary.","title":"Usage"},{"location":"user_guides/setup/distributed_training/#logical-replication-100gb-10tb","text":"Logical replication is a replication mechanism that's been available since PostgreSQL 10. It allows to copy entire tables and schemas from any database into PostgresML and keeping them up-to-date in real time fairly cheaply as the data in production changes. This is suitable for medium to large PostgreSQL deployments (e.g. 100GB - 10TB). Logical replication is designed as a pub/sub system, where your production database is the publisher and PostgresML is the subscriber. As data in your database changes, it is streamed into PostgresML in milliseconds, which is very similar to how Postgres streaming replication works as well. The setup is slightly more involved than Foreign Data Wrappers, and is documented below. All queries must be run as a superuser.","title":"Logical replication (100GB - 10TB)"},{"location":"user_guides/setup/distributed_training/#wal","text":"First, make sure that your production DB has logical replication enabled. For this, it has to be on PostgreSQL 10 or above and also have wal_level configuration set to logical . SQL Output SHOW wal_level ; wal_level ----------- logical (1 row) If this is not the case, you'll need to change it and restart the server.","title":"WAL"},{"location":"user_guides/setup/distributed_training/#publication","text":"The publication is created on your production DB and configures which tables are replicated using logical replication. To replicate all tables in your public schema, you can run this: CREATE PUBLICATION all_tables FOR ALL TABLES ;","title":"Publication"},{"location":"user_guides/setup/distributed_training/#schema","text":"Logical replication does not copy the schema, so it needs to be copied manually in advance; pg_dump is great for this: # Dump the schema from your production DB pg_dump \\ postgres://username:password@production-db.example.com/production_db \\ --schema-only \\ --no-owner > schema.sql # Import the schema in PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f schema.sql","title":"Schema"},{"location":"user_guides/setup/distributed_training/#subscription","text":"The subscription is created in your PostgresML database. To replicate all the tables we marked in the previous step, run: CREATE SUBSCRIPTION all_tables CONNECTION 'postgres://superuser:password@production-database.example.com/production_db' PUBLICATION all_tables ; As soon as you run this, logical replication will begin. It will start by copying all the data from your production database into PostgresML. That will take a while, depending on database size, network connection and hardware performance. Each table will be copied individually and the process is parallelized. Once the copy is complete, logical replication will synchronize and will replicate the data from your production database into PostgresML in real-time.","title":"Subscription"},{"location":"user_guides/setup/distributed_training/#schema-changes","text":"Logical replication has one notable limitation: it does not replicate schema (table) changes. If you change a table in your production DB in an incompatible way, e.g. by adding a column, the replication will break. To remediate this, when you're performing the schema change, make the change first in PostgresML and then in your production database.","title":"Schema changes"},{"location":"user_guides/setup/distributed_training/#native-installation-10tb-and-beyond","text":"For databases that are very large, e.g. 10TB+, we recommend you install the extension directly into your database. This option is available for databases of all sizes, but we recognize that many small to medium databases run on managed services, e.g. RDS, which don't allow this mechanism.","title":"Native installation (10TB and beyond)"},{"location":"user_guides/setup/gpu_support/","text":"GPU Support \u00b6 PostgresML is capable of leveraging GPUs when the underlying libraries and hardware are properly configured on the database server. The CUDA runtime is statically linked during the build process, so it does not introduce additional dependencies on the runtime host. Tip Models trained on GPU may also require GPU support to make predictions. Consult the documentation for each library on configuring training vs inference. Tensorflow \u00b6 GPU setup for Tensorflow is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face . Torch \u00b6 GPU setup for Torch is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face . Flax \u00b6 GPU setup for Flax is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face . XGBoost \u00b6 GPU setup for XGBoost is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'xgboost' , hyperparams => '{\"tree_method\" : \"gpu_hist\"}' ); LightGBM \u00b6 GPU setup for LightGBM is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'lightgbm' , hyperparams => '{\"device\" : \"cuda\"}' ); Scikit-learn \u00b6 None of the scikit-learn algorithms natively support GPU devices. There are a few projects to improve scikit performance with additional parallelism, although we currently have not integrated these with PostgresML: https://github.com/intel/scikit-learn-intelex https://github.com/rapidsai/cuml If your project would benefit from GPU support, please consider opening an issue, so we can prioritize integrations.","title":"GPU Support"},{"location":"user_guides/setup/gpu_support/#gpu-support","text":"PostgresML is capable of leveraging GPUs when the underlying libraries and hardware are properly configured on the database server. The CUDA runtime is statically linked during the build process, so it does not introduce additional dependencies on the runtime host. Tip Models trained on GPU may also require GPU support to make predictions. Consult the documentation for each library on configuring training vs inference.","title":"GPU Support"},{"location":"user_guides/setup/gpu_support/#tensorflow","text":"GPU setup for Tensorflow is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face .","title":"Tensorflow"},{"location":"user_guides/setup/gpu_support/#torch","text":"GPU setup for Torch is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face .","title":"Torch"},{"location":"user_guides/setup/gpu_support/#flax","text":"GPU setup for Flax is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face .","title":"Flax"},{"location":"user_guides/setup/gpu_support/#xgboost","text":"GPU setup for XGBoost is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'xgboost' , hyperparams => '{\"tree_method\" : \"gpu_hist\"}' );","title":"XGBoost"},{"location":"user_guides/setup/gpu_support/#lightgbm","text":"GPU setup for LightGBM is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'lightgbm' , hyperparams => '{\"device\" : \"cuda\"}' );","title":"LightGBM"},{"location":"user_guides/setup/gpu_support/#scikit-learn","text":"None of the scikit-learn algorithms natively support GPU devices. There are a few projects to improve scikit performance with additional parallelism, although we currently have not integrated these with PostgresML: https://github.com/intel/scikit-learn-intelex https://github.com/rapidsai/cuml If your project would benefit from GPU support, please consider opening an issue, so we can prioritize integrations.","title":"Scikit-learn"},{"location":"user_guides/setup/installation/","text":"Installation \u00b6 Tip With the release of PostgresML 2.0, this documentation has been deprecated. New installation instructions are available . A PostgresML deployment consists of two different runtimes. The foundational runtime is a Python extension for Postgres ( pgml-extension ) that facilitates the machine learning lifecycle inside the database. Additionally, we provide a dashboard ( pgml-dashboard ) that can connect to your Postgres server and provide additional management functionality. It will also provide visibility into the models you build and data they use. Install PostgreSQL with PL/Python \u00b6 PostgresML leverages Python libraries for their machine learning capabilities. You'll need to make sure the PostgreSQL installation has PL/Python built in. OS X Linux Windows We recommend you use Postgres.app because it comes with PL/Python . Otherwise, you'll need to install PL/Python manually. Once you have Postgres.app running, you'll need to install the Python framework. Mac OS has multiple distributions of Python, namely one from Brew and one from the Python community (Python.org); Postgres.app and PL/Python depend on the community one. The following versions of Python and Postgres.app are compatible: PostgreSQL version Python version Download link 14 3.9 Python 3.9 64-bit 13 3.8 Python 3.8 64-bit All Python.org installers for Mac OS are available here . You can also get more details about this in the Postgres.app documentation . Each Ubuntu/Debian distribution comes with its own version of PostgreSQL, the simplest way is to install it from Aptitude: $ sudo apt-get install -y postgresql-plpython3-12 python3 python3-pip postgresql-12 EnterpriseDB provides Windows builds of PostgreSQL available for download . Install the extension \u00b6 To use our Python package inside PostgreSQL, we need to install it into the global Python package space. Depending on which version of Python you installed in the previous step, use the corresponding pip executable. Change the --database-url option to point to your PostgreSQL server. sudo pip3 install pgml-extension python3 -m pgml_extension --database-url = postgres://user_name:password@localhost:5432/database_name If everything works, you should be able to run this successfully: psql -c 'SELECT pgml.version()' postgres://user_name:password@localhost:5432/database_name Run the dashboard \u00b6 The PostgresML dashboard is a Django app, that can be run against any PostgreSQL installation. There is an included Dockerfile if you wish to run it as a container, or you may want to setup a Python venv to isolate the dependencies. Basic install can be achieved with: Clone the repo: git clone https://github.com/postgresml/postgresml && cd postgresml/pgml-dashboard Set your PGML_DATABASE_URL environment variable: echo PGML_DATABASE_URL = postgres://user_name:password@localhost:5432/database_name > .env Install dependencies: pip install -r requirements.txt Run the server: python manage.py runserver","title":"v1.0"},{"location":"user_guides/setup/installation/#installation","text":"Tip With the release of PostgresML 2.0, this documentation has been deprecated. New installation instructions are available . A PostgresML deployment consists of two different runtimes. The foundational runtime is a Python extension for Postgres ( pgml-extension ) that facilitates the machine learning lifecycle inside the database. Additionally, we provide a dashboard ( pgml-dashboard ) that can connect to your Postgres server and provide additional management functionality. It will also provide visibility into the models you build and data they use.","title":"Installation"},{"location":"user_guides/setup/installation/#install-postgresql-with-plpython","text":"PostgresML leverages Python libraries for their machine learning capabilities. You'll need to make sure the PostgreSQL installation has PL/Python built in. OS X Linux Windows We recommend you use Postgres.app because it comes with PL/Python . Otherwise, you'll need to install PL/Python manually. Once you have Postgres.app running, you'll need to install the Python framework. Mac OS has multiple distributions of Python, namely one from Brew and one from the Python community (Python.org); Postgres.app and PL/Python depend on the community one. The following versions of Python and Postgres.app are compatible: PostgreSQL version Python version Download link 14 3.9 Python 3.9 64-bit 13 3.8 Python 3.8 64-bit All Python.org installers for Mac OS are available here . You can also get more details about this in the Postgres.app documentation . Each Ubuntu/Debian distribution comes with its own version of PostgreSQL, the simplest way is to install it from Aptitude: $ sudo apt-get install -y postgresql-plpython3-12 python3 python3-pip postgresql-12 EnterpriseDB provides Windows builds of PostgreSQL available for download .","title":"Install PostgreSQL with PL/Python"},{"location":"user_guides/setup/installation/#install-the-extension","text":"To use our Python package inside PostgreSQL, we need to install it into the global Python package space. Depending on which version of Python you installed in the previous step, use the corresponding pip executable. Change the --database-url option to point to your PostgreSQL server. sudo pip3 install pgml-extension python3 -m pgml_extension --database-url = postgres://user_name:password@localhost:5432/database_name If everything works, you should be able to run this successfully: psql -c 'SELECT pgml.version()' postgres://user_name:password@localhost:5432/database_name","title":"Install the extension"},{"location":"user_guides/setup/installation/#run-the-dashboard","text":"The PostgresML dashboard is a Django app, that can be run against any PostgreSQL installation. There is an included Dockerfile if you wish to run it as a container, or you may want to setup a Python venv to isolate the dependencies. Basic install can be achieved with: Clone the repo: git clone https://github.com/postgresml/postgresml && cd postgresml/pgml-dashboard Set your PGML_DATABASE_URL environment variable: echo PGML_DATABASE_URL = postgres://user_name:password@localhost:5432/database_name > .env Install dependencies: pip install -r requirements.txt Run the server: python manage.py runserver","title":"Run the dashboard"},{"location":"user_guides/setup/quick_start_with_docker/","text":"Quick Start w/ Docker \u00b6 We've prepared a Docker image that will allow you to quickly spin up a new PostgreSQL database with PostgresML already installed. It also includes some Scikit toy datasets so you can easily experiment with PostgresML workflows without having to import your own data. You can skip to Installation for production installation instructions. OS X Linux Windows Install Docker for OS X . Install Docker for Linux . Some package managers (e.g. Ubuntu/Debian) additionally require the docker-compose package to be installed separately. Install Docker for Windows . Use the Linux instructions if you're installing in Windows Subsystem for Linux. Clone the repo: git clone https://github.com/postgresml/postgresml Start Dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running: cd postgresml && docker-compose up Connect to Postgres in the container with PostgresML installed: psql postgres://postgres@localhost:5433/pgml_development Validate your installation: SQL Output SELECT pgml . version (); pgml_development=# SELECT pgml.version(); version --------- 2.0.0 (1 row) Browse the dashboard on http://localhost:8000/ Note If you'd like to preserve your database over multiple docker sessions, use docker-compose stop or ctrl+c when you shut down the containers. docker-compose down will remove the docker volumes, and completely reset the database. Basic Workflow \u00b6 Here is a simple PostgresML example to get you started. We'll import a Scikit dataset, train a couple models on it and make real time predictions, all of it using only SQL. Import the digits dataset: SQL Output SELECT * FROM pgml . load_dataset ( 'digits' ); pgml=# SELECT * FROM pgml.load_dataset('digits'); INFO: num_features: 64, num_samples: 1797, feature_names: [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"] table_name | rows -------------+------ pgml.digits | 1797 (1 row) Train an XGBoost model: SQL Output SELECT * FROM pgml . train ( 'My First PostgresML Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); pgml=# SELECT * FROM pgml.train('My First PostgresML Project', task => 'classification', relation_name => 'pgml.digits', y_column_name => 'target', algorithm => 'xgboost', hyperparams => '{ \"n_estimators\": 25 }' ); INFO: Snapshotting table \"pgml.digits\", this may take a little while... INFO: Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_1\" INFO: Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 } INFO: Training Model { id: 15, algorithm: xgboost, runtime: rust } INFO: Hyperparameter searches: 1, cross validation folds: 1 INFO: Hyperparams: { \"n_estimators\": 25 } INFO: Metrics: { \"f1\": 0.88522536, \"precision\": 0.8835865, \"recall\": 0.88687027, \"accuracy\": 0.8841871, \"mcc\": 0.87189955, \"fit_time\": 0.44059604, \"score_time\": 0.005983766 } project | task | algorithm | deployed -----------------------------+----------------+-----------+---------- My first PostgresML project | classification | xgboost | t (1 row) Train a LightGBM model: SQL Output SELECT * FROM pgml . train ( 'My First PostgresML Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'lightgbm' ); pgml=# SELECT * FROM pgml.train('My First PostgresML Project', task => 'classification', relation_name => 'pgml.digits', y_column_name => 'target', algorithm => 'lightgbm' ); INFO: Snapshotting table \"pgml.digits\", this may take a little while... INFO: Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_18\" INFO: Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 } INFO: Training Model { id: 16, algorithm: lightgbm, runtime: rust } INFO: Hyperparameter searches: 1, cross validation folds: 1 INFO: Hyperparams: {} INFO: Metrics: { \"f1\": 0.91579026, \"precision\": 0.915012, \"recall\": 0.9165698, \"accuracy\": 0.9153675, \"mcc\": 0.9063865, \"fit_time\": 0.27111048, \"score_time\": 0.004169579 } project | task | algorithm | deployed -----------------------------+----------------+-----------+---------- My first PostgresML project | classification | lightgbm | t (1 row) Looks like LightGBM did better with default hyperparameters. It's automatically deployed and will be used for inference. Infer a few data points in real time: SQL Output SELECT target , pgml . predict ( 'My First PostgresML Project' , image ) AS prediction FROM pgml . digits LIMIT 5 ; pgml=# SELECT target, pgml.predict('My First PostgresML Project', image) AS prediction FROM pgml.digits LIMIT 5; target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 The following common machine learning tasks are performed automatically by PostgresML: Snapshot the data so the experiment is reproducible Split the dataset into train and test sets Train and validate the model Save it into the model store (a Postgres table) Load it and cache it during inference Check out our Training and Predictions documentation for more details. Some more advanced topics like hyperparameter search and GPU acceleration are available as well. Dashboard \u00b6 The Dashboard app is available at http://localhost:8000/ . You can use it to write experiments in Jupyter-style notebooks, manage projects, and visualize datasets used by PostgresML.","title":"Quick Start with Docker"},{"location":"user_guides/setup/quick_start_with_docker/#quick-start-w-docker","text":"We've prepared a Docker image that will allow you to quickly spin up a new PostgreSQL database with PostgresML already installed. It also includes some Scikit toy datasets so you can easily experiment with PostgresML workflows without having to import your own data. You can skip to Installation for production installation instructions. OS X Linux Windows Install Docker for OS X . Install Docker for Linux . Some package managers (e.g. Ubuntu/Debian) additionally require the docker-compose package to be installed separately. Install Docker for Windows . Use the Linux instructions if you're installing in Windows Subsystem for Linux. Clone the repo: git clone https://github.com/postgresml/postgresml Start Dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running: cd postgresml && docker-compose up Connect to Postgres in the container with PostgresML installed: psql postgres://postgres@localhost:5433/pgml_development Validate your installation: SQL Output SELECT pgml . version (); pgml_development=# SELECT pgml.version(); version --------- 2.0.0 (1 row) Browse the dashboard on http://localhost:8000/ Note If you'd like to preserve your database over multiple docker sessions, use docker-compose stop or ctrl+c when you shut down the containers. docker-compose down will remove the docker volumes, and completely reset the database.","title":"Quick Start w/ Docker"},{"location":"user_guides/setup/quick_start_with_docker/#basic-workflow","text":"Here is a simple PostgresML example to get you started. We'll import a Scikit dataset, train a couple models on it and make real time predictions, all of it using only SQL. Import the digits dataset: SQL Output SELECT * FROM pgml . load_dataset ( 'digits' ); pgml=# SELECT * FROM pgml.load_dataset('digits'); INFO: num_features: 64, num_samples: 1797, feature_names: [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"] table_name | rows -------------+------ pgml.digits | 1797 (1 row) Train an XGBoost model: SQL Output SELECT * FROM pgml . train ( 'My First PostgresML Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); pgml=# SELECT * FROM pgml.train('My First PostgresML Project', task => 'classification', relation_name => 'pgml.digits', y_column_name => 'target', algorithm => 'xgboost', hyperparams => '{ \"n_estimators\": 25 }' ); INFO: Snapshotting table \"pgml.digits\", this may take a little while... INFO: Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_1\" INFO: Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 } INFO: Training Model { id: 15, algorithm: xgboost, runtime: rust } INFO: Hyperparameter searches: 1, cross validation folds: 1 INFO: Hyperparams: { \"n_estimators\": 25 } INFO: Metrics: { \"f1\": 0.88522536, \"precision\": 0.8835865, \"recall\": 0.88687027, \"accuracy\": 0.8841871, \"mcc\": 0.87189955, \"fit_time\": 0.44059604, \"score_time\": 0.005983766 } project | task | algorithm | deployed -----------------------------+----------------+-----------+---------- My first PostgresML project | classification | xgboost | t (1 row) Train a LightGBM model: SQL Output SELECT * FROM pgml . train ( 'My First PostgresML Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'lightgbm' ); pgml=# SELECT * FROM pgml.train('My First PostgresML Project', task => 'classification', relation_name => 'pgml.digits', y_column_name => 'target', algorithm => 'lightgbm' ); INFO: Snapshotting table \"pgml.digits\", this may take a little while... INFO: Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_18\" INFO: Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 } INFO: Training Model { id: 16, algorithm: lightgbm, runtime: rust } INFO: Hyperparameter searches: 1, cross validation folds: 1 INFO: Hyperparams: {} INFO: Metrics: { \"f1\": 0.91579026, \"precision\": 0.915012, \"recall\": 0.9165698, \"accuracy\": 0.9153675, \"mcc\": 0.9063865, \"fit_time\": 0.27111048, \"score_time\": 0.004169579 } project | task | algorithm | deployed -----------------------------+----------------+-----------+---------- My first PostgresML project | classification | lightgbm | t (1 row) Looks like LightGBM did better with default hyperparameters. It's automatically deployed and will be used for inference. Infer a few data points in real time: SQL Output SELECT target , pgml . predict ( 'My First PostgresML Project' , image ) AS prediction FROM pgml . digits LIMIT 5 ; pgml=# SELECT target, pgml.predict('My First PostgresML Project', image) AS prediction FROM pgml.digits LIMIT 5; target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 The following common machine learning tasks are performed automatically by PostgresML: Snapshot the data so the experiment is reproducible Split the dataset into train and test sets Train and validate the model Save it into the model store (a Postgres table) Load it and cache it during inference Check out our Training and Predictions documentation for more details. Some more advanced topics like hyperparameter search and GPU acceleration are available as well.","title":"Basic Workflow"},{"location":"user_guides/setup/quick_start_with_docker/#dashboard","text":"The Dashboard app is available at http://localhost:8000/ . You can use it to write experiments in Jupyter-style notebooks, manage projects, and visualize datasets used by PostgresML.","title":"Dashboard"},{"location":"user_guides/setup/v2/installation/","text":"Installation \u00b6 The PostgresML deployment consists of two parts: the Postgres extension and the dashboard app. The extension provides all the machine learning functionality and can be used independently. The dashboard app provides a system overview for easier management and notebooks for writing experiments. Extension \u00b6 The extension can be installed from our Ubuntu apt repository or, if you're using a different distribution, from source. Dependencies \u00b6 Python 3.7+ \u00b6 PostgresML 2.0 distributed through apt requires Python 3.7 or higher. We use Python to provide backwards compatibility with Scikit and other machine learning libraries from that ecosystem. You will also need to install the following Python packages: sudo pip3 install xgboost lightgbm scikit-learn Python 3.6 and below \u00b6 If your system Python is older, consider installing a newer version from ppa:deadsnakes/ppa or Homebrew. If you don't want to or can't have Python 3.7 or higher on your system, refer to From Source (Linux & WSL) below for building without Python support. PostgreSQL \u00b6 PostgresML is a Postgres extension and requires PostgreSQL to be installed. We support PostgreSQL 11 through 15. You can use the PostgreSQL version that comes with your system or get it from the PostgreSQL PPA . sudo apt-get update && \\ sudo apt-get install postgresql Install the extension \u00b6 Ubuntu From Source (Linux & WSL) From Source (Mac) Add our repository into your sources: echo \"deb [trusted=yes] https://apt.postgresml.org $( lsb_release -cs ) main\" | sudo tee -a /etc/apt/sources.list Install the extension: export POSTGRES_VERSION=15 sudo apt-get update && sudo apt-get install -y postgresql-pgml-${POSTGRES_VERSION} Both ARM and Intel/AMD architectures are supported. These instructions assume a Debian flavor Linux and PostgreSQL 15. Adjust the PostgreSQL version accordingly if yours is different. Other flavors of Linux should work, but have not been tested. PostgreSQL 11 through 15 are supported. Install the latest Rust compiler from rust-lang.org . Install a modern version of CMake. Install PostgreSQL development headers and other dependencies: export POSTGRES_VERSION = 15 sudo apt-get update && \\ sudo apt-get install -y \\ postgresql-server-dev- ${ POSTGRES_VERSION } \\ bison \\ build-essential \\ clang \\ cmake \\ flex \\ libclang-dev \\ libopenblas-dev \\ libpython3-dev \\ libreadline-dev \\ libssl-dev \\ pkg-config \\ python3-dev Clone our git repository: git clone https://github.com/postgresml/postgresml && \\ cd postgresml && \\ git submodule update --init --recursive && \\ cd pgml-extension Install pgrx and build the extension (this will take a few minutes): With Python support: export POSTGRES_VERSION = 15 cargo install cargo-pgrx --version \"0.8.2\" --locked && \\ cargo pgrx init --pg ${ POSTGRES_VERSION } /usr/bin/pg_config && \\ cargo pgrx package Without Python support: export POSTGRES_VERSION = 15 cp docker/Cargo.toml.no-python Cargo.toml && \\ cargo install cargo-pgrx --version \"0.8.2\" --locked && \\ cargo pgrx init --pg ${ POSTGRES_VERSION } /usr/bin/pg_config && \\ cargo pgrx package Copy the extension binaries into Postgres system folders: export POSTGRES_VERSION = 15 # Copy the extension .so sudo cp target/release/pgml-pg ${ POSTGRES_VERSION } /usr/lib/postgresql/ ${ POSTGRES_VERSION } /lib/* \\ /usr/lib/postgresql/ ${ POSTGRES_VERSION } /lib # Copy the control & SQL files sudo cp target/release/pgml-pg ${ POSTGRES_VERSION } /usr/share/postgresql/ ${ POSTGRES_VERSION } /extension/* \\ /usr/share/postgresql/ ${ POSTGRES_VERSION } /extension N.B. : Apple M1s have an issue with openmp which XGBoost and LightGBM depend on, so presently the extension doesn't work on M1s and M2s. We're tracking this issue . Install the latest Rust compiler from rust-lang.org . Clone our git repository: git clone https://github.com/postgresml/postgresml && \\ cd postgresml && \\ git submodule update --init --recursive && \\ cd pgml-extension Install PostgreSQL and other dependencies: brew install llvm postgresql cmake openssl pkg-config Make sure to follow instructions from each package for post-installation steps. For example, openssl requires some environment variables set in ~/.zsh for the compiler to find the library. Install pgrx and build the extension (this will take a few minutes): cargo install cargo-pgrx && \\ cargo pgrx init --pg15 /usr/bin/pg_config && \\ cargo pgrx install Enable the extension \u00b6 Update postgresql.conf \u00b6 PostgresML needs to be preloaded at server startup, so you need to add it into shared_preload_libraries : shared_preload_libraries = 'pgml,pg_stat_statements' This setting change requires PostgreSQL to be restarted: sudo service postgresql restart Install into database \u00b6 Now that the extension is installed on your system, add it into the database where you'd like to use it: Note If you already have a v1.0 installation, see Upgrading to v2.0 . SQL Output Connect to the database and create the extension: CREATE EXTENSION pgml ; postgres=# CREATE EXTENSION pgml; INFO: Python version: 3.10.4 (main, Jun 29 2022, 12:14:53) [GCC 11.2.0] INFO: Scikit-learn 1.1.3, XGBoost 1.7.1, LightGBM 3.3.3, NumPy 1.23.5 CREATE EXTENSION That's it, PostgresML is ready. You can validate the installation by running: SQL Output SELECT pgml . version (); postgres=# select pgml.version(); version ------------------- 2.4.8 (1 row) Run the dashboard \u00b6 The dashboard is a web app that can be run against any Postgres database with the extension installed. There is a Dockerfile included with the source code if you wish to run it as a container. Basic installation can be achieved with: Clone the repo (if you haven't already for the extension): git clone https://github.com/postgresml/postgresml && cd postgresml/pgml-dashboard Set the DATABASE_URL environment variable: export DATABASE_URL = postgres://user_name:password@localhost:5432/database_name Build and run the web application: cargo run The dashboard can be packaged for distribution. You'll need to copy the static files along with the target/release directory to your server.","title":"v2.0"},{"location":"user_guides/setup/v2/installation/#installation","text":"The PostgresML deployment consists of two parts: the Postgres extension and the dashboard app. The extension provides all the machine learning functionality and can be used independently. The dashboard app provides a system overview for easier management and notebooks for writing experiments.","title":"Installation"},{"location":"user_guides/setup/v2/installation/#extension","text":"The extension can be installed from our Ubuntu apt repository or, if you're using a different distribution, from source.","title":"Extension"},{"location":"user_guides/setup/v2/installation/#dependencies","text":"","title":"Dependencies"},{"location":"user_guides/setup/v2/installation/#python-37","text":"PostgresML 2.0 distributed through apt requires Python 3.7 or higher. We use Python to provide backwards compatibility with Scikit and other machine learning libraries from that ecosystem. You will also need to install the following Python packages: sudo pip3 install xgboost lightgbm scikit-learn","title":"Python 3.7+"},{"location":"user_guides/setup/v2/installation/#python-36-and-below","text":"If your system Python is older, consider installing a newer version from ppa:deadsnakes/ppa or Homebrew. If you don't want to or can't have Python 3.7 or higher on your system, refer to From Source (Linux & WSL) below for building without Python support.","title":"Python 3.6 and below"},{"location":"user_guides/setup/v2/installation/#postgresql","text":"PostgresML is a Postgres extension and requires PostgreSQL to be installed. We support PostgreSQL 11 through 15. You can use the PostgreSQL version that comes with your system or get it from the PostgreSQL PPA . sudo apt-get update && \\ sudo apt-get install postgresql","title":"PostgreSQL"},{"location":"user_guides/setup/v2/installation/#install-the-extension","text":"Ubuntu From Source (Linux & WSL) From Source (Mac) Add our repository into your sources: echo \"deb [trusted=yes] https://apt.postgresml.org $( lsb_release -cs ) main\" | sudo tee -a /etc/apt/sources.list Install the extension: export POSTGRES_VERSION=15 sudo apt-get update && sudo apt-get install -y postgresql-pgml-${POSTGRES_VERSION} Both ARM and Intel/AMD architectures are supported. These instructions assume a Debian flavor Linux and PostgreSQL 15. Adjust the PostgreSQL version accordingly if yours is different. Other flavors of Linux should work, but have not been tested. PostgreSQL 11 through 15 are supported. Install the latest Rust compiler from rust-lang.org . Install a modern version of CMake. Install PostgreSQL development headers and other dependencies: export POSTGRES_VERSION = 15 sudo apt-get update && \\ sudo apt-get install -y \\ postgresql-server-dev- ${ POSTGRES_VERSION } \\ bison \\ build-essential \\ clang \\ cmake \\ flex \\ libclang-dev \\ libopenblas-dev \\ libpython3-dev \\ libreadline-dev \\ libssl-dev \\ pkg-config \\ python3-dev Clone our git repository: git clone https://github.com/postgresml/postgresml && \\ cd postgresml && \\ git submodule update --init --recursive && \\ cd pgml-extension Install pgrx and build the extension (this will take a few minutes): With Python support: export POSTGRES_VERSION = 15 cargo install cargo-pgrx --version \"0.8.2\" --locked && \\ cargo pgrx init --pg ${ POSTGRES_VERSION } /usr/bin/pg_config && \\ cargo pgrx package Without Python support: export POSTGRES_VERSION = 15 cp docker/Cargo.toml.no-python Cargo.toml && \\ cargo install cargo-pgrx --version \"0.8.2\" --locked && \\ cargo pgrx init --pg ${ POSTGRES_VERSION } /usr/bin/pg_config && \\ cargo pgrx package Copy the extension binaries into Postgres system folders: export POSTGRES_VERSION = 15 # Copy the extension .so sudo cp target/release/pgml-pg ${ POSTGRES_VERSION } /usr/lib/postgresql/ ${ POSTGRES_VERSION } /lib/* \\ /usr/lib/postgresql/ ${ POSTGRES_VERSION } /lib # Copy the control & SQL files sudo cp target/release/pgml-pg ${ POSTGRES_VERSION } /usr/share/postgresql/ ${ POSTGRES_VERSION } /extension/* \\ /usr/share/postgresql/ ${ POSTGRES_VERSION } /extension N.B. : Apple M1s have an issue with openmp which XGBoost and LightGBM depend on, so presently the extension doesn't work on M1s and M2s. We're tracking this issue . Install the latest Rust compiler from rust-lang.org . Clone our git repository: git clone https://github.com/postgresml/postgresml && \\ cd postgresml && \\ git submodule update --init --recursive && \\ cd pgml-extension Install PostgreSQL and other dependencies: brew install llvm postgresql cmake openssl pkg-config Make sure to follow instructions from each package for post-installation steps. For example, openssl requires some environment variables set in ~/.zsh for the compiler to find the library. Install pgrx and build the extension (this will take a few minutes): cargo install cargo-pgrx && \\ cargo pgrx init --pg15 /usr/bin/pg_config && \\ cargo pgrx install","title":"Install the extension"},{"location":"user_guides/setup/v2/installation/#enable-the-extension","text":"","title":"Enable the extension"},{"location":"user_guides/setup/v2/installation/#update-postgresqlconf","text":"PostgresML needs to be preloaded at server startup, so you need to add it into shared_preload_libraries : shared_preload_libraries = 'pgml,pg_stat_statements' This setting change requires PostgreSQL to be restarted: sudo service postgresql restart","title":"Update postgresql.conf"},{"location":"user_guides/setup/v2/installation/#install-into-database","text":"Now that the extension is installed on your system, add it into the database where you'd like to use it: Note If you already have a v1.0 installation, see Upgrading to v2.0 . SQL Output Connect to the database and create the extension: CREATE EXTENSION pgml ; postgres=# CREATE EXTENSION pgml; INFO: Python version: 3.10.4 (main, Jun 29 2022, 12:14:53) [GCC 11.2.0] INFO: Scikit-learn 1.1.3, XGBoost 1.7.1, LightGBM 3.3.3, NumPy 1.23.5 CREATE EXTENSION That's it, PostgresML is ready. You can validate the installation by running: SQL Output SELECT pgml . version (); postgres=# select pgml.version(); version ------------------- 2.4.8 (1 row)","title":"Install into database"},{"location":"user_guides/setup/v2/installation/#run-the-dashboard","text":"The dashboard is a web app that can be run against any Postgres database with the extension installed. There is a Dockerfile included with the source code if you wish to run it as a container. Basic installation can be achieved with: Clone the repo (if you haven't already for the extension): git clone https://github.com/postgresml/postgresml && cd postgresml/pgml-dashboard Set the DATABASE_URL environment variable: export DATABASE_URL = postgres://user_name:password@localhost:5432/database_name Build and run the web application: cargo run The dashboard can be packaged for distribution. You'll need to copy the static files along with the target/release directory to your server.","title":"Run the dashboard"},{"location":"user_guides/setup/v2/upgrade-from-v1/","text":"Upgrade a v1.0 installation to v2.0 \u00b6 The API is identical between v1.0 and v2.0, and models trained with v1.0 can be imported into v2.0. Note Make sure you've set up the system requirements in v2.0 installation , so that the v2.0 extension may be installed. Migration \u00b6 You may run this migration to install the v2.0 extension and copy all existing assets from an existing v1.0 installation. -- Run this migration as an atomic step BEGIN ; -- Move the existing installation to a temporary schema ALTER SCHEMA pgml RENAME to pgml_tmp ; -- Create the v2.0 extension CREATE EXTENSION pgml ; -- Copy v1.0 projects into v2.0 INSERT INTO pgml . projects ( id , name , task , created_at , updated_at ) SELECT id , name , task :: pgml . task , created_at , updated_at FROM pgml_tmp . projects ; SELECT setval ( 'pgml.projects_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . projects ), 1 ), false ); -- Copy v1.0 snapshots into v2.0 INSERT INTO pgml . snapshots ( id , relation_name , y_column_name , test_size , test_sampling , status , columns , analysis , created_at , updated_at ) SELECT id , relation_name , y_column_name , test_size , test_sampling :: pgml . sampling , status , columns , analysis , created_at , updated_at FROM pgml_tmp . snapshots ; SELECT setval ( 'pgml.snapshots_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . snapshots ), 1 ), false ); -- Copy v1.0 models into v2.0 INSERT INTO pgml . models ( id , project_id , snapshot_id , num_features , algorithm , hyperparams , status , metrics , search , search_params , search_args , created_at , updated_at ) SELECT models . id , project_id , snapshot_id , ( SELECT count ( * ) FROM jsonb_object_keys ( snapshots . columns )) - array_length ( snapshots . y_column_name , 1 ) num_features , case when algorithm_name = 'orthoganl_matching_pursuit' then 'orthogonal_matching_pursuit' :: pgml . algorithm else algorithm_name :: pgml . algorithm end , hyperparams , models . status , metrics , search , search_params , search_args , models . created_at , models . updated_at FROM pgml_tmp . models JOIN pgml_tmp . snapshots ON snapshots . id = models . snapshot_id ; SELECT setval ( 'pgml.models_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . models ), 1 ), false ); -- Copy v1.0 deployments into v2.0 INSERT INTO pgml . deployments SELECT id , project_id , model_id , strategy :: pgml . strategy , created_at FROM pgml_tmp . deployments ; SELECT setval ( 'pgml.deployments_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . deployments ), 1 ), false ); -- Copy v1.0 files into v2.0 INSERT INTO pgml . files ( id , model_id , path , part , created_at , updated_at , data ) SELECT id , model_id , path , part , created_at , updated_at , data FROM pgml_tmp . files ; SELECT setval ( 'pgml.files_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . files ), 1 ), false ); -- Complete the migration COMMIT ; Cleanup v1.0 \u00b6 Make sure you validate the v2.0 installation first by running some predictions with existing models, before removing the v1.0 installation completely. DROP SCHEMA pgml_tmp ;","title":"Upgrading from v1.0 to v2.0"},{"location":"user_guides/setup/v2/upgrade-from-v1/#upgrade-a-v10-installation-to-v20","text":"The API is identical between v1.0 and v2.0, and models trained with v1.0 can be imported into v2.0. Note Make sure you've set up the system requirements in v2.0 installation , so that the v2.0 extension may be installed.","title":"Upgrade a v1.0 installation to v2.0"},{"location":"user_guides/setup/v2/upgrade-from-v1/#migration","text":"You may run this migration to install the v2.0 extension and copy all existing assets from an existing v1.0 installation. -- Run this migration as an atomic step BEGIN ; -- Move the existing installation to a temporary schema ALTER SCHEMA pgml RENAME to pgml_tmp ; -- Create the v2.0 extension CREATE EXTENSION pgml ; -- Copy v1.0 projects into v2.0 INSERT INTO pgml . projects ( id , name , task , created_at , updated_at ) SELECT id , name , task :: pgml . task , created_at , updated_at FROM pgml_tmp . projects ; SELECT setval ( 'pgml.projects_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . projects ), 1 ), false ); -- Copy v1.0 snapshots into v2.0 INSERT INTO pgml . snapshots ( id , relation_name , y_column_name , test_size , test_sampling , status , columns , analysis , created_at , updated_at ) SELECT id , relation_name , y_column_name , test_size , test_sampling :: pgml . sampling , status , columns , analysis , created_at , updated_at FROM pgml_tmp . snapshots ; SELECT setval ( 'pgml.snapshots_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . snapshots ), 1 ), false ); -- Copy v1.0 models into v2.0 INSERT INTO pgml . models ( id , project_id , snapshot_id , num_features , algorithm , hyperparams , status , metrics , search , search_params , search_args , created_at , updated_at ) SELECT models . id , project_id , snapshot_id , ( SELECT count ( * ) FROM jsonb_object_keys ( snapshots . columns )) - array_length ( snapshots . y_column_name , 1 ) num_features , case when algorithm_name = 'orthoganl_matching_pursuit' then 'orthogonal_matching_pursuit' :: pgml . algorithm else algorithm_name :: pgml . algorithm end , hyperparams , models . status , metrics , search , search_params , search_args , models . created_at , models . updated_at FROM pgml_tmp . models JOIN pgml_tmp . snapshots ON snapshots . id = models . snapshot_id ; SELECT setval ( 'pgml.models_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . models ), 1 ), false ); -- Copy v1.0 deployments into v2.0 INSERT INTO pgml . deployments SELECT id , project_id , model_id , strategy :: pgml . strategy , created_at FROM pgml_tmp . deployments ; SELECT setval ( 'pgml.deployments_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . deployments ), 1 ), false ); -- Copy v1.0 files into v2.0 INSERT INTO pgml . files ( id , model_id , path , part , created_at , updated_at , data ) SELECT id , model_id , path , part , created_at , updated_at , data FROM pgml_tmp . files ; SELECT setval ( 'pgml.files_id_seq' , COALESCE (( SELECT MAX ( id ) + 1 FROM pgml . files ), 1 ), false ); -- Complete the migration COMMIT ;","title":"Migration"},{"location":"user_guides/setup/v2/upgrade-from-v1/#cleanup-v10","text":"Make sure you validate the v2.0 installation first by running some predictions with existing models, before removing the v1.0 installation completely. DROP SCHEMA pgml_tmp ;","title":"Cleanup v1.0"},{"location":"user_guides/training/algorithm_selection/","text":"Algorithm Selection \u00b6 We currently support regression and classification algorithms from scikit-learn , XGBoost , and LightGBM . Algorithms \u00b6 Gradient Boosting \u00b6 Algorithm Regression Classification xgboost XGBRegressor XGBClassifier xgboost_random_forest XGBRFRegressor XGBRFClassifier lightgbm LGBMRegressor LGBMClassifier Scikit Ensembles \u00b6 Algorithm Regression Classification ada_boost AdaBoostRegressor AdaBoostClassifier bagging BaggingRegressor BaggingClassifier extra_trees ExtraTreesRegressor ExtraTreesClassifier gradient_boosting_trees GradientBoostingRegressor GradientBoostingClassifier random_forest RandomForestRegressor RandomForestClassifier hist_gradient_boosting HistGradientBoostingRegressor HistGradientBoostingClassifier Support Vector Machines \u00b6 Algorithm Regression Classification svm SVR SVC nu_svm NuSVR NuSVC linear_svm LinearSVR LinearSVC Linear Models \u00b6 Algorithm Regression Classification linear LinearRegression LogisticRegression ridge Ridge RidgeClassifier lasso Lasso - elastic_net ElasticNet - least_angle LARS - lasso_least_angle LassoLars - orthoganl_matching_pursuit OrthogonalMatchingPursuit - bayesian_ridge BayesianRidge - automatic_relevance_determination ARDRegression - stochastic_gradient_descent SGDRegressor SGDClassifier perceptron - Perceptron passive_aggressive PassiveAggressiveRegressor PassiveAggressiveClassifier ransac RANSACRegressor - theil_sen TheilSenRegressor - huber HuberRegressor - quantile QuantileRegressor - Other \u00b6 Algorithm Regression Classification kernel_ridge KernelRidge - gaussian_process GaussianProcessRegressor GaussianProcessClassifier Comparing Algorithms \u00b6 Any of the above algorithms can be passed to our pgml.train() function using the algorithm parameter. If the parameter is omitted, linear regression is used by default. Example SELECT * FROM pgml . train ( 'My First PostgresML Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' , ); The hyperparams argument will pass the hyperparameters on to the algorithm. Take a look at the associated documentation for valid hyperparameters of each algorithm. Our interface uses the scikit-learn notation for all parameters. Example SELECT * FROM pgml . train ( 'My First PostgresML Project' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); Once prepared, the training data can be efficiently reused by other PostgresML algorithms for training and predictions. Every time the pgml.train() function receives the relation_name and y_column_name arguments, it will create a new snapshot of the relation (table) and save it in the pgml schema. To train another algorithm on the same dataset, omit the two arguments. PostgresML will reuse the latest snapshot with the new algorithm. Tip Try experimenting with multiple algorithms to explore their performance characteristics on your dataset. It's often hard to know which algorithm will be the best. Dashboard \u00b6 The PostgresML dashboard makes it easy to compare various algorithms on your dataset. You can explore individual metrics & compare algorithms to each other, all trained on the same dataset for a fair benchmark.","title":"Algorithm Selection"},{"location":"user_guides/training/algorithm_selection/#algorithm-selection","text":"We currently support regression and classification algorithms from scikit-learn , XGBoost , and LightGBM .","title":"Algorithm Selection"},{"location":"user_guides/training/algorithm_selection/#algorithms","text":"","title":"Algorithms"},{"location":"user_guides/training/algorithm_selection/#gradient-boosting","text":"Algorithm Regression Classification xgboost XGBRegressor XGBClassifier xgboost_random_forest XGBRFRegressor XGBRFClassifier lightgbm LGBMRegressor LGBMClassifier","title":"Gradient Boosting"},{"location":"user_guides/training/algorithm_selection/#scikit-ensembles","text":"Algorithm Regression Classification ada_boost AdaBoostRegressor AdaBoostClassifier bagging BaggingRegressor BaggingClassifier extra_trees ExtraTreesRegressor ExtraTreesClassifier gradient_boosting_trees GradientBoostingRegressor GradientBoostingClassifier random_forest RandomForestRegressor RandomForestClassifier hist_gradient_boosting HistGradientBoostingRegressor HistGradientBoostingClassifier","title":"Scikit Ensembles"},{"location":"user_guides/training/algorithm_selection/#support-vector-machines","text":"Algorithm Regression Classification svm SVR SVC nu_svm NuSVR NuSVC linear_svm LinearSVR LinearSVC","title":"Support Vector Machines"},{"location":"user_guides/training/algorithm_selection/#linear-models","text":"Algorithm Regression Classification linear LinearRegression LogisticRegression ridge Ridge RidgeClassifier lasso Lasso - elastic_net ElasticNet - least_angle LARS - lasso_least_angle LassoLars - orthoganl_matching_pursuit OrthogonalMatchingPursuit - bayesian_ridge BayesianRidge - automatic_relevance_determination ARDRegression - stochastic_gradient_descent SGDRegressor SGDClassifier perceptron - Perceptron passive_aggressive PassiveAggressiveRegressor PassiveAggressiveClassifier ransac RANSACRegressor - theil_sen TheilSenRegressor - huber HuberRegressor - quantile QuantileRegressor -","title":"Linear Models"},{"location":"user_guides/training/algorithm_selection/#other","text":"Algorithm Regression Classification kernel_ridge KernelRidge - gaussian_process GaussianProcessRegressor GaussianProcessClassifier","title":"Other"},{"location":"user_guides/training/algorithm_selection/#comparing-algorithms","text":"Any of the above algorithms can be passed to our pgml.train() function using the algorithm parameter. If the parameter is omitted, linear regression is used by default. Example SELECT * FROM pgml . train ( 'My First PostgresML Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' , algorithm => 'xgboost' , ); The hyperparams argument will pass the hyperparameters on to the algorithm. Take a look at the associated documentation for valid hyperparameters of each algorithm. Our interface uses the scikit-learn notation for all parameters. Example SELECT * FROM pgml . train ( 'My First PostgresML Project' , algorithm => 'xgboost' , hyperparams => '{ \"n_estimators\": 25 }' ); Once prepared, the training data can be efficiently reused by other PostgresML algorithms for training and predictions. Every time the pgml.train() function receives the relation_name and y_column_name arguments, it will create a new snapshot of the relation (table) and save it in the pgml schema. To train another algorithm on the same dataset, omit the two arguments. PostgresML will reuse the latest snapshot with the new algorithm. Tip Try experimenting with multiple algorithms to explore their performance characteristics on your dataset. It's often hard to know which algorithm will be the best.","title":"Comparing Algorithms"},{"location":"user_guides/training/algorithm_selection/#dashboard","text":"The PostgresML dashboard makes it easy to compare various algorithms on your dataset. You can explore individual metrics & compare algorithms to each other, all trained on the same dataset for a fair benchmark.","title":"Dashboard"},{"location":"user_guides/training/hyperparameter_search/","text":"Hyperparameter Search \u00b6 Models can be further refined by using hyperparameter search and cross validation. We currently support random and grid search algorithms, and k-fold cross validation. API \u00b6 The parameters passed to pgml.train() easily allow one to perform hyperparameter tuning. The three parameters relevant to this are: search , search_params and search_args . Parameter Example search grid search_params {\"alpha\": [0.1, 0.2, 0.5] } search_args {\"n_iter\": 10 } Example SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , algorithm => 'xgboost' , search => 'grid' , search_params => '{ \"max_depth\": [1, 2, 3, 4, 5, 6], \"n_estimators\": [20, 40, 80, 160] }' ); You may pass any of the arguments listed in the algorithms documentation as hyperparameters. See Algorithms for the complete list of algorithms and their associated hyperparameters. Search Algorithms \u00b6 We currently support two search algorithms: random and grid . Algorithm Description grid Trains every permutation of search_params using a cartesian product. random Randomly samples search_params up to n_iter number of iterations provided in search_args . Analysis \u00b6 PostgresML automatically selects the optimal set of hyperparameters for the model, and that combination is highlighted in the Dashboard, among all other search candidates. The impact of each hyperparameter is measured against the key metric ( r2 for regression and f1 for classification), as well as the training and test times. Tip In our example case, it's interesting that as max_depth increases, the \"Test Score\" on the key metric trends lower, so the smallest value of max_depth is chosen to maximize the \"Test Score\". Luckily, the smallest max_depth values also have the fastest \"Fit Time\", indicating that we pay less for training these higher quality models. It's a little less obvious how the different values n_estimators and learning_rate impact the test score. We may want to rerun our search and zoom in on our the search space to get more insight. Performance \u00b6 In our example above, the grid search will train len(max_depth) * len(n_estimators) * len(learning_rate) = 6 * 4 * 4 = 96 combinations to compare all possible permutations of search_params . It only took about a minute on my computer because we're using optimized Rust/C++ XGBoost bindings, but you can delete some values if you want to speed things up even further. I like to watch all cores operate at 100% utilization in a separate terminal with htop : In the end, we get the following output: project | task | algorithm | deployed ------------------------------------+----------------+-----------+---------- Handwritten Digit Image Classifier | classification | xgboost | t (1 row) A new model has been deployed with better performance and metrics. There will also be a new analysis available for this model, viewable in the dashboard.","title":"Hyperparameter Search"},{"location":"user_guides/training/hyperparameter_search/#hyperparameter-search","text":"Models can be further refined by using hyperparameter search and cross validation. We currently support random and grid search algorithms, and k-fold cross validation.","title":"Hyperparameter Search"},{"location":"user_guides/training/hyperparameter_search/#api","text":"The parameters passed to pgml.train() easily allow one to perform hyperparameter tuning. The three parameters relevant to this are: search , search_params and search_args . Parameter Example search grid search_params {\"alpha\": [0.1, 0.2, 0.5] } search_args {\"n_iter\": 10 } Example SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , algorithm => 'xgboost' , search => 'grid' , search_params => '{ \"max_depth\": [1, 2, 3, 4, 5, 6], \"n_estimators\": [20, 40, 80, 160] }' ); You may pass any of the arguments listed in the algorithms documentation as hyperparameters. See Algorithms for the complete list of algorithms and their associated hyperparameters.","title":"API"},{"location":"user_guides/training/hyperparameter_search/#search-algorithms","text":"We currently support two search algorithms: random and grid . Algorithm Description grid Trains every permutation of search_params using a cartesian product. random Randomly samples search_params up to n_iter number of iterations provided in search_args .","title":"Search Algorithms"},{"location":"user_guides/training/hyperparameter_search/#analysis","text":"PostgresML automatically selects the optimal set of hyperparameters for the model, and that combination is highlighted in the Dashboard, among all other search candidates. The impact of each hyperparameter is measured against the key metric ( r2 for regression and f1 for classification), as well as the training and test times. Tip In our example case, it's interesting that as max_depth increases, the \"Test Score\" on the key metric trends lower, so the smallest value of max_depth is chosen to maximize the \"Test Score\". Luckily, the smallest max_depth values also have the fastest \"Fit Time\", indicating that we pay less for training these higher quality models. It's a little less obvious how the different values n_estimators and learning_rate impact the test score. We may want to rerun our search and zoom in on our the search space to get more insight.","title":"Analysis"},{"location":"user_guides/training/hyperparameter_search/#performance","text":"In our example above, the grid search will train len(max_depth) * len(n_estimators) * len(learning_rate) = 6 * 4 * 4 = 96 combinations to compare all possible permutations of search_params . It only took about a minute on my computer because we're using optimized Rust/C++ XGBoost bindings, but you can delete some values if you want to speed things up even further. I like to watch all cores operate at 100% utilization in a separate terminal with htop : In the end, we get the following output: project | task | algorithm | deployed ------------------------------------+----------------+-----------+---------- Handwritten Digit Image Classifier | classification | xgboost | t (1 row) A new model has been deployed with better performance and metrics. There will also be a new analysis available for this model, viewable in the dashboard.","title":"Performance"},{"location":"user_guides/training/joint_optimization/","text":"Joint Optimization \u00b6 Some algorithms support joint optimization of the task across multiple outputs, which can improve results compared to using multiple independent models. To leverage multiple outputs in PostgresML, you'll need to substitute the standard usage of pgml.train() with pgml.train_joint() , which has the same API, except the notable exception of y_column_name parameter, which now accepts an array instead of a simple string. Example SELECT * FROM pgml . train_join ( 'My Joint Project' , task => 'regression' , relation_name => 'my_table' , y_column_name => ARRAY [ 'target_a' , 'target_b' ], ); You can read more in scikit-learn documentation.","title":"Joint Optimization"},{"location":"user_guides/training/joint_optimization/#joint-optimization","text":"Some algorithms support joint optimization of the task across multiple outputs, which can improve results compared to using multiple independent models. To leverage multiple outputs in PostgresML, you'll need to substitute the standard usage of pgml.train() with pgml.train_joint() , which has the same API, except the notable exception of y_column_name parameter, which now accepts an array instead of a simple string. Example SELECT * FROM pgml . train_join ( 'My Joint Project' , task => 'regression' , relation_name => 'my_table' , y_column_name => ARRAY [ 'target_a' , 'target_b' ], ); You can read more in scikit-learn documentation.","title":"Joint Optimization"},{"location":"user_guides/training/overview/","text":"Training Models \u00b6 The training function is at the heart of PostgresML. It's a powerful single mechanism that can handle many different training tasks which are configurable with the function parameters. API \u00b6 Most parameters are optional and have configured defaults. The project_name parameter is required and is an easily recognizable identifier to organize your work. pgml.train() pgml . train ( project_name TEXT , task TEXT DEFAULT NULL , relation_name TEXT DEFAULT NULL , y_column_name TEXT DEFAULT NULL , algorithm TEXT DEFAULT 'linear' , hyperparams JSONB DEFAULT '{}' :: JSONB , search TEXT DEFAULT NULL , search_params JSONB DEFAULT '{}' :: JSONB , search_args JSONB DEFAULT '{}' :: JSONB , test_size REAL DEFAULT 0.25 , test_sampling TEXT DEFAULT 'random' ) Parameters \u00b6 Parameter Description Example project_name An easily recognizable identifier to organize your work. My First PostgresML Project task The objective of the experiment: regression or classification . classification relation_name The Postgres table or view where the training data is stored or defined. public.users y_column_name The name of the label (aka \"target\" or \"unknown\") column in the training table. is_bot algorithm The algorithm to train on the dataset, see Algorithm Selection for details. xgboost hyperparams The hyperparameters to pass to the algorithm for training, JSON formatted. { \"n_estimators\": 25 } search If set, PostgresML will perform a hyperparameter search to find the best hyperparameters for the algorithm. See Hyperparameter Search for details. grid search_params Search parameters used in the hyperparameter search, using the scikit-learn notation, JSON formatted. { \"n_estimators\": [5, 10, 25, 100] } search_args Configuration parameters for the search, JSON formatted. Currently only n_iter is supported for random search. { \"n_iter\": 10 } test_size Fraction of the dataset to use for the test set and algorithm validation. 0.25 test_sampling Algorithm used to fetch test data from the dataset: random , first , or last . random Example SELECT * FROM pgml . train ( project_name => 'My Classification Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' ); This will create a \"My Classification Project\", copy the pgml.digits table into the pgml schema, naming it pgml.snapshot_{id} where id is the primary key of the snapshot, and train a linear classification model on the snapshot using the target column as the label. When used for the first time in a project, pgml.train() function requires the task parameter, which can be either regression or classification . The task determines the relevant metrics and analysis performed on the data. All models trained within the project will refer to those metrics and analysis for benchmarking and deployment. The first time it's called, the function will also require a relation_name and y_column_name . The two arguments will be used to create the first snapshot of training and test data. By default, 25% of the data (specified by the test_size parameter) will be randomly sampled to measure the performance of the model after the algorithm has been trained on the 75% of the data. Tip Postgres supports named arguments in functions, so you can easily recognize them and pass them as needed: SELECT * FROM pgml . train ( 'My Classification Project' , algorithm => 'xgboost' ); Future calls to pgml.train() may restate the same task for a project or omit it, but they can't change it. Projects manage their deployed model using the metrics relevant to a particular task (e.g. r2 or f1 ), so changing it would mean some models in the project are no longer directly comparable. In that case, it's better to start a new project. Note If you'd like to train multiple models on the same snapshot, follow up calls to pgml.train() may omit the relation_name , y_column_name , test_size and test_sampling arguments to reuse identical data with multiple algorithms or hyperparameters. The snapshot is always saved after training runs if any follow up analysis required. Getting Training Data \u00b6 A large part of the machine learning workflow is acquiring, cleaning, and preparing data for training algorithms. Naturally, we think Postgres is a great place to store your data. For the purpose of this example, we'll load a toy dataset, the classic handwritten digits image collection, from scikit-learn. SQL Output SELECT * FROM pgml . load_dataset ( 'digits' ); pgml=# SELECT * FROM pgml.load_dataset('digits'); NOTICE: table \"digits\" does not exist, skipping table_name | rows -------------+------ pgml.digits | 1797 (1 row) This NOTICE can safely be ignored. PostgresML attempts to do a clean reload by dropping the pgml.digits table if it exists. The first time this command is run, the table does not exist. PostgresML loaded the Digits dataset into the pgml.digits table. You can examine the 2D arrays of image data, as well as the label in the target column: SQL Output SELECT target , image FROM pgml . digits LIMIT 5 ; target | image --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 | {{0,0,5,13,9,1,0,0},{0,0,13,15,10,15,5,0},{0,3,15,2,0,11,8,0},{0,4,12,0,0,8,8,0},{0,5,8,0,0,9,8,0},{0,4,11,0,1,12,7,0},{0,2,14,5,10,12,0,0},{0,0,6,13,10,0,0,0}} 1 | {{0,0,0,12,13,5,0,0},{0,0,0,11,16,9,0,0},{0,0,3,15,16,6,0,0},{0,7,15,16,16,2,0,0},{0,0,1,16,16,3,0,0},{0,0,1,16,16,6,0,0},{0,0,1,16,16,6,0,0},{0,0,0,11,16,10,0,0}} 2 | {{0,0,0,4,15,12,0,0},{0,0,3,16,15,14,0,0},{0,0,8,13,8,16,0,0},{0,0,1,6,15,11,0,0},{0,1,8,13,15,1,0,0},{0,9,16,16,5,0,0,0},{0,3,13,16,16,11,5,0},{0,0,0,3,11,16,9,0}} 3 | {{0,0,7,15,13,1,0,0},{0,8,13,6,15,4,0,0},{0,2,1,13,13,0,0,0},{0,0,2,15,11,1,0,0},{0,0,0,1,12,12,1,0},{0,0,0,0,1,10,8,0},{0,0,8,4,5,14,9,0},{0,0,7,13,13,9,0,0}} 4 | {{0,0,0,1,11,0,0,0},{0,0,0,7,8,0,0,0},{0,0,1,13,6,2,2,0},{0,0,7,15,0,9,8,0},{0,5,16,10,0,16,6,0},{0,4,15,16,13,16,1,0},{0,0,0,3,15,10,0,0},{0,0,0,2,16,4,0,0}} (5 rows) Training a Model \u00b6 Now that we've got data, we're ready to train a model using an algorithm. We'll start with the default linear algorithm to demonstrate the basics. See the Algorithms for a complete list of available algorithms. SQL Output SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , 'classification' , 'pgml.digits' , 'target' ); INFO: Snapshotting table \"pgml.digits\", this may take a little while... INFO: Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_1\" INFO: Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 } INFO: Training Model { id: 1, algorithm: linear, runtime: python } INFO: Hyperparameter searches: 1, cross validation folds: 1 INFO: Hyperparams: {} INFO: Metrics: { \"f1\": 0.91903764, \"precision\": 0.9175061, \"recall\": 0.9205743, \"accuracy\": 0.9175947, \"mcc\": 0.90866333, \"fit_time\": 0.17586434, \"score_time\": 0.01282608 } project | task | algorithm | deployed ------------------------------------+----------------+-----------+---------- Handwritten Digit Image Classifier | classification | linear | t (1 row) The output gives us information about the training run, including the deployed status. This is great news indicating training has successfully reached a new high score for the project's key metric and our new model was automatically deployed as the one that will be used to make new predictions for the project. See Deployments for a guide to managing the active model. Inspecting the results \u00b6 Now we can inspect some of the artifacts a training run creates. SQL Output 1 SELECT * FROM pgml . overview ; pgml=# SELECT * FROM pgml.overview; name | deployed_at | task | algorithm | runtime | relation_name | y_column_name | test_sampling | test_size ------------------------------------+----------------------------+----------------+-----------+---------+---------------+---------------+---------------+----------- Handwritten Digit Image Classifier | 2022-10-11 12:43:15.346482 | classification | linear | python | pgml.digits | {target} | last | 0.25 (1 row) More Examples \u00b6 See examples in our git repository for more kinds of training with different types of features, algorithms and tasks.","title":"Training Overview"},{"location":"user_guides/training/overview/#training-models","text":"The training function is at the heart of PostgresML. It's a powerful single mechanism that can handle many different training tasks which are configurable with the function parameters.","title":"Training Models"},{"location":"user_guides/training/overview/#api","text":"Most parameters are optional and have configured defaults. The project_name parameter is required and is an easily recognizable identifier to organize your work. pgml.train() pgml . train ( project_name TEXT , task TEXT DEFAULT NULL , relation_name TEXT DEFAULT NULL , y_column_name TEXT DEFAULT NULL , algorithm TEXT DEFAULT 'linear' , hyperparams JSONB DEFAULT '{}' :: JSONB , search TEXT DEFAULT NULL , search_params JSONB DEFAULT '{}' :: JSONB , search_args JSONB DEFAULT '{}' :: JSONB , test_size REAL DEFAULT 0.25 , test_sampling TEXT DEFAULT 'random' )","title":"API"},{"location":"user_guides/training/overview/#parameters","text":"Parameter Description Example project_name An easily recognizable identifier to organize your work. My First PostgresML Project task The objective of the experiment: regression or classification . classification relation_name The Postgres table or view where the training data is stored or defined. public.users y_column_name The name of the label (aka \"target\" or \"unknown\") column in the training table. is_bot algorithm The algorithm to train on the dataset, see Algorithm Selection for details. xgboost hyperparams The hyperparameters to pass to the algorithm for training, JSON formatted. { \"n_estimators\": 25 } search If set, PostgresML will perform a hyperparameter search to find the best hyperparameters for the algorithm. See Hyperparameter Search for details. grid search_params Search parameters used in the hyperparameter search, using the scikit-learn notation, JSON formatted. { \"n_estimators\": [5, 10, 25, 100] } search_args Configuration parameters for the search, JSON formatted. Currently only n_iter is supported for random search. { \"n_iter\": 10 } test_size Fraction of the dataset to use for the test set and algorithm validation. 0.25 test_sampling Algorithm used to fetch test data from the dataset: random , first , or last . random Example SELECT * FROM pgml . train ( project_name => 'My Classification Project' , task => 'classification' , relation_name => 'pgml.digits' , y_column_name => 'target' ); This will create a \"My Classification Project\", copy the pgml.digits table into the pgml schema, naming it pgml.snapshot_{id} where id is the primary key of the snapshot, and train a linear classification model on the snapshot using the target column as the label. When used for the first time in a project, pgml.train() function requires the task parameter, which can be either regression or classification . The task determines the relevant metrics and analysis performed on the data. All models trained within the project will refer to those metrics and analysis for benchmarking and deployment. The first time it's called, the function will also require a relation_name and y_column_name . The two arguments will be used to create the first snapshot of training and test data. By default, 25% of the data (specified by the test_size parameter) will be randomly sampled to measure the performance of the model after the algorithm has been trained on the 75% of the data. Tip Postgres supports named arguments in functions, so you can easily recognize them and pass them as needed: SELECT * FROM pgml . train ( 'My Classification Project' , algorithm => 'xgboost' ); Future calls to pgml.train() may restate the same task for a project or omit it, but they can't change it. Projects manage their deployed model using the metrics relevant to a particular task (e.g. r2 or f1 ), so changing it would mean some models in the project are no longer directly comparable. In that case, it's better to start a new project. Note If you'd like to train multiple models on the same snapshot, follow up calls to pgml.train() may omit the relation_name , y_column_name , test_size and test_sampling arguments to reuse identical data with multiple algorithms or hyperparameters. The snapshot is always saved after training runs if any follow up analysis required.","title":"Parameters"},{"location":"user_guides/training/overview/#getting-training-data","text":"A large part of the machine learning workflow is acquiring, cleaning, and preparing data for training algorithms. Naturally, we think Postgres is a great place to store your data. For the purpose of this example, we'll load a toy dataset, the classic handwritten digits image collection, from scikit-learn. SQL Output SELECT * FROM pgml . load_dataset ( 'digits' ); pgml=# SELECT * FROM pgml.load_dataset('digits'); NOTICE: table \"digits\" does not exist, skipping table_name | rows -------------+------ pgml.digits | 1797 (1 row) This NOTICE can safely be ignored. PostgresML attempts to do a clean reload by dropping the pgml.digits table if it exists. The first time this command is run, the table does not exist. PostgresML loaded the Digits dataset into the pgml.digits table. You can examine the 2D arrays of image data, as well as the label in the target column: SQL Output SELECT target , image FROM pgml . digits LIMIT 5 ; target | image --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 | {{0,0,5,13,9,1,0,0},{0,0,13,15,10,15,5,0},{0,3,15,2,0,11,8,0},{0,4,12,0,0,8,8,0},{0,5,8,0,0,9,8,0},{0,4,11,0,1,12,7,0},{0,2,14,5,10,12,0,0},{0,0,6,13,10,0,0,0}} 1 | {{0,0,0,12,13,5,0,0},{0,0,0,11,16,9,0,0},{0,0,3,15,16,6,0,0},{0,7,15,16,16,2,0,0},{0,0,1,16,16,3,0,0},{0,0,1,16,16,6,0,0},{0,0,1,16,16,6,0,0},{0,0,0,11,16,10,0,0}} 2 | {{0,0,0,4,15,12,0,0},{0,0,3,16,15,14,0,0},{0,0,8,13,8,16,0,0},{0,0,1,6,15,11,0,0},{0,1,8,13,15,1,0,0},{0,9,16,16,5,0,0,0},{0,3,13,16,16,11,5,0},{0,0,0,3,11,16,9,0}} 3 | {{0,0,7,15,13,1,0,0},{0,8,13,6,15,4,0,0},{0,2,1,13,13,0,0,0},{0,0,2,15,11,1,0,0},{0,0,0,1,12,12,1,0},{0,0,0,0,1,10,8,0},{0,0,8,4,5,14,9,0},{0,0,7,13,13,9,0,0}} 4 | {{0,0,0,1,11,0,0,0},{0,0,0,7,8,0,0,0},{0,0,1,13,6,2,2,0},{0,0,7,15,0,9,8,0},{0,5,16,10,0,16,6,0},{0,4,15,16,13,16,1,0},{0,0,0,3,15,10,0,0},{0,0,0,2,16,4,0,0}} (5 rows)","title":"Getting Training Data"},{"location":"user_guides/training/overview/#training-a-model","text":"Now that we've got data, we're ready to train a model using an algorithm. We'll start with the default linear algorithm to demonstrate the basics. See the Algorithms for a complete list of available algorithms. SQL Output SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , 'classification' , 'pgml.digits' , 'target' ); INFO: Snapshotting table \"pgml.digits\", this may take a little while... INFO: Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_1\" INFO: Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 } INFO: Training Model { id: 1, algorithm: linear, runtime: python } INFO: Hyperparameter searches: 1, cross validation folds: 1 INFO: Hyperparams: {} INFO: Metrics: { \"f1\": 0.91903764, \"precision\": 0.9175061, \"recall\": 0.9205743, \"accuracy\": 0.9175947, \"mcc\": 0.90866333, \"fit_time\": 0.17586434, \"score_time\": 0.01282608 } project | task | algorithm | deployed ------------------------------------+----------------+-----------+---------- Handwritten Digit Image Classifier | classification | linear | t (1 row) The output gives us information about the training run, including the deployed status. This is great news indicating training has successfully reached a new high score for the project's key metric and our new model was automatically deployed as the one that will be used to make new predictions for the project. See Deployments for a guide to managing the active model.","title":"Training a Model"},{"location":"user_guides/training/overview/#inspecting-the-results","text":"Now we can inspect some of the artifacts a training run creates. SQL Output 1 SELECT * FROM pgml . overview ; pgml=# SELECT * FROM pgml.overview; name | deployed_at | task | algorithm | runtime | relation_name | y_column_name | test_sampling | test_size ------------------------------------+----------------------------+----------------+-----------+---------+---------------+---------------+---------------+----------- Handwritten Digit Image Classifier | 2022-10-11 12:43:15.346482 | classification | linear | python | pgml.digits | {target} | last | 0.25 (1 row)","title":"Inspecting the results"},{"location":"user_guides/training/overview/#more-examples","text":"See examples in our git repository for more kinds of training with different types of features, algorithms and tasks.","title":"More Examples"},{"location":"user_guides/training/preprocessing/","text":"Preprocessing Data \u00b6 The training function also provides the option to preprocess data with the preprocess param. Preprocessors can be configured on a per-column basis for the training data set. There are currently three types of preprocessing available, for both categorical and quantitative variables. Below is a brief example for training data to learn a model of whether we should carry an umbrella or not. Note Preprocessing steps are saved after training, and repeated identically for future calls to predict . weather_data \u00b6 month clouds humidity temp rain 'jan' 'cumulus' 0.8 5 true 'jan' NULL 0.1 10 false \u2026 \u2026 \u2026 \u2026 \u2026 'dec' 'nimbus' 0.9 -2 false In this example: - month is an ordinal categorical TEXT variable - clouds is a nullable nominal categorical INT4 variable - humidity is a continuous quantitative FLOAT4 variable - temp is a discrete quantitative INT4 variable - rain is a nominal categorical BOOL label There are 3 steps to preprocessing data: Encoding categorical values into quantitative values Imputing NULL values to some quantitative value Scaling quantitative values across all variables to similar ranges These preprocessing steps may be specified on a per-column basis to the train() function. By default, PostgresML does minimal preprocessing on training data, and will raise an error during analysis if NULL values are encountered without a preprocessor. All types other than TEXT are treated as quantitative variables and cast to floating point representations before passing them to the underlying algorithm implementations. pgml.train() SELECT pgml . train ( project_name => 'preprocessed_model' , task => 'classification' , relation_name => 'weather_data' , target => 'rain' , preprocess => '{ \"month\": {\"encode\": {\"ordinal\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]}} \"clouds\": {\"encode\": \"target\", scale: \"standard\"} \"humidity\": {\"impute\": \"mean\", scale: \"standard\"} \"temp\": {\"scale\": \"standard\"} }' ); In some cases, it may make sense to use multiple steps for a single column. For example, the clouds column will be target encoded, and then scaled to the standard range to avoid dominating other variables, but there are some interactions between preprocessors to keep in mind. NULL and NaN are treated as additional, independent categories if seen during training, so columns that encode will only ever impute novel when novel data is encountered during training values. It usually makes sense to scale all variables to the same scale. It does not usually help to scale or preprocess the target data, as that is essentially the problem formulation and/or task selection. Note TEXT is used in this document to also refer to VARCHAR and CHAR(N) types. Predicting with Preprocessors \u00b6 A model that has been trained with preprocessors should use a Postgres tuple for prediction, rather than a FLOAT4[] . Tuples may contain multiple different types (like TEXT and BIGINT ), while an ARRAY may only contain a single type. You can use parenthesis around values to create a Postgres tuple. pgml.predict() SELECT pgml . predict ( 'preprocessed_model' , ( 'jan' , 'nimbus' , 0.5 , 7 )); Categorical encodings \u00b6 Encoding categorical variables is an O(N log(M)) where N is the number of rows, and M is the number of distinct categories. name description none Default - Casts the variable to a 32-bit floating point representation compatible with numerics. This is the default for non- TEXT values. target Encodes the variable as the average value of the target label for all members of the category. This is the default for TEXT variables. one_hot Encodes the variable as multiple independent boolean columns. ordinal Encodes the variable as integer values provided by their position in the input array. NULLS are always 0. target encoding \u00b6 Target encoding is a relatively efficient way to represent a categorical variable. The average value of the target is computed for each category in the training data set. It is reasonable to scale target encoded variables using the same method as other variables. preprocess => '{ \"clouds\": {\"encode\": \"target\" } } Note Target encoding is currently limited to the first label column specified in a joint optimization model when there are multiple labels. one_hot encoding \u00b6 One-hot encoding converts each category into an independent boolean column, where all columns are false except the one column the instance is a member of. This is generally not as efficient or as effective as target encoding because the number of additional columns for a single feature can swamp the other features, regardless of scaling in some algorithms. In addition, the columns are highly correlated which can also cause quality issues in some algorithms. PostgresML drops one column by default to break the correlation but preserves the information, which is also referred to as dummy encoding. preprocess => '{ \"clouds\": {\"encode\": \"one_hot\" } } Note All one-hot encoded data is scaled from 0-1 by definition, and will not be further scaled, unlike the other encodings which are scaled. ordinal encoding \u00b6 Some categorical variables have a natural ordering, like months of the year, or days of the week that can be effectively treated as a discrete quantitative variable. You may set the order of your categorical values, by passing an exhaustive ordered array. e.g. preprocess => '{ \"month\": {\"encode\": {\"ordinal\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]}} } Imputing missing values \u00b6 NULL and NAN values can be replaced by several statistical measures observed in the training data. name description error Default - will abort training or inference when a NULL or NAN is encountered mean the mean value of the variable in the training data set median the middle value of the variable in the sorted training data set mode the most common value of the variable in the training data set min the minimum value of the variable in the training data set max the maximum value of the variable in the training data set zero replaces all missing values with 0.0 Example preprocess => '{ \"temp\": {\"impute\": \"mean\"} } Scaling values \u00b6 Scaling all variables to a standardized range can help make sure that no feature dominates the model, strictly because it has a naturally larger scale. name description preserve Default - Does not scale the variable at all. standard Scales data to have a mean of zero, and variance of one. min_max Scales data from zero to one. The minimum becomes 0.0 and maximum becomes 1.0. max_abs Scales data from -1.0 to +1.0. Data will not be centered around 0, unless abs(min) == abs(max). robust Scales data as a factor of the first and third quartiles. This method may handle outliers more robustly than others. Example preprocess => '{ \"temp\": {\"scale\": \"standard\"} }","title":"Preprocessing Data"},{"location":"user_guides/training/preprocessing/#preprocessing-data","text":"The training function also provides the option to preprocess data with the preprocess param. Preprocessors can be configured on a per-column basis for the training data set. There are currently three types of preprocessing available, for both categorical and quantitative variables. Below is a brief example for training data to learn a model of whether we should carry an umbrella or not. Note Preprocessing steps are saved after training, and repeated identically for future calls to predict .","title":"Preprocessing Data"},{"location":"user_guides/training/preprocessing/#weather_data","text":"month clouds humidity temp rain 'jan' 'cumulus' 0.8 5 true 'jan' NULL 0.1 10 false \u2026 \u2026 \u2026 \u2026 \u2026 'dec' 'nimbus' 0.9 -2 false In this example: - month is an ordinal categorical TEXT variable - clouds is a nullable nominal categorical INT4 variable - humidity is a continuous quantitative FLOAT4 variable - temp is a discrete quantitative INT4 variable - rain is a nominal categorical BOOL label There are 3 steps to preprocessing data: Encoding categorical values into quantitative values Imputing NULL values to some quantitative value Scaling quantitative values across all variables to similar ranges These preprocessing steps may be specified on a per-column basis to the train() function. By default, PostgresML does minimal preprocessing on training data, and will raise an error during analysis if NULL values are encountered without a preprocessor. All types other than TEXT are treated as quantitative variables and cast to floating point representations before passing them to the underlying algorithm implementations. pgml.train() SELECT pgml . train ( project_name => 'preprocessed_model' , task => 'classification' , relation_name => 'weather_data' , target => 'rain' , preprocess => '{ \"month\": {\"encode\": {\"ordinal\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]}} \"clouds\": {\"encode\": \"target\", scale: \"standard\"} \"humidity\": {\"impute\": \"mean\", scale: \"standard\"} \"temp\": {\"scale\": \"standard\"} }' ); In some cases, it may make sense to use multiple steps for a single column. For example, the clouds column will be target encoded, and then scaled to the standard range to avoid dominating other variables, but there are some interactions between preprocessors to keep in mind. NULL and NaN are treated as additional, independent categories if seen during training, so columns that encode will only ever impute novel when novel data is encountered during training values. It usually makes sense to scale all variables to the same scale. It does not usually help to scale or preprocess the target data, as that is essentially the problem formulation and/or task selection. Note TEXT is used in this document to also refer to VARCHAR and CHAR(N) types.","title":"weather_data"},{"location":"user_guides/training/preprocessing/#predicting-with-preprocessors","text":"A model that has been trained with preprocessors should use a Postgres tuple for prediction, rather than a FLOAT4[] . Tuples may contain multiple different types (like TEXT and BIGINT ), while an ARRAY may only contain a single type. You can use parenthesis around values to create a Postgres tuple. pgml.predict() SELECT pgml . predict ( 'preprocessed_model' , ( 'jan' , 'nimbus' , 0.5 , 7 ));","title":"Predicting with Preprocessors"},{"location":"user_guides/training/preprocessing/#categorical-encodings","text":"Encoding categorical variables is an O(N log(M)) where N is the number of rows, and M is the number of distinct categories. name description none Default - Casts the variable to a 32-bit floating point representation compatible with numerics. This is the default for non- TEXT values. target Encodes the variable as the average value of the target label for all members of the category. This is the default for TEXT variables. one_hot Encodes the variable as multiple independent boolean columns. ordinal Encodes the variable as integer values provided by their position in the input array. NULLS are always 0.","title":"Categorical encodings"},{"location":"user_guides/training/preprocessing/#target-encoding","text":"Target encoding is a relatively efficient way to represent a categorical variable. The average value of the target is computed for each category in the training data set. It is reasonable to scale target encoded variables using the same method as other variables. preprocess => '{ \"clouds\": {\"encode\": \"target\" } } Note Target encoding is currently limited to the first label column specified in a joint optimization model when there are multiple labels.","title":"target encoding"},{"location":"user_guides/training/preprocessing/#one_hot-encoding","text":"One-hot encoding converts each category into an independent boolean column, where all columns are false except the one column the instance is a member of. This is generally not as efficient or as effective as target encoding because the number of additional columns for a single feature can swamp the other features, regardless of scaling in some algorithms. In addition, the columns are highly correlated which can also cause quality issues in some algorithms. PostgresML drops one column by default to break the correlation but preserves the information, which is also referred to as dummy encoding. preprocess => '{ \"clouds\": {\"encode\": \"one_hot\" } } Note All one-hot encoded data is scaled from 0-1 by definition, and will not be further scaled, unlike the other encodings which are scaled.","title":"one_hot encoding"},{"location":"user_guides/training/preprocessing/#ordinal-encoding","text":"Some categorical variables have a natural ordering, like months of the year, or days of the week that can be effectively treated as a discrete quantitative variable. You may set the order of your categorical values, by passing an exhaustive ordered array. e.g. preprocess => '{ \"month\": {\"encode\": {\"ordinal\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]}} }","title":"ordinal encoding"},{"location":"user_guides/training/preprocessing/#imputing-missing-values","text":"NULL and NAN values can be replaced by several statistical measures observed in the training data. name description error Default - will abort training or inference when a NULL or NAN is encountered mean the mean value of the variable in the training data set median the middle value of the variable in the sorted training data set mode the most common value of the variable in the training data set min the minimum value of the variable in the training data set max the maximum value of the variable in the training data set zero replaces all missing values with 0.0 Example preprocess => '{ \"temp\": {\"impute\": \"mean\"} }","title":"Imputing missing values"},{"location":"user_guides/training/preprocessing/#scaling-values","text":"Scaling all variables to a standardized range can help make sure that no feature dominates the model, strictly because it has a naturally larger scale. name description preserve Default - Does not scale the variable at all. standard Scales data to have a mean of zero, and variance of one. min_max Scales data from zero to one. The minimum becomes 0.0 and maximum becomes 1.0. max_abs Scales data from -1.0 to +1.0. Data will not be centered around 0, unless abs(min) == abs(max). robust Scales data as a factor of the first and third quartiles. This method may handle outliers more robustly than others. Example preprocess => '{ \"temp\": {\"scale\": \"standard\"} }","title":"Scaling values"},{"location":"user_guides/transformers/fine_tuning/","text":"Fine Tuning \u00b6 Pre-trained models allow you to get up and running quickly, but you can likely improve performance on your dataset by fine tuning them. Normally, you'll bring your own data to the party, but for these examples we'll use datasets published on Hugging Face. Make sure you've installed the required data dependencies detailed in setup . Translation Example \u00b6 The Helsinki-NLP organization provides more than a thousand pre-trained models to translate between different language pairs. These can be further fine tuned on additional datasets with domain specific vocabulary. Researchers have also created large collections of documents that have been manually translated across languages by experts for training data. Prepare the data \u00b6 The kde4 dataset contains many language pairs. Subsets can be loaded into your Postgres instance with a call to pgml.load_dataset , or you may wish to create your own fine tuning dataset with vocabulary specific to your domain. load_data.sql 1 SELECT pgml . load_dataset ( 'kde4' , kwargs => '{\"lang1\": \"en\", \"lang2\": \"es\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . kde4 LIMIT 5 ; 1 2 3 4 5 6 7 8 9 10 id | translation -----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 99 | { \"en\" : \"If you wish to manipulate the DOM tree in any way you will have to use an external script to do so.\" , \"es\" : \"Si desea manipular el \u00e1rbol DOM deber\u00e1 utilizar un script externo para hacerlo.\" } 100 | { \"en\" : \"Credits\" , \"es\" : \"Cr\u00e9ditos\" } 101 | { \"en\" : \"The domtreeviewer plugin is Copyright & copy; 2001 The Kafka Team/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch\" , \"es\" : \"Derechos de autor de la extensi\u00f3n domtreeviewer & copy;. 2001. El equipo de Kafka/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch.\" } 102 | { \"en\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" , \"es\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" } 103 | { \"en\" : \"ROLES_OF_TRANSLATORS\" , \"es\" : \"Rafael Osuna rosuna@wol. es Traductor\" } ( 5 rows ) This huggingface dataset stores the data as language key pairs in a JSON document. To use it with PostgresML, we'll need to provide a VIEW that structures the data into more primitively typed columns. SQL Result 1 2 3 4 CREATE OR REPLACE VIEW kde4_en_to_es AS SELECT translation ->> 'en' AS \"en\" , translation ->> 'es' AS \"es\" FROM pgml . kde4 LIMIT 10 ; 1 CREATE VIEW Now, we can see the data in more normalized form. The exact column names don't matter for now, we'll specify which one is the target during the training call, and the other one will be used as the input. SQL Result 1 SELECT * FROM kde4_en_to_es LIMIT 10 ; ```sql linenums=\"1\" en | es --------------------------------------------------------------------------------------------+-------------------------------------------------------------------------- \u00b6 Lauri Watts | Lauri Watts & Lauri. Watts. mail; | & Lauri. Watts. mail; ROLES_OF_TRANSLATORS | Rafael Osuna rosuna@wol. es Traductor Miguel Revilla Rodr\u00edguez yo@miguelr evilla. com Traductor 2006-02-26 3.5.1 | 2006-02-26 3.5.1 The Babel & konqueror; plugin gives you quick access to the Babelfish translation service. | La extensi\u00f3n Babel de & konqueror; le permite un acceso r\u00e1pido al servici o de traducci\u00f3n de Babelfish. KDE | KDE kdeaddons | kdeaddons konqueror | konqueror plugins | extensiones babelfish | babelfish (10 rows) ``` Tune the model \u00b6 Tuning is very similar to training with PostgresML, although we specify a model_name to download from Hugging Face instead of the base algorithm . tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . tune ( 'Translate English to Spanish' , task => 'translation' , relation_name => 'kde4_en_to_es' , y_column_name => 'es' , -- translate into spanish model_name => 'Helsinki-NLP/opus-mt-en-es' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_length\": 128 }' , test_size => 0 . 5 , test_sampling => 'last' ); Generate Translations \u00b6 Note Translations use the pgml.generate API since they return TEXT rather than numeric values. You may also call pgml.generate with a TEXT[] for batch processing. SQL Result 1 2 SELECT pgml . generate ( 'Translate English to Spanish' , 'I love SQL' ) AS spanish ; 1 2 3 4 5 6 spanish ---------------- Me encanta SQL ( 1 row ) Time : 126 . 837 ms See the task documentation for more examples, use cases, models and datasets. Text Classification Example \u00b6 DistilBERT is a small, fast, cheap and light Transformer model based on the BERT architecture. It can be fine tuned on specific datasets to learn further nuance between positive and negative examples. For this example, we'll fine tune distilbert-base-uncased on the IMBD dataset, which is a list of movie reviews along with a positive or negative label. Without tuning, DistilBERT classifies every single movie review as positive , and has a F 1 score of 0.367, which is about what you'd expect for a relatively useless classifier. However, after training for a single epoch (takes about 10 minutes on an Nvidia 1080 TI), the F 1 jumps to 0.928 which is a huge improvement, indicating DistilBERT can now fairly accurately predict sentiment from IMDB reviews. Further training for another epoch only results in a very minor improvement to 0.931, and the 3 rd epoch is flat, also at 0.931 which indicates DistilBERT is unlikely to continue learning more about this particular dataset with additional training. You can view the results of each model, like those trained from scratch, in the dashboard. Once our model has been fine tuned on the dataset, it'll be saved and deployed with a Project visible in the Dashboard, just like models built from simpler algorithms. Prepare the data \u00b6 The IMDB dataset has 50,000 examples of user reviews with positive or negative viewing experiences as the labels, and is split 50/50 into training and evaluation datasets. load_dataset.sql 1 SELECT pgml . load_dataset ( 'imdb' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . imdb LIMIT 1 ; 1 2 3 4 text | label -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------- This has to be the funniest stand up comedy I have ever seen . Eddie Izzard is a genius , he picks in Brits , Americans and everyone in between . His style is completely natural and completely hilarious . I doubt that anyone could sit through this and not laugh their a ** off . Watch , enjoy , it ' s funny . | 1 ( 1 row ) Tune the model \u00b6 Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT pgml . tune ( 'IMDB Review Sentiment' , task => 'text-classification' , relation_name => 'pgml.imdb' , y_column_name => 'label' , model_name => 'distilbert-base-uncased' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01 }' , test_size => 0 . 5 , test_sampling => 'last' ); Make predictions \u00b6 SQL Result 1 2 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ----------- 1 ( 1 row ) Time : 16 . 681 ms The default for predict in a classification problem classifies the statement as one of the labels. In this case, 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API: SQL Result 1 2 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ------------------------------------------- [ 0 . 06266672909259796 , 0 . 9373332858085632 ] ( 1 row ) Time : 18 . 101 ms This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (positive sentiment). See the task documentation for more examples, use cases, models and datasets. Summarization Example \u00b6 At a high level, summarization uses similar techniques to translation. Both use an input sequence to generate an output sequence. The difference being that summarization extracts the most relevant parts of the input sequence to generate the output. Prepare the data \u00b6 BillSum is a dataset with training examples that summarize US Congressional and California state bills. You can pass kwargs specific to loading datasets, in this case we'll restrict the dataset to California samples: load_dataset.sql 1 SELECT pgml . load_dataset ( 'billsum' , kwargs => '{\"split\": \"ca_test\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . billsum LIMIT 1 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 text | summary | title -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------- The people of the State of California do enact as follows : +| Existing property tax law establishes a veterans \u2019 organization exemption under which property is exempt from taxation if , among other things , that property is used exclusively for charitable purposes and is owned by a veterans \u2019 organization . +| An act to amend Section 215 . 1 of the Revenue and Taxation Code , relating to taxation , to take effect immediately , tax levy . +| This bill would provide that the veterans \u2019 organization exemption shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes , and would make specific findings and declarations in that regard . The bill would also provide that the exemption shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . +| +| Section 2229 of the Revenue and Taxation Code requires the Legislature to reimburse local agencies annually for certain property tax revenues lost as a result of any exemption or classification of property for purposes of ad valorem property taxation . +| SECTION 1 . +| This bill would provide that , notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made and the state shall not reimburse local agencies for property tax revenues lost by them pursuant to the bill . +| The Legislature finds and declares all of the following : +| This bill would take effect immediately as a tax levy . | ( a ) ( 1 ) Since 1899 congressionally chartered veterans \u2019 organizations have provided a valuable service to our nation \u2019 s returning service members . These organizations help preserve the memories and incidents of the great hostilities fought by our nation , and preserve and strengthen comradeship among members . +| | ( 2 ) These veterans \u2019 organizations also own and manage various properties including lodges , posts , and fraternal halls . These properties act as a safe haven where veterans of all ages and their families can gather together to find camaraderie and fellowship , share stories , and seek support from people who understand their unique experiences . This aids in the healing process for these returning veterans , and ensures their health and happiness . +| | ( b ) As a result of congressional chartering of these veterans \u2019 organizations , the United States Internal Revenue Service created a special tax exemption for these organizations under Section 501 ( c )( 19 ) of the Internal Revenue Code . +| | ( c ) Section 501 ( c )( 19 ) of the Internal Revenue Code and related federal regulations provide for the exemption for posts or organizations of war veterans , or an auxiliary unit or society of , or a trust or foundation for , any such post or organization that , among other attributes , carries on programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , conducts programs for religious , charitable , scientific , literary , or educational purposes , sponsors or participates in activities of a patriotic nature , and provides social and recreational activities for their members . +| | ( d ) Section 215 . 1 of the Revenue and Taxation Code stipulates that all buildings , support and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which ensures to the benefit of any private individual or member thereof , are exempt from taxation . +| | ( e ) The Chief Counsel of the State Board of Equalization concluded , based on a 1979 appellate court decision , that only parts of American Legion halls are exempt from property taxation and that other parts , such as billiard rooms , card rooms , and similar areas , are not exempt . +| | ( f ) In a 1994 memorandum , the State Board of Equalization \u2019 s legal division further concluded that the areas normally considered eligible for exemptions are the office areas used to counsel veterans and the area used to store veterans \u2019 records , but that the meeting hall and bar found in most of the facilities are not considered used for charitable purposes . +| | ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| +| ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| | SECTION 1 . +| | SEC . 2 . +| | Section 215 . 1 of the Revenue and Taxation Code is amended to read : +| | 215 . 1 . +| | ( a ) All buildings , and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , and exempt from federal income tax as an organization described in Section 501 ( c )( 19 ) of the Internal Revenue Code when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which inures to the benefit of any private individual or member thereof , shall be exempt from taxation . +| | ( b ) The exemption provided for in this section shall apply to the property of all organizations meeting the requirements of this section , subdivision ( b ) of Section 4 of Article XIII of the California Constitution , and paragraphs ( 1 ) to ( 4 ), inclusive , ( 6 ), and ( 7 ) of subdivision ( a ) of Section 214 . +| | ( c ) ( 1 ) The exemption specified by subdivision ( a ) shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes . +| | ( 2 ) With regard to this subdivision , the Legislature finds and declares all of the following : +| | ( A ) The exempt activities of a veterans \u2019 organization as described in subdivision ( a ) qualitatively differ from the exempt activities of other nonprofit entities that use property for fraternal , lodge , or social club purposes in that the exempt purpose of the veterans \u2019 organization is to conduct programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , to conduct programs for religious , charitable , scientific , literary , or educational purposes , to sponsor or participate in activities of a patriotic nature , and to provide social and recreational activities for their members . +| | ( B ) In light of this distinction , the use of real property by a veterans \u2019 organization as described in subdivision ( a ), for fraternal , lodge , or social club purposes is central to that organization \u2019 s exempt purposes and activities . +| | ( C ) In light of the factors set forth in subparagraphs ( A ) and ( B ), the use of real property by a veterans \u2019 organization as described in subdivision ( a ) for fraternal , lodge , or social club purposes , constitutes the exclusive use of that property for a charitable purpose within the meaning of subdivision ( b ) of Section 4 of Article XIII of the California Constitution . +| | ( d ) The exemption provided for in this section shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . The portion of the property ineligible for the veterans \u2019 organization exemption shall be that area used primarily to prepare and serve alcoholic beverages . +| | ( e ) An organization that files a claim for the exemption provided for in this section shall file with the assessor a valid organizational clearance certificate issued pursuant to Section 254 . 6 . +| | ( f ) This exemption shall be known as the \u201c veterans \u2019 organization exemption . \u201d +| | SEC . 2 . +| | SEC . 3 . +| | Notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made by this act and the state shall not reimburse any local agency for any property tax revenues lost by it pursuant to this act . +| | SEC . 3 . +| | SEC . 4 . +| | This act provides for a tax levy within the meaning of Article IV of the Constitution and shall go into immediate effect . | | ( 1 row ) This dataset has 3 fields, but summarization transformers only take a single input to produce their output. We can create a view that simply omits the title from the training data: omit_title.sql 1 2 CREATE OR REPLACE VIEW billsum_training_data AS SELECT \"text\" , summary FROM pgml . billsum ; Or, it might be interesting to concat the title to the text field to see how relevant it actually is to the bill. If the title of a bill is the first sentence, and doesn't appear in summary, it may indicate that it's a poorly chosen title for the bill: concat_title.sql 1 2 3 CREATE OR REPLACE VIEW billsum_training_data AS SELECT title || '\\n' || \"text\" AS \"text\" , summary FROM pgml . billsum LIMIT 10 ; Tune the model \u00b6 Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . tune ( 'Legal Summarization' , task => 'summarization' , relation_name => 'billsum_training_data' , y_column_name => 'summary' , model_name => 'sshleifer/distilbart-xsum-12-1' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 2, \"per_device_eval_batch_size\": 2, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_length\": 1024 }' , test_size => 0 . 2 , test_sampling => 'last' ); Make predictions \u00b6 SQL Result 1 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment 1 (1 row) Time: 16.681 ms ``` The default for predict in a classification problem classifies the statement as one of the labels. In this case 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API SQL Result 1 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment [0.06266672909259796, 0.9373332858085632] (1 row) Time: 18.101 ms ``` This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (positive sentiment). See the task documentation for more examples, use cases, models and datasets. Text Generation \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . load_dataset ( 'bookcorpus' , \"limit\" => 100 ); SELECT pgml . tune ( 'GPT Generator' , task => 'text-generation' , relation_name => 'pgml.bookcorpus' , y_column_name => 'text' , model_name => 'gpt2' , hyperparams => '{ \"learning_rate\": 2e-5, \"num_train_epochs\": 1 }' , test_size => 0.2 , test_sampling => 'last' ); SELECT pgml . generate ( 'GPT Generator' , 'While I wandered weak and weary' );","title":"Fine Tuning"},{"location":"user_guides/transformers/fine_tuning/#fine-tuning","text":"Pre-trained models allow you to get up and running quickly, but you can likely improve performance on your dataset by fine tuning them. Normally, you'll bring your own data to the party, but for these examples we'll use datasets published on Hugging Face. Make sure you've installed the required data dependencies detailed in setup .","title":"Fine Tuning"},{"location":"user_guides/transformers/fine_tuning/#translation-example","text":"The Helsinki-NLP organization provides more than a thousand pre-trained models to translate between different language pairs. These can be further fine tuned on additional datasets with domain specific vocabulary. Researchers have also created large collections of documents that have been manually translated across languages by experts for training data.","title":"Translation Example"},{"location":"user_guides/transformers/fine_tuning/#prepare-the-data","text":"The kde4 dataset contains many language pairs. Subsets can be loaded into your Postgres instance with a call to pgml.load_dataset , or you may wish to create your own fine tuning dataset with vocabulary specific to your domain. load_data.sql 1 SELECT pgml . load_dataset ( 'kde4' , kwargs => '{\"lang1\": \"en\", \"lang2\": \"es\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . kde4 LIMIT 5 ; 1 2 3 4 5 6 7 8 9 10 id | translation -----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 99 | { \"en\" : \"If you wish to manipulate the DOM tree in any way you will have to use an external script to do so.\" , \"es\" : \"Si desea manipular el \u00e1rbol DOM deber\u00e1 utilizar un script externo para hacerlo.\" } 100 | { \"en\" : \"Credits\" , \"es\" : \"Cr\u00e9ditos\" } 101 | { \"en\" : \"The domtreeviewer plugin is Copyright & copy; 2001 The Kafka Team/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch\" , \"es\" : \"Derechos de autor de la extensi\u00f3n domtreeviewer & copy;. 2001. El equipo de Kafka/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch.\" } 102 | { \"en\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" , \"es\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" } 103 | { \"en\" : \"ROLES_OF_TRANSLATORS\" , \"es\" : \"Rafael Osuna rosuna@wol. es Traductor\" } ( 5 rows ) This huggingface dataset stores the data as language key pairs in a JSON document. To use it with PostgresML, we'll need to provide a VIEW that structures the data into more primitively typed columns. SQL Result 1 2 3 4 CREATE OR REPLACE VIEW kde4_en_to_es AS SELECT translation ->> 'en' AS \"en\" , translation ->> 'es' AS \"es\" FROM pgml . kde4 LIMIT 10 ; 1 CREATE VIEW Now, we can see the data in more normalized form. The exact column names don't matter for now, we'll specify which one is the target during the training call, and the other one will be used as the input. SQL Result 1 SELECT * FROM kde4_en_to_es LIMIT 10 ; ```sql linenums=\"1\" en | es","title":"Prepare the data"},{"location":"user_guides/transformers/fine_tuning/#-","text":"Lauri Watts | Lauri Watts & Lauri. Watts. mail; | & Lauri. Watts. mail; ROLES_OF_TRANSLATORS | Rafael Osuna rosuna@wol. es Traductor Miguel Revilla Rodr\u00edguez yo@miguelr evilla. com Traductor 2006-02-26 3.5.1 | 2006-02-26 3.5.1 The Babel & konqueror; plugin gives you quick access to the Babelfish translation service. | La extensi\u00f3n Babel de & konqueror; le permite un acceso r\u00e1pido al servici o de traducci\u00f3n de Babelfish. KDE | KDE kdeaddons | kdeaddons konqueror | konqueror plugins | extensiones babelfish | babelfish (10 rows) ```","title":"--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------"},{"location":"user_guides/transformers/fine_tuning/#tune-the-model","text":"Tuning is very similar to training with PostgresML, although we specify a model_name to download from Hugging Face instead of the base algorithm . tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . tune ( 'Translate English to Spanish' , task => 'translation' , relation_name => 'kde4_en_to_es' , y_column_name => 'es' , -- translate into spanish model_name => 'Helsinki-NLP/opus-mt-en-es' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_length\": 128 }' , test_size => 0 . 5 , test_sampling => 'last' );","title":"Tune the model"},{"location":"user_guides/transformers/fine_tuning/#generate-translations","text":"Note Translations use the pgml.generate API since they return TEXT rather than numeric values. You may also call pgml.generate with a TEXT[] for batch processing. SQL Result 1 2 SELECT pgml . generate ( 'Translate English to Spanish' , 'I love SQL' ) AS spanish ; 1 2 3 4 5 6 spanish ---------------- Me encanta SQL ( 1 row ) Time : 126 . 837 ms See the task documentation for more examples, use cases, models and datasets.","title":"Generate Translations"},{"location":"user_guides/transformers/fine_tuning/#text-classification-example","text":"DistilBERT is a small, fast, cheap and light Transformer model based on the BERT architecture. It can be fine tuned on specific datasets to learn further nuance between positive and negative examples. For this example, we'll fine tune distilbert-base-uncased on the IMBD dataset, which is a list of movie reviews along with a positive or negative label. Without tuning, DistilBERT classifies every single movie review as positive , and has a F 1 score of 0.367, which is about what you'd expect for a relatively useless classifier. However, after training for a single epoch (takes about 10 minutes on an Nvidia 1080 TI), the F 1 jumps to 0.928 which is a huge improvement, indicating DistilBERT can now fairly accurately predict sentiment from IMDB reviews. Further training for another epoch only results in a very minor improvement to 0.931, and the 3 rd epoch is flat, also at 0.931 which indicates DistilBERT is unlikely to continue learning more about this particular dataset with additional training. You can view the results of each model, like those trained from scratch, in the dashboard. Once our model has been fine tuned on the dataset, it'll be saved and deployed with a Project visible in the Dashboard, just like models built from simpler algorithms.","title":"Text Classification Example"},{"location":"user_guides/transformers/fine_tuning/#prepare-the-data_1","text":"The IMDB dataset has 50,000 examples of user reviews with positive or negative viewing experiences as the labels, and is split 50/50 into training and evaluation datasets. load_dataset.sql 1 SELECT pgml . load_dataset ( 'imdb' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . imdb LIMIT 1 ; 1 2 3 4 text | label -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------- This has to be the funniest stand up comedy I have ever seen . Eddie Izzard is a genius , he picks in Brits , Americans and everyone in between . His style is completely natural and completely hilarious . I doubt that anyone could sit through this and not laugh their a ** off . Watch , enjoy , it ' s funny . | 1 ( 1 row )","title":"Prepare the data"},{"location":"user_guides/transformers/fine_tuning/#tune-the-model_1","text":"Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT pgml . tune ( 'IMDB Review Sentiment' , task => 'text-classification' , relation_name => 'pgml.imdb' , y_column_name => 'label' , model_name => 'distilbert-base-uncased' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01 }' , test_size => 0 . 5 , test_sampling => 'last' );","title":"Tune the model"},{"location":"user_guides/transformers/fine_tuning/#make-predictions","text":"SQL Result 1 2 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ----------- 1 ( 1 row ) Time : 16 . 681 ms The default for predict in a classification problem classifies the statement as one of the labels. In this case, 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API: SQL Result 1 2 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ------------------------------------------- [ 0 . 06266672909259796 , 0 . 9373332858085632 ] ( 1 row ) Time : 18 . 101 ms This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (positive sentiment). See the task documentation for more examples, use cases, models and datasets.","title":"Make predictions"},{"location":"user_guides/transformers/fine_tuning/#summarization-example","text":"At a high level, summarization uses similar techniques to translation. Both use an input sequence to generate an output sequence. The difference being that summarization extracts the most relevant parts of the input sequence to generate the output.","title":"Summarization Example"},{"location":"user_guides/transformers/fine_tuning/#prepare-the-data_2","text":"BillSum is a dataset with training examples that summarize US Congressional and California state bills. You can pass kwargs specific to loading datasets, in this case we'll restrict the dataset to California samples: load_dataset.sql 1 SELECT pgml . load_dataset ( 'billsum' , kwargs => '{\"split\": \"ca_test\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . billsum LIMIT 1 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 text | summary | title -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------- The people of the State of California do enact as follows : +| Existing property tax law establishes a veterans \u2019 organization exemption under which property is exempt from taxation if , among other things , that property is used exclusively for charitable purposes and is owned by a veterans \u2019 organization . +| An act to amend Section 215 . 1 of the Revenue and Taxation Code , relating to taxation , to take effect immediately , tax levy . +| This bill would provide that the veterans \u2019 organization exemption shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes , and would make specific findings and declarations in that regard . The bill would also provide that the exemption shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . +| +| Section 2229 of the Revenue and Taxation Code requires the Legislature to reimburse local agencies annually for certain property tax revenues lost as a result of any exemption or classification of property for purposes of ad valorem property taxation . +| SECTION 1 . +| This bill would provide that , notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made and the state shall not reimburse local agencies for property tax revenues lost by them pursuant to the bill . +| The Legislature finds and declares all of the following : +| This bill would take effect immediately as a tax levy . | ( a ) ( 1 ) Since 1899 congressionally chartered veterans \u2019 organizations have provided a valuable service to our nation \u2019 s returning service members . These organizations help preserve the memories and incidents of the great hostilities fought by our nation , and preserve and strengthen comradeship among members . +| | ( 2 ) These veterans \u2019 organizations also own and manage various properties including lodges , posts , and fraternal halls . These properties act as a safe haven where veterans of all ages and their families can gather together to find camaraderie and fellowship , share stories , and seek support from people who understand their unique experiences . This aids in the healing process for these returning veterans , and ensures their health and happiness . +| | ( b ) As a result of congressional chartering of these veterans \u2019 organizations , the United States Internal Revenue Service created a special tax exemption for these organizations under Section 501 ( c )( 19 ) of the Internal Revenue Code . +| | ( c ) Section 501 ( c )( 19 ) of the Internal Revenue Code and related federal regulations provide for the exemption for posts or organizations of war veterans , or an auxiliary unit or society of , or a trust or foundation for , any such post or organization that , among other attributes , carries on programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , conducts programs for religious , charitable , scientific , literary , or educational purposes , sponsors or participates in activities of a patriotic nature , and provides social and recreational activities for their members . +| | ( d ) Section 215 . 1 of the Revenue and Taxation Code stipulates that all buildings , support and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which ensures to the benefit of any private individual or member thereof , are exempt from taxation . +| | ( e ) The Chief Counsel of the State Board of Equalization concluded , based on a 1979 appellate court decision , that only parts of American Legion halls are exempt from property taxation and that other parts , such as billiard rooms , card rooms , and similar areas , are not exempt . +| | ( f ) In a 1994 memorandum , the State Board of Equalization \u2019 s legal division further concluded that the areas normally considered eligible for exemptions are the office areas used to counsel veterans and the area used to store veterans \u2019 records , but that the meeting hall and bar found in most of the facilities are not considered used for charitable purposes . +| | ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| +| ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| | SECTION 1 . +| | SEC . 2 . +| | Section 215 . 1 of the Revenue and Taxation Code is amended to read : +| | 215 . 1 . +| | ( a ) All buildings , and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , and exempt from federal income tax as an organization described in Section 501 ( c )( 19 ) of the Internal Revenue Code when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which inures to the benefit of any private individual or member thereof , shall be exempt from taxation . +| | ( b ) The exemption provided for in this section shall apply to the property of all organizations meeting the requirements of this section , subdivision ( b ) of Section 4 of Article XIII of the California Constitution , and paragraphs ( 1 ) to ( 4 ), inclusive , ( 6 ), and ( 7 ) of subdivision ( a ) of Section 214 . +| | ( c ) ( 1 ) The exemption specified by subdivision ( a ) shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes . +| | ( 2 ) With regard to this subdivision , the Legislature finds and declares all of the following : +| | ( A ) The exempt activities of a veterans \u2019 organization as described in subdivision ( a ) qualitatively differ from the exempt activities of other nonprofit entities that use property for fraternal , lodge , or social club purposes in that the exempt purpose of the veterans \u2019 organization is to conduct programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , to conduct programs for religious , charitable , scientific , literary , or educational purposes , to sponsor or participate in activities of a patriotic nature , and to provide social and recreational activities for their members . +| | ( B ) In light of this distinction , the use of real property by a veterans \u2019 organization as described in subdivision ( a ), for fraternal , lodge , or social club purposes is central to that organization \u2019 s exempt purposes and activities . +| | ( C ) In light of the factors set forth in subparagraphs ( A ) and ( B ), the use of real property by a veterans \u2019 organization as described in subdivision ( a ) for fraternal , lodge , or social club purposes , constitutes the exclusive use of that property for a charitable purpose within the meaning of subdivision ( b ) of Section 4 of Article XIII of the California Constitution . +| | ( d ) The exemption provided for in this section shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . The portion of the property ineligible for the veterans \u2019 organization exemption shall be that area used primarily to prepare and serve alcoholic beverages . +| | ( e ) An organization that files a claim for the exemption provided for in this section shall file with the assessor a valid organizational clearance certificate issued pursuant to Section 254 . 6 . +| | ( f ) This exemption shall be known as the \u201c veterans \u2019 organization exemption . \u201d +| | SEC . 2 . +| | SEC . 3 . +| | Notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made by this act and the state shall not reimburse any local agency for any property tax revenues lost by it pursuant to this act . +| | SEC . 3 . +| | SEC . 4 . +| | This act provides for a tax levy within the meaning of Article IV of the Constitution and shall go into immediate effect . | | ( 1 row ) This dataset has 3 fields, but summarization transformers only take a single input to produce their output. We can create a view that simply omits the title from the training data: omit_title.sql 1 2 CREATE OR REPLACE VIEW billsum_training_data AS SELECT \"text\" , summary FROM pgml . billsum ; Or, it might be interesting to concat the title to the text field to see how relevant it actually is to the bill. If the title of a bill is the first sentence, and doesn't appear in summary, it may indicate that it's a poorly chosen title for the bill: concat_title.sql 1 2 3 CREATE OR REPLACE VIEW billsum_training_data AS SELECT title || '\\n' || \"text\" AS \"text\" , summary FROM pgml . billsum LIMIT 10 ;","title":"Prepare the data"},{"location":"user_guides/transformers/fine_tuning/#tune-the-model_2","text":"Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . tune ( 'Legal Summarization' , task => 'summarization' , relation_name => 'billsum_training_data' , y_column_name => 'summary' , model_name => 'sshleifer/distilbart-xsum-12-1' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 2, \"per_device_eval_batch_size\": 2, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_length\": 1024 }' , test_size => 0 . 2 , test_sampling => 'last' );","title":"Tune the model"},{"location":"user_guides/transformers/fine_tuning/#make-predictions_1","text":"SQL Result 1 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment 1 (1 row) Time: 16.681 ms ``` The default for predict in a classification problem classifies the statement as one of the labels. In this case 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API SQL Result 1 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment [0.06266672909259796, 0.9373332858085632] (1 row) Time: 18.101 ms ``` This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (positive sentiment). See the task documentation for more examples, use cases, models and datasets.","title":"Make predictions"},{"location":"user_guides/transformers/fine_tuning/#text-generation","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . load_dataset ( 'bookcorpus' , \"limit\" => 100 ); SELECT pgml . tune ( 'GPT Generator' , task => 'text-generation' , relation_name => 'pgml.bookcorpus' , y_column_name => 'text' , model_name => 'gpt2' , hyperparams => '{ \"learning_rate\": 2e-5, \"num_train_epochs\": 1 }' , test_size => 0.2 , test_sampling => 'last' ); SELECT pgml . generate ( 'GPT Generator' , 'While I wandered weak and weary' );","title":"Text Generation"},{"location":"user_guides/transformers/pre_trained_models/","text":"Pre-Trained Models \u00b6 PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task . We'll demonstrate some of the tasks that are immediately available to users of your database upon installation: translation , sentiment analysis , summarization , question answering and text generation . Examples \u00b6 All of the tasks and models demonstrated here can be customized by passing additional arguments to the Pipeline initializer or call. You'll find additional links to documentation in the examples below. The Hugging Face Pipeline API is exposed in Postgres via: transformer.sql 1 2 3 4 5 6 pgml . transform ( task TEXT OR JSONB , -- task name or full pipeline initializer arguments call JSONB , -- additional call arguments alongside the inputs inputs TEXT [] OR BYTEA [], -- inputs for inference cache BOOLEAN -- if TRUE, the model will be cached in memory. FALSE by default. ) This is roughly equivalent to the following Python: transformer.py 1 2 3 4 import transformers def transform ( task , call , inputs ): return transformers . pipeline ( ** task )( inputs , ** call ) Most pipelines operate on TEXT[] inputs, but some require binary BYTEA[] data like audio classifiers. inputs can be SELECT ed from tables in the database, or they may be passed in directly with the query. The output of this call is a JSONB structure that is task specific. See the Postgres JSON reference for ways to process this output dynamically. Tip Models will be downloaded and stored locally on disk after the first call. They are also cached per connection to improve repeated calls in a single session. To free that memory, you'll need to close your connection. You may want to establish dedicated credentials and connection pools via pgcat or pgbouncer for larger models that have billions of parameters. You may also pass {\"cache\": false} in the JSON call args to prevent this behavior. Translation \u00b6 There are thousands of different pre-trained translation models between language pairs. They generally take a single input string in the \"from\" language, and translate it into the \"to\" language as a result of the call. PostgresML transformations provide a batch interface where you can pass an array of TEXT to process in a single call for efficiency. Not all language pairs have a default task name like this example of English to French. In those cases, you'll need to specify the desired model by name. You can see how to specify a model in the next example . Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. For a translation from English to French with the default pre-trained model: SQL Result 1 2 3 4 5 6 7 SELECT pgml . transform ( 'translation_en_to_fr' , inputs => ARRAY [ 'Welcome to the future!' , 'Where have you been all this time?' ] ) AS french ; 1 2 3 4 5 6 french ------------------------------------------------------------ [ { \"translation_text\" : \"Bienvenue \u00e0 l'avenir!\" } , { \"translation_text\" : \"O\u00f9 \u00eates-vous all\u00e9 tout ce temps?\" } ] See translation documentation for more options. Sentiment Analysis \u00b6 Sentiment analysis is one use of text-classification , but there are many others . This model returns both a label classification [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"] , as well as the score where 0.0 is perfectly negative, and 1.0 is perfectly positive. This example demonstrates specifying the model to be used rather than the task. The roberta-large-mnli model specifies the task of sentiment-analysis in it's default configuration, so we may omit it from the parameters. Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. SQL Result 1 2 3 4 5 6 7 8 SELECT pgml . transform ( '{\"model\": \"roberta-large-mnli\"}' :: JSONB , inputs => ARRAY [ 'I love how amazingly simple ML has become!' , 'I hate doing mundane and thankless tasks. \u2639\ufe0f' ], cache => TRUE ) AS positivity ; 1 2 3 4 5 6 positivity ------------------------------------------------------ [ { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 8143417835235596 } , { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 7637073993682861 } ] See text classification documentation for more options and potential use cases beyond sentiment analysis. You'll notice the outputs are not great in this example. RoBERTa is a breakthrough model, that demonstrated just how important each particular hyperparameter is for the task and particular dataset regardless of how large your model is. We'll show how to fine tune models on your data in the next step. Summarization \u00b6 Sometimes we need all the nuanced detail, but sometimes it's nice to get to the point. Summarization can reduce a very long and complex document to a few sentences. One studied application is reducing legal bills passed by Congress into a plain english summary. Hollywood may also need some intelligence to reduce a full synopsis down to a pithy blurb for movies like Inception. SQL Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 SELECT pgml . transform ( 'summarization' , inputs => ARRAY [ ' Dominic Cobb is the foremost practitioner of the artistic science of extraction, inserting oneself into a subject''s dreams to obtain hidden information without the subject knowing, a concept taught to him by his professor father-in-law, Dr. Stephen Miles. Dom''s associates are Miles'' former students, who Dom requires as he has given up being the dream architect for reasons he won''t disclose. Dom''s primary associate, Arthur, believes it has something to do with Dom''s deceased wife, Mal, who often figures prominently and violently in those dreams, or Dom''s want to \"go home\" (get back to his own reality, which includes two young children). Dom''s work is generally in corporate espionage. As the subjects don''t want the information to get into the wrong hands, the clients have zero tolerance for failure. Dom is also a wanted man, as many of his past subjects have learned what Dom has done to them. One of those subjects, Mr. Saito, offers Dom a job he can''t refuse: to take the concept one step further into inception, namely planting thoughts into the subject''s dreams without them knowing. Inception can fundamentally alter that person as a being. Saito''s target is Robert Michael Fischer, the heir to an energy business empire, which has the potential to rule the world if continued on the current trajectory. Beyond the complex logistics of the dream architecture of the case and some unknowns concerning Fischer, the biggest obstacles in success for the team become worrying about one aspect of inception which Cobb fails to disclose to the other team members prior to the job, and Cobb''s newest associate Ariadne''s belief that Cobb''s own subconscious, especially as it relates to Mal, may be taking over what happens in the dreams. ' ] ) AS result ; 1 2 3 4 5 6 7 result -------------------------------------------------------------------------- [ { \"summary_text\" : \"Dominic Cobb is the foremost practitioner of the artistic science of extraction . his associates are former students, who Dom requires as he has given up being the dream architect . he is also a wanted man, as many of his past subjects have learned what Dom has done to them .\" } ] See summarization documentation for more options. Question Answering \u00b6 Question Answering extracts an answer from a given context. Recent progress has enabled models to also specify if the answer is present in the context at all. If you were trying to build a general question answering system, you could first turn the question into a keyword search against Wikipedia articles, and then use a model to retrieve the correct answer from the top hit. Another application would provide automated support from a knowledge base, based on the customers question. SQL Result 1 2 3 4 5 6 7 8 9 SELECT pgml . transform ( 'question-answering' , inputs => ARRAY [ '{ \"question\": \"Am I dreaming?\", \"context\": \"I got a good nights sleep last night and started a simple tutorial over my cup of morning coffee. The capabilities seem unreal, compared to what I came to expect from the simple SQL standard I studied so long ago. The answer is staring me in the face, and I feel the uncanny call from beyond the screen to check the results.\" }' ] ) AS answer ; 1 2 3 4 5 6 7 8 answer ----------------------------------------------------- { \"end\" : 36 , \"score\" : 0 . 20027603209018707 , \"start\" : 0 , \"answer\" : \"I got a good nights sleep last night\" } See question answering documentation for more options. Text Generation \u00b6 If you need to expand on some thoughts, you can have AI complete your sentences for you: SQL Result 1 2 3 4 5 SELECT pgml . transform ( 'text-generation' , '{\"num_return_sequences\": 2}' , ARRAY [ 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone' ] ) AS result ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 result ----------------------------------------------------------------------------- [[ { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and five for the Elves.\\nWhen, from all that's happening, he sees these things, he says to himself,\" } , { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Eight for the Erogean-kings in their halls of stone -- \\\" and so forth ; \\ \" and \\\" of these \" } ]] More \u00b6 There are many different tasks and tens of thousands of state-of-the-art models available for you to explore. The possibilities are expanding every day. There can be amazing performance improvements in domain specific versions of these general tasks by fine tuning published models on your dataset. See the next section for fine tuning demonstrations.","title":"Pre-Trained Models"},{"location":"user_guides/transformers/pre_trained_models/#pre-trained-models","text":"PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task . We'll demonstrate some of the tasks that are immediately available to users of your database upon installation: translation , sentiment analysis , summarization , question answering and text generation .","title":"Pre-Trained Models"},{"location":"user_guides/transformers/pre_trained_models/#examples","text":"All of the tasks and models demonstrated here can be customized by passing additional arguments to the Pipeline initializer or call. You'll find additional links to documentation in the examples below. The Hugging Face Pipeline API is exposed in Postgres via: transformer.sql 1 2 3 4 5 6 pgml . transform ( task TEXT OR JSONB , -- task name or full pipeline initializer arguments call JSONB , -- additional call arguments alongside the inputs inputs TEXT [] OR BYTEA [], -- inputs for inference cache BOOLEAN -- if TRUE, the model will be cached in memory. FALSE by default. ) This is roughly equivalent to the following Python: transformer.py 1 2 3 4 import transformers def transform ( task , call , inputs ): return transformers . pipeline ( ** task )( inputs , ** call ) Most pipelines operate on TEXT[] inputs, but some require binary BYTEA[] data like audio classifiers. inputs can be SELECT ed from tables in the database, or they may be passed in directly with the query. The output of this call is a JSONB structure that is task specific. See the Postgres JSON reference for ways to process this output dynamically. Tip Models will be downloaded and stored locally on disk after the first call. They are also cached per connection to improve repeated calls in a single session. To free that memory, you'll need to close your connection. You may want to establish dedicated credentials and connection pools via pgcat or pgbouncer for larger models that have billions of parameters. You may also pass {\"cache\": false} in the JSON call args to prevent this behavior.","title":"Examples"},{"location":"user_guides/transformers/pre_trained_models/#translation","text":"There are thousands of different pre-trained translation models between language pairs. They generally take a single input string in the \"from\" language, and translate it into the \"to\" language as a result of the call. PostgresML transformations provide a batch interface where you can pass an array of TEXT to process in a single call for efficiency. Not all language pairs have a default task name like this example of English to French. In those cases, you'll need to specify the desired model by name. You can see how to specify a model in the next example . Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. For a translation from English to French with the default pre-trained model: SQL Result 1 2 3 4 5 6 7 SELECT pgml . transform ( 'translation_en_to_fr' , inputs => ARRAY [ 'Welcome to the future!' , 'Where have you been all this time?' ] ) AS french ; 1 2 3 4 5 6 french ------------------------------------------------------------ [ { \"translation_text\" : \"Bienvenue \u00e0 l'avenir!\" } , { \"translation_text\" : \"O\u00f9 \u00eates-vous all\u00e9 tout ce temps?\" } ] See translation documentation for more options.","title":"Translation"},{"location":"user_guides/transformers/pre_trained_models/#sentiment-analysis","text":"Sentiment analysis is one use of text-classification , but there are many others . This model returns both a label classification [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"] , as well as the score where 0.0 is perfectly negative, and 1.0 is perfectly positive. This example demonstrates specifying the model to be used rather than the task. The roberta-large-mnli model specifies the task of sentiment-analysis in it's default configuration, so we may omit it from the parameters. Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. SQL Result 1 2 3 4 5 6 7 8 SELECT pgml . transform ( '{\"model\": \"roberta-large-mnli\"}' :: JSONB , inputs => ARRAY [ 'I love how amazingly simple ML has become!' , 'I hate doing mundane and thankless tasks. \u2639\ufe0f' ], cache => TRUE ) AS positivity ; 1 2 3 4 5 6 positivity ------------------------------------------------------ [ { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 8143417835235596 } , { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 7637073993682861 } ] See text classification documentation for more options and potential use cases beyond sentiment analysis. You'll notice the outputs are not great in this example. RoBERTa is a breakthrough model, that demonstrated just how important each particular hyperparameter is for the task and particular dataset regardless of how large your model is. We'll show how to fine tune models on your data in the next step.","title":"Sentiment Analysis"},{"location":"user_guides/transformers/pre_trained_models/#summarization","text":"Sometimes we need all the nuanced detail, but sometimes it's nice to get to the point. Summarization can reduce a very long and complex document to a few sentences. One studied application is reducing legal bills passed by Congress into a plain english summary. Hollywood may also need some intelligence to reduce a full synopsis down to a pithy blurb for movies like Inception. SQL Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 SELECT pgml . transform ( 'summarization' , inputs => ARRAY [ ' Dominic Cobb is the foremost practitioner of the artistic science of extraction, inserting oneself into a subject''s dreams to obtain hidden information without the subject knowing, a concept taught to him by his professor father-in-law, Dr. Stephen Miles. Dom''s associates are Miles'' former students, who Dom requires as he has given up being the dream architect for reasons he won''t disclose. Dom''s primary associate, Arthur, believes it has something to do with Dom''s deceased wife, Mal, who often figures prominently and violently in those dreams, or Dom''s want to \"go home\" (get back to his own reality, which includes two young children). Dom''s work is generally in corporate espionage. As the subjects don''t want the information to get into the wrong hands, the clients have zero tolerance for failure. Dom is also a wanted man, as many of his past subjects have learned what Dom has done to them. One of those subjects, Mr. Saito, offers Dom a job he can''t refuse: to take the concept one step further into inception, namely planting thoughts into the subject''s dreams without them knowing. Inception can fundamentally alter that person as a being. Saito''s target is Robert Michael Fischer, the heir to an energy business empire, which has the potential to rule the world if continued on the current trajectory. Beyond the complex logistics of the dream architecture of the case and some unknowns concerning Fischer, the biggest obstacles in success for the team become worrying about one aspect of inception which Cobb fails to disclose to the other team members prior to the job, and Cobb''s newest associate Ariadne''s belief that Cobb''s own subconscious, especially as it relates to Mal, may be taking over what happens in the dreams. ' ] ) AS result ; 1 2 3 4 5 6 7 result -------------------------------------------------------------------------- [ { \"summary_text\" : \"Dominic Cobb is the foremost practitioner of the artistic science of extraction . his associates are former students, who Dom requires as he has given up being the dream architect . he is also a wanted man, as many of his past subjects have learned what Dom has done to them .\" } ] See summarization documentation for more options.","title":"Summarization"},{"location":"user_guides/transformers/pre_trained_models/#question-answering","text":"Question Answering extracts an answer from a given context. Recent progress has enabled models to also specify if the answer is present in the context at all. If you were trying to build a general question answering system, you could first turn the question into a keyword search against Wikipedia articles, and then use a model to retrieve the correct answer from the top hit. Another application would provide automated support from a knowledge base, based on the customers question. SQL Result 1 2 3 4 5 6 7 8 9 SELECT pgml . transform ( 'question-answering' , inputs => ARRAY [ '{ \"question\": \"Am I dreaming?\", \"context\": \"I got a good nights sleep last night and started a simple tutorial over my cup of morning coffee. The capabilities seem unreal, compared to what I came to expect from the simple SQL standard I studied so long ago. The answer is staring me in the face, and I feel the uncanny call from beyond the screen to check the results.\" }' ] ) AS answer ; 1 2 3 4 5 6 7 8 answer ----------------------------------------------------- { \"end\" : 36 , \"score\" : 0 . 20027603209018707 , \"start\" : 0 , \"answer\" : \"I got a good nights sleep last night\" } See question answering documentation for more options.","title":"Question Answering"},{"location":"user_guides/transformers/pre_trained_models/#text-generation","text":"If you need to expand on some thoughts, you can have AI complete your sentences for you: SQL Result 1 2 3 4 5 SELECT pgml . transform ( 'text-generation' , '{\"num_return_sequences\": 2}' , ARRAY [ 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone' ] ) AS result ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 result ----------------------------------------------------------------------------- [[ { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and five for the Elves.\\nWhen, from all that's happening, he sees these things, he says to himself,\" } , { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Eight for the Erogean-kings in their halls of stone -- \\\" and so forth ; \\ \" and \\\" of these \" } ]]","title":"Text Generation"},{"location":"user_guides/transformers/pre_trained_models/#more","text":"There are many different tasks and tens of thousands of state-of-the-art models available for you to explore. The possibilities are expanding every day. There can be amazing performance improvements in domain specific versions of these general tasks by fine tuning published models on your dataset. See the next section for fine tuning demonstrations.","title":"More"},{"location":"user_guides/transformers/setup/","text":"label img { position: relative; top: 0.3em; left: -0.1em; height: auto !important; width: 1.2em !important; } \ud83e\udd17 Transformers \u00b6 PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task . Setup \u00b6 Install the machine learning dependencies on the database for the transformers you would like to use: PyTorch Tensorflow Flax See the Pytorch docs for more information. $ sudo pip3 install torch See the Tensorflow docs for more information. $ sudo pip3 install tensorflow See the Flax docs for more information. $ sudo pip3 install flax Models will be downloaded and cached on the database for repeated usage. View the Transformers installation docs for cache management details and offline deployments. You may also want to install GPU support when working with larger models. Standard Datasets \u00b6 Many datasets have been published to stimulate research and benchmark architectures, but also to help demonstrate API usage in the tutorials. The Datasets package provides a way to load published datasets into Postgres: $ sudo pip3 install datasets Audio Processing \u00b6 Torch Audio is required for many models that process audio data. You can install the additional dependencies with: $ sudo pip3 install torchaudio","title":"Setup"},{"location":"user_guides/transformers/setup/#transformers","text":"PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task .","title":"\ud83e\udd17 Transformers"},{"location":"user_guides/transformers/setup/#setup","text":"Install the machine learning dependencies on the database for the transformers you would like to use: PyTorch Tensorflow Flax See the Pytorch docs for more information. $ sudo pip3 install torch See the Tensorflow docs for more information. $ sudo pip3 install tensorflow See the Flax docs for more information. $ sudo pip3 install flax Models will be downloaded and cached on the database for repeated usage. View the Transformers installation docs for cache management details and offline deployments. You may also want to install GPU support when working with larger models.","title":"Setup"},{"location":"user_guides/transformers/setup/#standard-datasets","text":"Many datasets have been published to stimulate research and benchmark architectures, but also to help demonstrate API usage in the tutorials. The Datasets package provides a way to load published datasets into Postgres: $ sudo pip3 install datasets","title":"Standard Datasets"},{"location":"user_guides/transformers/setup/#audio-processing","text":"Torch Audio is required for many models that process audio data. You can install the additional dependencies with: $ sudo pip3 install torchaudio","title":"Audio Processing"},{"location":"user_guides/vector_operations/overview/","text":"Vector Operations \u00b6 PostgresML adds optimized vector operations that can be used inside SQL queries. Vector operations are particularly useful for dealing with embeddings that have been generated from other machine learning algorithms, and can provide functions like nearest neighbor calculations using various distance functions. Embeddings can be a relatively efficient mechanism to leverage the power of deep learning, without the runtime inference costs. These functions are fast with the most expensive distance functions computing upwards of ~100k per second for a memory resident dataset on modern hardware. The PostgreSQL planner will also automatically parallelize evaluation on larger datasets, if configured to take advantage of multiple CPU cores when available. Vector operations are implemented in Rust using ndarray and BLAS, for maximum performance. Element-wise Arithmetic with Constants \u00b6 Addition \u00b6 pgml . add ( a REAL [], b REAL ) -> REAL [] SQL Output SELECT pgml . add ( ARRAY [ 1.0 , 2.0 , 3.0 ], 3 ); pgml=# SELECT pgml.add(ARRAY[1.0, 2.0, 3.0], 3); add --------- {4,5,6} (1 row) Subtraction \u00b6 pgml . subtract ( minuend REAL [], subtrahend REAL ) -> REAL [] Multiplication \u00b6 pgml . multiply ( multiplicand REAL [], multiplier REAL ) -> REAL [] Division \u00b6 pgml . divide ( dividend REAL [], divisor REAL ) -> REAL [] Pairwise arithmetic with Vectors \u00b6 Addition \u00b6 pgml . add ( a REAL [], b REAL []) -> REAL [] Subtraction \u00b6 pgml . subtract ( minuend REAL [], subtrahend REAL []) -> REAL [] Multiplication \u00b6 pgml . multiply ( multiplicand REAL [], multiplier REAL []) -> REAL [] Division \u00b6 pgml . divide ( dividend REAL [], divisor REAL []) -> REAL [] Norms \u00b6 Dimensions not at origin \u00b6 pgml . norm_l0 ( vector REAL []) -> REAL Manhattan distance from origin \u00b6 pgml . norm_l1 ( vector REAL []) -> REAL Euclidean distance from origin \u00b6 pgml . norm_l2 ( vector REAL []) -> REAL Absolute value of largest element \u00b6 pgml . norm_max ( vector REAL []) -> REAL Normalization \u00b6 Unit Vector \u00b6 pgml . normalize_l1 ( vector REAL []) -> REAL [] Squared Unit Vector \u00b6 pgml . normalize_l2 ( vector REAL []) -> REAL [] -1:1 values \u00b6 pgml . normalize_max ( vector REAL []) -> REAL [] Distances \u00b6 Manhattan \u00b6 pgml . distance_l1 ( a REAL [], b REAL []) -> REAL Euclidean \u00b6 pgml . distance_l2 ( a REAL [], b REAL []) -> REAL Projection \u00b6 pgml . dot_product ( a REAL [], b REAL []) -> REAL Direction \u00b6 pgml . cosine_similarity ( a REAL [], b REAL []) -> REAL Nearest Neighbor Example \u00b6 If we had precalculated the embeddings for a set of user and product data, we could find the 100 best products for a user with a similarity search. SELECT products . id , pgml . cosine_similarity ( users . embedding , products . embedding ) AS distance FROM users JOIN products WHERE users . id = 123 ORDER BY distance ASC LIMIT 100 ;","title":"Vector Operations"},{"location":"user_guides/vector_operations/overview/#vector-operations","text":"PostgresML adds optimized vector operations that can be used inside SQL queries. Vector operations are particularly useful for dealing with embeddings that have been generated from other machine learning algorithms, and can provide functions like nearest neighbor calculations using various distance functions. Embeddings can be a relatively efficient mechanism to leverage the power of deep learning, without the runtime inference costs. These functions are fast with the most expensive distance functions computing upwards of ~100k per second for a memory resident dataset on modern hardware. The PostgreSQL planner will also automatically parallelize evaluation on larger datasets, if configured to take advantage of multiple CPU cores when available. Vector operations are implemented in Rust using ndarray and BLAS, for maximum performance.","title":"Vector Operations"},{"location":"user_guides/vector_operations/overview/#element-wise-arithmetic-with-constants","text":"","title":"Element-wise Arithmetic with Constants"},{"location":"user_guides/vector_operations/overview/#addition","text":"pgml . add ( a REAL [], b REAL ) -> REAL [] SQL Output SELECT pgml . add ( ARRAY [ 1.0 , 2.0 , 3.0 ], 3 ); pgml=# SELECT pgml.add(ARRAY[1.0, 2.0, 3.0], 3); add --------- {4,5,6} (1 row)","title":"Addition"},{"location":"user_guides/vector_operations/overview/#subtraction","text":"pgml . subtract ( minuend REAL [], subtrahend REAL ) -> REAL []","title":"Subtraction"},{"location":"user_guides/vector_operations/overview/#multiplication","text":"pgml . multiply ( multiplicand REAL [], multiplier REAL ) -> REAL []","title":"Multiplication"},{"location":"user_guides/vector_operations/overview/#division","text":"pgml . divide ( dividend REAL [], divisor REAL ) -> REAL []","title":"Division"},{"location":"user_guides/vector_operations/overview/#pairwise-arithmetic-with-vectors","text":"","title":"Pairwise arithmetic with Vectors"},{"location":"user_guides/vector_operations/overview/#addition_1","text":"pgml . add ( a REAL [], b REAL []) -> REAL []","title":"Addition"},{"location":"user_guides/vector_operations/overview/#subtraction_1","text":"pgml . subtract ( minuend REAL [], subtrahend REAL []) -> REAL []","title":"Subtraction"},{"location":"user_guides/vector_operations/overview/#multiplication_1","text":"pgml . multiply ( multiplicand REAL [], multiplier REAL []) -> REAL []","title":"Multiplication"},{"location":"user_guides/vector_operations/overview/#division_1","text":"pgml . divide ( dividend REAL [], divisor REAL []) -> REAL []","title":"Division"},{"location":"user_guides/vector_operations/overview/#norms","text":"","title":"Norms"},{"location":"user_guides/vector_operations/overview/#dimensions-not-at-origin","text":"pgml . norm_l0 ( vector REAL []) -> REAL","title":"Dimensions not at origin"},{"location":"user_guides/vector_operations/overview/#manhattan-distance-from-origin","text":"pgml . norm_l1 ( vector REAL []) -> REAL","title":"Manhattan distance from origin"},{"location":"user_guides/vector_operations/overview/#euclidean-distance-from-origin","text":"pgml . norm_l2 ( vector REAL []) -> REAL","title":"Euclidean distance from origin"},{"location":"user_guides/vector_operations/overview/#absolute-value-of-largest-element","text":"pgml . norm_max ( vector REAL []) -> REAL","title":"Absolute value of largest element"},{"location":"user_guides/vector_operations/overview/#normalization","text":"","title":"Normalization"},{"location":"user_guides/vector_operations/overview/#unit-vector","text":"pgml . normalize_l1 ( vector REAL []) -> REAL []","title":"Unit Vector"},{"location":"user_guides/vector_operations/overview/#squared-unit-vector","text":"pgml . normalize_l2 ( vector REAL []) -> REAL []","title":"Squared Unit Vector"},{"location":"user_guides/vector_operations/overview/#-11-values","text":"pgml . normalize_max ( vector REAL []) -> REAL []","title":"-1:1 values"},{"location":"user_guides/vector_operations/overview/#distances","text":"","title":"Distances"},{"location":"user_guides/vector_operations/overview/#manhattan","text":"pgml . distance_l1 ( a REAL [], b REAL []) -> REAL","title":"Manhattan"},{"location":"user_guides/vector_operations/overview/#euclidean","text":"pgml . distance_l2 ( a REAL [], b REAL []) -> REAL","title":"Euclidean"},{"location":"user_guides/vector_operations/overview/#projection","text":"pgml . dot_product ( a REAL [], b REAL []) -> REAL","title":"Projection"},{"location":"user_guides/vector_operations/overview/#direction","text":"pgml . cosine_similarity ( a REAL [], b REAL []) -> REAL","title":"Direction"},{"location":"user_guides/vector_operations/overview/#nearest-neighbor-example","text":"If we had precalculated the embeddings for a set of user and product data, we could find the 100 best products for a user with a similarity search. SELECT products . id , pgml . cosine_similarity ( users . embedding , products . embedding ) AS distance FROM users JOIN products WHERE users . id = 123 ORDER BY distance ASC LIMIT 100 ;","title":"Nearest Neighbor Example"}]}