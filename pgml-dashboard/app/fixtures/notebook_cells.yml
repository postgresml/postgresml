- model: app.notebookcell
  pk: 1
  fields:
    notebook: 0
    cell_type: 1
    contents: '## Welcome!


      You''re set up and running on PostgresML! This is an end-to-end system for training
      and deploying real time machine learning models. It handles data versioning,
      model training and validation, and safe production release. This dashboard web
      app will give you an overview of what''s happening in the system and also helps
      build and deploy projects. You can use notebooks like this one to interact with
      your database in real time and organize your SQL while documenting your code.



      ### Notebooks


      These notebooks are similar to Jupyter Notebooks, which you might be familiar
      with already. On the bottom of the page, you will find a text editor which is
      used to create new cells. Each cell can contain either Markdown which is just
      text really, and SQL which will be executed directly by your Postgres database
      server.


      Each cell has a little menu in the top right corner, allowing you to (re)run
      it (if it''s SQL), edit it, and delete it.



      Let me give you an example. The next cell (cell #2) will be a SQL cell which
      will execute a simple query. Go ahead and click the next "Play" button now.'
    rendering: '<article class="markdown-body"><h2>Welcome!</h2>

      <p>You''re set up and running on PostgresML! This is an end-to-end system for
      training and deploying real time machine learning models. It handles data versioning,
      model training and validation, and safe production release. This dashboard web
      app will give you an overview of what''s happening in the system and also helps
      build and deploy projects. You can use notebooks like this one to interact with
      your database in real time and organize your SQL while documenting your code.</p>

      <h3>Notebooks</h3>

      <p>These notebooks are similar to Jupyter Notebooks, which you might be familiar
      with already. On the bottom of the page, you will find a text editor which is
      used to create new cells. Each cell can contain either Markdown which is just
      text really, and SQL which will be executed directly by your Postgres database
      server.</p>

      <p>Each cell has a little menu in the top right corner, allowing you to (re)run
      it (if it''s SQL), edit it, and delete it.</p>

      <p>Let me give you an example. The next cell (cell #2) will be a SQL cell which
      will execute a simple query. Go ahead and click the next "Play" button now.</p></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 2
  fields:
    notebook: 0
    cell_type: 3
    contents: SELECT random();
    rendering: null
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 3
  fields:
    notebook: 0
    cell_type: 1
    contents: "We just asked Postgres to return a random number. Pretty simple query,
      but it demonstrates the notebook functionality pretty well. You can see that
      the result of `random()` is a float between 0 and 1. On the bottom right corner,
      you can see that it took `0:00:00.000654` or 0 hours, 0 minutes, 0 seconds and
      only 654ns, or 0.6ms. This run time is good to keep an eye on. It will help
      build an intuition for how fast Postgres really is, and how certain operations
      scale as the data grows. You'll be able to see how long \n\nTry rerunning the
      cell again by clicking the \"play\" button in the top right corner. You'll see
      that the random number will change. Rerunning is a real time operation and Postgres
      will give you a different random number every time (otherwise it wouldn't be
      random).\n\n#### Editing a cell\n\nYou can edit a cell at any time, including
      SQL cells which will then run the new query immediately.\n\n#### Deleting a
      cell\n\nDeleting a cell is pretty easy: just click on the delete button in the
      top right corner. You'll have 10 seconds to undo the delete if you so desire;
      we wouldn't want you to lose your work because of an accidental click.\n\n####
      Shortcuts\n\nThe text editor supports the following helpful shortcuts:\n\n\n|
      Shortcut |             Description               |\n-----------| --------------------------------------\n|
      `Cmd-/` or `Ctrl-/` | Comment out SQL code.      |\n| `Cmd-Enter` or `Ctrl-Enter`
      | Save/create a cell.|\n\nBy the way, this was a Markdown table, you can make
      those here as well."
    rendering: '<article class="markdown-body"><p>We just asked Postgres to return
      a random number. Pretty simple query, but it demonstrates the notebook functionality
      pretty well. You can see that the result of <code>random()</code> is a float
      between 0 and 1. On the bottom right corner, you can see that it took <code>0:00:00.000654</code>
      or 0 hours, 0 minutes, 0 seconds and only 654ns, or 0.6ms. This run time is
      good to keep an eye on. It will help build an intuition for how fast Postgres
      really is, and how certain operations scale as the data grows. You''ll be able
      to see how long </p>

      <p>Try rerunning the cell again by clicking the "play" button in the top right
      corner. You''ll see that the random number will change. Rerunning is a real
      time operation and Postgres will give you a different random number every time
      (otherwise it wouldn''t be random).</p>

      <h4>Editing a cell</h4>

      <p>You can edit a cell at any time, including SQL cells which will then run
      the new query immediately.</p>

      <h4>Deleting a cell</h4>

      <p>Deleting a cell is pretty easy: just click on the delete button in the top
      right corner. You''ll have 10 seconds to undo the delete if you so desire; we
      wouldn''t want you to lose your work because of an accidental click.</p>

      <h4>Shortcuts</h4>

      <p>The text editor supports the following helpful shortcuts:</p>

      <table>

      <thead>

      <tr>

      <th>Shortcut</th>

      <th>Description</th>

      </tr>

      </thead>

      <tbody>

      <tr>

      <td><code>Cmd-/</code> or <code>Ctrl-/</code></td>

      <td>Comment out SQL code.</td>

      </tr>

      <tr>

      <td><code>Cmd-Enter</code> or <code>Ctrl-Enter</code></td>

      <td>Save/create a cell.</td>

      </tr>

      </tbody>

      </table>

      <p>By the way, this was a Markdown table, you can make those here as well.</p></article>'
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: 2022-08-22 21:13:49.583297+00:00
- model: app.notebookcell
  pk: 4
  fields:
    notebook: 0
    cell_type: 1
    contents: "We just asked Postgres to return a random number. Pretty simple query,
      but it demonstrates the notebook functionality pretty well. You can see that
      the result of `random()` is a float between 0 and 1. On the bottom right corner,
      you can see that it took `0:00:00.000654` or 0 hours, 0 minutes, 0 seconds and
      only 654ns, or 0.6ms. This run time is good to keep an eye on. It will help
      build an intuition for how fast Postgres really is, and how certain operations
      scale as the data grows.\n\nTry rerunning the cell again by clicking the \"Play\"
      button in the top right corner. You'll see that the random number will change.
      Rerunning is a real time operation and Postgres will give you a different random
      number every time (otherwise it wouldn't be random).\n\n#### Editing a cell\nYou
      can edit a cell at any time, including SQL cells which will then run the new
      query immediately.\n\n#### Deleting a cell\nDeleting a cell is pretty easy:
      just click on the delete button in the top right corner. You'll have 10 seconds
      to undo the delete if you so desire; we wouldn't want you to lose your work
      because of an accidental click.\n\n#### Shortcuts\nThe text editor supports
      the following helpful shortcuts:\n\n\n|  Shortcut |             Description
      \              \n|-----------| --------------------------------------\n| `Cmd-/`
      or `Ctrl-/` | Comment out SQL code.      |\n| `Cmd-Enter` or `Ctrl-Enter` |
      Save/create a cell.|\n| `Shift-Enter` | Run the currently selected cell. |\n\nBy
      the way, this was a Markdown table, you can make those here as well.\n      \n###
      Thank you\nThank you for trying out PostgresML! We hope you enjoy your time
      here and have fun learning about machine learning, in the comfort of your favorite
      database.\n\nYou may want to check out the rest of [the tutorials](../) or dive
      straight in with a notebook to test [Tutorial 1: ⏱️ Real Time Fraud Detection](../1/)"
    rendering: '<article class="markdown-body"><p>We just asked Postgres to return
      a random number. Pretty simple query, but it demonstrates the notebook functionality
      pretty well. You can see that the result of <code>random()</code> is a float
      between 0 and 1. On the bottom right corner, you can see that it took <code>0:00:00.000654</code>
      or 0 hours, 0 minutes, 0 seconds and only 654ns, or 0.6ms. This run time is
      good to keep an eye on. It will help build an intuition for how fast Postgres
      really is, and how certain operations scale as the data grows.</p>

      <p>Try rerunning the cell again by clicking the "Play" button in the top right
      corner. You''ll see that the random number will change. Rerunning is a real
      time operation and Postgres will give you a different random number every time
      (otherwise it wouldn''t be random).</p>

      <h4>Editing a cell</h4>

      <p>You can edit a cell at any time, including SQL cells which will then run
      the new query immediately.</p>

      <h4>Deleting a cell</h4>

      <p>Deleting a cell is pretty easy: just click on the delete button in the top
      right corner. You''ll have 10 seconds to undo the delete if you so desire; we
      wouldn''t want you to lose your work because of an accidental click.</p>

      <h4>Shortcuts</h4>

      <p>The text editor supports the following helpful shortcuts:</p>

      <table>

      <thead>

      <tr>

      <th>Shortcut</th>

      <th>Description</th>

      </tr>

      </thead>

      <tbody>

      <tr>

      <td><code>Cmd-/</code> or <code>Ctrl-/</code></td>

      <td>Comment out SQL code.</td>

      </tr>

      <tr>

      <td><code>Cmd-Enter</code> or <code>Ctrl-Enter</code></td>

      <td>Save/create a cell.</td>

      </tr>

      <tr>

      <td><code>Shift-Enter</code></td>

      <td>Run the currently selected cell.</td>

      </tr>

      </tbody>

      </table>

      <p>By the way, this was a Markdown table, you can make those here as well.</p>

      <h3>Thank you</h3>

      <p>Thank you for trying out PostgresML! We hope you enjoy your time here and
      have fun learning about machine learning, in the comfort of your favorite database.</p>

      <p>You may want to check out the rest of <a href="../">the tutorials</a> or
      dive straight in with a notebook to test <a href="../1/">Tutorial 1: ⏱️ Real
      Time Fraud Detection</a></p></article>'
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 5
  fields:
    notebook: 0
    cell_type: 3
    contents: SELECT 'Have a nice day!' AS greeting;
    rendering: null
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 6
  fields:
    notebook: 1
    cell_type: 1
    contents: 'Introduction

      ------------


      Most organizations have some risks that may be minimized using machine learning,
      by predicting the likelihood of negative outcomes before they happen. As long
      as you''re able to track the information leading up to the unfortunate events,
      there are many different machine learning algorithms that can tease out the
      correlations across multiple variables.


      One example of risk ecommerce companies face is credit card fraud with stolen
      credit cards. When the owner of the card sees charges they never authorized
      on their monthly statement, they''ll report these to the credit card company,
      and the charges will be reversed. The ecommerce company will lose the merchandise
      as well as shipping charges and labor costs. If a company receives too many
      chargebacks, not only will they incur expensive losses, but the credit card
      processors may remove them from the platform, so it''s important they have some
      certainty about the owner of the cards identity and legitimate interests.


      In this notebook, we''ll demonstrate how a simplified ecommerce application
      might track customer orders, and use machine learning to detect chargeback risks
      in real time during checkout. The most important step in building any Machine
      Learning model is understanding the data. Knowing its structure, application
      use, and the full meaning for the business will allow us to create meaningful
      features and labels for our models. In this notebook, we''ve included a fair
      bit of SQL to implement logic that would normally be written at the application
      layer to help you build an intuition about the domain.


      **Contents**


      - Part 1: Ecommerce Application Data Model

      - Part 2: Structuring the Training Data

      - Part 3: Training a Model

      - Part 4: Adding More Features

      - Part 5: Upgrading the Machine Learning Algorithm


      Part 1: Ecommerce Application Data Model

      --------------------------------

      We''ll build out a simple ecommerce schema, and populate it with some example
      data. First, our store needs some products to sell. Products have a name, their
      price, and other metadata, like whether or not they are perishable goods.'
    rendering: '<article class="markdown-body"><h2>Introduction</h2>

      <p>Most organizations have some risks that may be minimized using machine learning,
      by predicting the likelihood of negative outcomes before they happen. As long
      as you''re able to track the information leading up to the unfortunate events,
      there are many different machine learning algorithms that can tease out the
      correlations across multiple variables.</p>

      <p>One example of risk ecommerce companies face is credit card fraud with stolen
      credit cards. When the owner of the card sees charges they never authorized
      on their monthly statement, they''ll report these to the credit card company,
      and the charges will be reversed. The ecommerce company will lose the merchandise
      as well as shipping charges and labor costs. If a company receives too many
      chargebacks, not only will they incur expensive losses, but the credit card
      processors may remove them from the platform, so it''s important they have some
      certainty about the owner of the cards identity and legitimate interests.</p>

      <p>In this notebook, we''ll demonstrate how a simplified ecommerce application
      might track customer orders, and use machine learning to detect chargeback risks
      in real time during checkout. The most important step in building any Machine
      Learning model is understanding the data. Knowing its structure, application
      use, and the full meaning for the business will allow us to create meaningful
      features and labels for our models. In this notebook, we''ve included a fair
      bit of SQL to implement logic that would normally be written at the application
      layer to help you build an intuition about the domain.</p>

      <p><strong>Contents</strong></p>

      <ul>

      <li>Part 1: Ecommerce Application Data Model</li>

      <li>Part 2: Structuring the Training Data</li>

      <li>Part 3: Training a Model</li>

      <li>Part 4: Adding More Features</li>

      <li>Part 5: Upgrading the Machine Learning Algorithm</li>

      </ul>

      <h2>Part 1: Ecommerce Application Data Model</h2>

      <p>We''ll build out a simple ecommerce schema, and populate it with some example
      data. First, our store needs some products to sell. Products have a name, their
      price, and other metadata, like whether or not they are perishable goods.</p></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 7
  fields:
    notebook: 1
    cell_type: 3
    contents: "CREATE TABLE products (\n  emoji TEXT PRIMARY KEY,\n  name TEXT,\n
      \ price MONEY,\n  perishable BOOLEAN\n);"
    rendering: null
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 8
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO PRODUCTS (emoji, name, price, perishable) \nVALUES\n  ('\U0001F4B0',
      '1oz gold bar', '$1999.99', false),\n  ('\U0001F4D5', 'a tale of 2 cities',
      '$19.99', false),\n  ('\U0001F96C', 'head of lettuce', '$1.99', true)\nRETURNING
      *;"
    rendering: null
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 9
  fields:
    notebook: 1
    cell_type: 1
    contents: "Now that we're in business, our first customer has shown up, named
      Alice. She is a chef that owns a salad shop, so she is going to create an order
      for 1,000 \U0001F96C `head of lettuce`.\n\nOur ecommerce site will record `orders`
      and their `line_items` in our database with the following schema."
    rendering: "<article class=\"markdown-body\"><p>Now that we're in business, our
      first customer has shown up, named Alice. She is a chef that owns a salad shop,
      so she is going to create an order for 1,000 \U0001F96C <code>head of lettuce</code>.</p>\n<p>Our
      ecommerce site will record <code>orders</code> and their <code>line_items</code>
      in our database with the following schema.</p></article>"
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 10
  fields:
    notebook: 1
    cell_type: 3
    contents: "CREATE TABLE orders (\n  id BIGSERIAL PRIMARY KEY,\n  customer_name
      TEXT\n);\n\nCREATE TABLE line_items (\n  id BIGSERIAL PRIMARY KEY,\n  order_id
      BIGINT,\n  product_emoji TEXT,\n  count INTEGER\n);"
    rendering: null
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 11
  fields:
    notebook: 1
    cell_type: 1
    contents: Now that we have created the schema, we can record Alice's order
    rendering: <article class="markdown-body"><p>Now that we have created the schema,
      we can record Alice's order</p></article>
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 12
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO orders (customer_name) VALUES ('Alice');\n\nINSERT INTO
      line_items (order_id, product_emoji, count) \nVALUES (\n  -- a query to find
      Alice's most recent order\n  (SELECT max(id) FROM orders WHERE customer_name
      = 'Alice'),\n  '\U0001F96C',\n  1000\n)\nRETURNING *;"
    rendering: null
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 13
  fields:
    notebook: 1
    cell_type: 1
    contents: "\U0001F50E That inline subquery in #7 is a little weird.\n\n```sql\n--
      a query to find Alice's most recent order\n(SELECT max(id) FROM orders WHERE
      customer_name = 'Alice')\n```\n\nTypically this ID would be passed in from the
      application layer, instead of being retrieved during the INSERT statement itself.
      But anyway... \n\nNext, we'll record her payment in full via credit card in
      our `payments` table."
    rendering: "<article class=\"markdown-body\"><p>\U0001F50E That inline subquery
      in #7 is a little weird.</p>\n<pre><code class=\"language-sql\">-- a query to
      find Alice's most recent order\n(SELECT max(id) FROM orders WHERE customer_name
      = 'Alice')\n</code></pre>\n<p>Typically this ID would be passed in from the
      application layer, instead of being retrieved during the INSERT statement itself.
      But anyway... </p>\n<p>Next, we'll record her payment in full via credit card
      in our <code>payments</code> table.</p></article>"
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 14
  fields:
    notebook: 1
    cell_type: 3
    contents: "CREATE TABLE payments (\n  id BIGSERIAL PRIMARY KEY,\n  order_id BIGINT,\n
      \ amount MONEY\n);"
    rendering: null
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 15
  fields:
    notebook: 1
    cell_type: 1
    contents: We'll be doing a little bit of heavy lifting in the next query to calculate
      her payment total on the fly.
    rendering: <article class="markdown-body"><p>We'll be doing a little bit of heavy
      lifting in the next query to calculate her payment total on the fly.</p></article>
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 16
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO payments (order_id, amount) \n-- a query to compute the
      full amount of Alice's most recent order\nSELECT order_id, SUM(count * price)
      AS amount\nFROM orders\nJOIN line_items ON line_items.order_id = orders.id\nJOIN
      products ON products.emoji = line_items.product_emoji\nWHERE orders.id = (SELECT
      max(id) AS order_id FROM orders WHERE customer_name = 'Alice')\nGROUP BY 1\nRETURNING
      *;"
    rendering: null
    execution_time: null
    cell_number: 11
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 17
  fields:
    notebook: 1
    cell_type: 1
    contents: "\U0001F389 Time to celebrate! Alice has paid in full for our first
      order, and business is good.\n\n\nNow, along comes Bob \"the bad guy\" who places
      an order for a \U0001F4B0 1oz gold bar."
    rendering: "<article class=\"markdown-body\"><p>\U0001F389 Time to celebrate!
      Alice has paid in full for our first order, and business is good.</p>\n<p>Now,
      along comes Bob \"the bad guy\" who places an order for a \U0001F4B0 1oz gold
      bar.</p></article>"
    execution_time: null
    cell_number: 12
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 18
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO orders (customer_name) VALUES ('Bob');\nINSERT INTO line_items
      (order_id, product_emoji, count) VALUES (\n  (SELECT max(id) FROM orders WHERE
      customer_name = 'Bob'),\n  '\U0001F4B0',\n  1\n)\nRETURNING *;"
    rendering: null
    execution_time: null
    cell_number: 13
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 19
  fields:
    notebook: 1
    cell_type: 1
    contents: Unfortunately, Bob makes his payment with a stolen credit card, but
      we don't know that yet.
    rendering: <article class="markdown-body"><p>Unfortunately, Bob makes his payment
      with a stolen credit card, but we don't know that yet.</p></article>
    execution_time: null
    cell_number: 14
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 20
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO payments (order_id, amount) \n-- a query to compute the
      full amount of Bob's most recent order\nSELECT order_id, SUM(count * price)
      AS amount\nFROM orders\nJOIN line_items ON line_items.order_id = orders.id\nJOIN
      products ON products.emoji = line_items.product_emoji\nWHERE orders.id = (SELECT
      max(id) AS order_id FROM orders WHERE customer_name = 'Bob')\nGROUP BY 1\nRETURNING
      *;"
    rendering: null
    execution_time: null
    cell_number: 15
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 21
  fields:
    notebook: 1
    cell_type: 1
    contents: At the end of the month, the credit card company lets us know about
      the chargeback from the real card owner.  We'll need to create another table
      to keep track of this.
    rendering: <article class="markdown-body"><p>At the end of the month, the credit
      card company lets us know about the chargeback from the real card owner.  We'll
      need to create another table to keep track of this.</p></article>
    execution_time: null
    cell_number: 16
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 22
  fields:
    notebook: 1
    cell_type: 3
    contents: "CREATE TABLE chargebacks (\n  id BIGSERIAL PRIMARY KEY,\n  payment_id
      BIGINT\n)"
    rendering: null
    execution_time: null
    cell_number: 17
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 23
  fields:
    notebook: 1
    cell_type: 1
    contents: And now we can record the example of fraud
    rendering: <article class="markdown-body"><p>And now we can record the example
      of fraud</p></article>
    execution_time: null
    cell_number: 18
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 24
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO chargebacks (payment_id) \nSELECT max(payments.id) AS payment_id\nFROM
      payments \nJOIN orders ON payments.order_id = orders.id \nWHERE customer_name
      = 'Bob'\nRETURNING *;"
    rendering: null
    execution_time: null
    cell_number: 19
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 25
  fields:
    notebook: 1
    cell_type: 1
    contents: "\U0001F3C1 Congratulations! \U0001F3C1 \n----------------\nIf you've
      made it this far, you've won half the machine learning battle. We have created
      2 training data examples that are perfect for \"supervised\" machine learning.
      The chargebacks act as the ground truth to inform the machine learning algorithm
      of whether or not an order is fraudulent. These records are what we refer to
      as \"labels\", a.k.a \"targets\" or \"Y-values\" for the data.\n\nPart 2: Structuring
      the Training Data\n--------------------------\nWe can construct a query that
      provides a summary view of our orders, including the fraudulent label."
    rendering: "<article class=\"markdown-body\"><h2>\U0001F3C1 Congratulations! \U0001F3C1</h2>\n<p>If
      you've made it this far, you've won half the machine learning battle. We have
      created 2 training data examples that are perfect for \"supervised\" machine
      learning. The chargebacks act as the ground truth to inform the machine learning
      algorithm of whether or not an order is fraudulent. These records are what we
      refer to as \"labels\", a.k.a \"targets\" or \"Y-values\" for the data.</p>\n<h2>Part
      2: Structuring the Training Data</h2>\n<p>We can construct a query that provides
      a summary view of our orders, including the fraudulent label.</p></article>"
    execution_time: null
    cell_number: 20
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 26
  fields:
    notebook: 1
    cell_type: 3
    contents: "CREATE VIEW orders_summaries AS\nSELECT \n  orders.id AS order_id,
      \n  orders.customer_name,\n  payments.amount AS total, \n  ARRAY_AGG(products.emoji)
      AS product_emojis,\n  CASE WHEN chargebacks.id IS NOT NULL \n    THEN true \n
      \   ELSE false \n  END AS fraudulent\nFROM orders\nLEFT JOIN payments ON payments.order_id
      = orders.id\nLEFT JOIN chargebacks ON chargebacks.payment_id = payments.id\nLEFT
      JOIN line_items ON line_items.order_id = orders.id\nLEFT JOIN products ON products.emoji
      = line_items.product_emoji\nGROUP BY 1, 2, 3, 5\nORDER BY orders.id;"
    rendering: null
    execution_time: null
    cell_number: 21
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 27
  fields:
    notebook: 1
    cell_type: 1
    contents: Now, let's have a look at the summary
    rendering: <article class="markdown-body"><p>Now, let's have a look at the summary</p></article>
    execution_time: null
    cell_number: 22
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 28
  fields:
    notebook: 1
    cell_type: 3
    contents: SELECT * FROM orders_summaries;
    rendering: null
    execution_time: null
    cell_number: 23
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 29
  fields:
    notebook: 1
    cell_type: 1
    contents: It's intuitive that thieves will be more attracted to gold bars than
      a head of lettuce because the resell value is better. Perishable goods are more
      difficult to move on the black market. A good piece of information for our model
      would be the percentage of the order that is perishable. We call this a "feature"
      of the data model. We can construct a query to return this feature for each
      order, along with the chargeback label.
    rendering: <article class="markdown-body"><p>It's intuitive that thieves will
      be more attracted to gold bars, than a head of lettuce because the resell value
      is better. Perishable goods are more difficult to move on the black market.
      A good piece of information for our model would be the percentage of the order
      that is perishable. We call this a "feature" of the data model. We can construct
      a query to return this feature for each order, along with the chargeback label.</p></article>
    execution_time: null
    cell_number: 24
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 30
  fields:
    notebook: 1
    cell_type: 3
    contents: "CREATE VIEW fraud_samples AS\nSELECT \n  SUM(CASE WHEN products.perishable
      THEN (count * price) ELSE '$0.0' END) / SUM(payments.amount) AS perishable_percentage,
      \n  CASE WHEN chargebacks.id IS NOT NULL \n    THEN true \n    ELSE false \n
      \ END AS fraudulent\nFROM orders\nLEFT JOIN payments ON payments.order_id =
      orders.id\nLEFT JOIN chargebacks ON chargebacks.payment_id = payments.id\nLEFT
      JOIN line_items ON line_items.order_id = orders.id\nLEFT JOIN products ON products.emoji
      = line_items.product_emoji\nGROUP BY orders.id, chargebacks.id\nORDER BY orders.id;"
    rendering: null
    execution_time: null
    cell_number: 25
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 31
  fields:
    notebook: 1
    cell_type: 3
    contents: SELECT * FROM fraud_samples;
    rendering: null
    execution_time: null
    cell_number: 26
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 32
  fields:
    notebook: 1
    cell_type: 1
    contents: 'Training a model

      ----------------


      This is a great training set for a machine learning model. We''ve found a feature
      `perishable_percentage` that perfectly correlates with the label `fraudulent`.
      Perishable orders are less likely to result in a chargeback. A good model will
      be able to generalize from the example data we have to new examples that we
      may never have seen before, like an order that is only 33% perishable goods.
      Now that we have a `VIEW` of this data, we can train a "classification" model
      to classify the features as `fraudulent` or not.'
    rendering: '<article class="markdown-body"><h2>Training a model</h2>

      <p>This is a great training set for a machine learning model. We''ve found a
      feature <code>perishable_percentage</code> that perfectly correlates with the
      label <code>fraudulent</code>. Perishable orders are less likely to result in
      a chargeback. A good model will be able to generalize from the example data
      we have to new examples that we may never have seen before, like an order that
      is only 33% perishable goods. Now that we have a <code>VIEW</code> of this data,
      we can train a "classification" model to classify the features as <code>fraudulent</code>
      or not.</p></article>'
    execution_time: null
    cell_number: 27
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 33
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Our Fraud Model', --
      a friendly name we'll use to identify this machine learning project\n  task
      => 'classification', -- we want to classify into true or false\n  relation_name
      => 'fraud_samples', -- our view of the data\n  y_column_name => 'fraudulent',
      -- the \"labels\"\n  test_sampling => 'last', -- the part of the data to use
      for testing our model\n  test_size => 0.5 -- use half the data for tests\n);"
    rendering: null
    execution_time: null
    cell_number: 28
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 34
  fields:
    notebook: 1
    cell_type: 1
    contents: 'Oops. We''re going to get an error:


      ```

      ERROR:  ValueError: This solver needs samples of at least 2 classes in the data,
      but the data contains only one class: False

      ```


      Wait a second, we know there is both a True and a False label, because we have
      an example of both a fraudulent and legit order. What gives? This is a glimpse
      into how PostgresML works inside the black box. It splits the sample data into
      2 sets. One is used for training the model as we expected, and the other is
      used to test the model''s predictions against the remaining known labels. This
      way we can see how well the model generalizes. In this case, since there are
      only 2 data samples, 1 is used for training (the False label) and 1 is used
      for testing (the True label). Now we can understand there isn''t enough data
      to actually train and test. We need to generate a couple more examples so we
      have enough to train and test.'
    rendering: '<article class="markdown-body"><p>Oops. We''re going to get an error:</p>

      <pre><code>ERROR:  ValueError: This solver needs samples of at least 2 classes
      in the data, but the data contains only one class: False

      </code></pre>

      <p>Wait a second, we know there is both a True and a False label, because we
      have an example of both a fraudulent and legit order. What gives? This is a
      glimpse into how PostgresML works inside the black box. It splits the sample
      data into 2 sets. One is used for training the model as we expected, and the
      other is used to test the model''s predictions against the remaining known labels.
      This way we can see how well the model generalizes. In this case, since there
      are only 2 data samples, 1 is used for training (the False label) and 1 is used
      for testing (the True label). Now we can understand there isn''t enough data
      to actually train and test. We need to generate a couple more examples so we
      have enough to train and test.</p></article>'
    execution_time: null
    cell_number: 29
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 35
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO orders (customer_name) VALUES ('Carol');\nINSERT INTO line_items
      (\n  order_id, \n  product_emoji, \n  count\n) VALUES (\n  (SELECT max(id) FROM
      orders WHERE customer_name = 'Carol'),\n  '\U0001F4D5',\n  10\n)\nRETURNING
      *;"
    rendering: null
    execution_time: null
    cell_number: 30
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 36
  fields:
    notebook: 1
    cell_type: 1
    contents: Carol has bought a book, and now will legitimately pay in full.
    rendering: <article class="markdown-body"><p>Carol has bought a book, and now
      will legitimately pay in full.</p></article>
    execution_time: null
    cell_number: 31
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 37
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO payments (order_id, amount) \n-- a query to compute the
      full amount of Bob's most recent order\nSELECT order_id, SUM(count * price)
      AS amount\nFROM orders\nJOIN line_items ON line_items.order_id = orders.id\nJOIN
      products ON products.emoji = line_items.product_emoji\nWHERE orders.id = (SELECT
      max(id) AS order_id FROM orders WHERE customer_name = 'Carol')\nGROUP BY 1\nRETURNING
      *;"
    rendering: null
    execution_time: null
    cell_number: 32
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 38
  fields:
    notebook: 1
    cell_type: 1
    contents: 'And now Dan (another fraudster) shows up to steal more books:'
    rendering: <article class="markdown-body"><p>And now Dan (another fraudster) shows
      up to steal more books:</p></article>
    execution_time: null
    cell_number: 33
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 39
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO orders (customer_name) VALUES ('Dan');\nINSERT INTO line_items
      (\n  order_id, \n  product_emoji, \n  count\n) VALUES (\n  (SELECT max(id) FROM
      orders WHERE customer_name = 'Dan'),\n  '\U0001F4D5',\n  50\n)\nRETURNING *;"
    rendering: null
    execution_time: null
    cell_number: 34
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 40
  fields:
    notebook: 1
    cell_type: 1
    contents: Here comes the fraudulent payment.
    rendering: <article class="markdown-body"><p>Here comes the fraudulent payment.</p></article>
    execution_time: null
    cell_number: 35
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 41
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO payments (order_id, amount) \n-- a query to compute the
      full amount of Bob's most recent order\nSELECT order_id, SUM(count * price)
      AS amount\nFROM orders\nJOIN line_items ON line_items.order_id = orders.id\nJOIN
      products ON products.emoji = line_items.product_emoji\nWHERE orders.id = (SELECT
      max(id) AS order_id FROM orders WHERE customer_name = 'Dan')\nGROUP BY 1\nRETURNING
      *;"
    rendering: null
    execution_time: null
    cell_number: 36
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 42
  fields:
    notebook: 1
    cell_type: 1
    contents: And when the credit card company let's us know about the issue, we'll
      record it.
    rendering: <article class="markdown-body"><p>And when the credit card company
      let's us know about the issue, we'll record it.</p></article>
    execution_time: null
    cell_number: 37
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 43
  fields:
    notebook: 1
    cell_type: 3
    contents: "INSERT INTO chargebacks (payment_id) \nSELECT max(payments.id) AS payment_id\nFROM
      payments \nJOIN orders ON payments.order_id = orders.id \nWHERE customer_name
      = 'Dan'\nRETURNING *;"
    rendering: null
    execution_time: null
    cell_number: 38
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 44
  fields:
    notebook: 1
    cell_type: 1
    contents: And now we can try to train the model again.
    rendering: <article class="markdown-body"><p>And now we can try to train the model
      again.</p></article>
    execution_time: null
    cell_number: 39
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 45
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Our Fraud Classification',
      -- a friendly name we'll use to identify this machine learning project\n  task
      => 'classification', -- we want to classify into true or false\n  relation_name
      => 'fraud_samples', -- our view of the data\n  y_column_name => 'fraudulent',
      -- the \"labels\"\n  test_sampling => 'last',\n  test_size => 0.5 -- use half
      the data for testing rather than the default test size of 25%\n);"
    rendering: null
    execution_time: null
    cell_number: 40
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 46
  fields:
    notebook: 1
    cell_type: 1
    contents: "\U0001F3C1 Success! \U0001F3C1\n--------------\n\nWe can demonstrate
      basic usage of the model with another SQL call"
    rendering: "<article class=\"markdown-body\"><h2>\U0001F3C1 Success! \U0001F3C1</h2>\n<p>We
      can demonstrate basic usage of the model with another SQL call</p></article>"
    execution_time: null
    cell_number: 41
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 47
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT \n  perishable_percentage, \n  fraudulent, \n  pgml.predict('Our
      Fraud Classification', ARRAY[perishable_percentage]) AS predict_fraud \nFROM
      fraud_samples;"
    rendering: null
    execution_time: null
    cell_number: 42
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 48
  fields:
    notebook: 1
    cell_type: 1
    contents: Uh oh, the model was trained on a perfectly small dataset. It learned
      that unless the order is perishable goods, it's going to predict fraud 100%
      of the time, but our test data shows that's not 100% true. Let's generate some
      samples to further explore our model.
    rendering: <article class="markdown-body"><p>Uh oh, the model was trained on a
      perfectly small dataset. It learned that unless the order is perishable goods,
      it's going to predict fraud 100% of the time, but our test data shows that's
      not 100% true. Let's generate some samples to further explore our model.</p></article>
    execution_time: null
    cell_number: 43
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 49
  fields:
    notebook: 1
    cell_type: 3
    contents: "WITH exploration_samples AS (\n  SELECT generate_series(0, 1, 0.1)
      AS perishable_percentage\n)\nSELECT \n  perishable_percentage, \n  pgml.predict('Our
      Fraud Classification', ARRAY[perishable_percentage]) AS predict_fraud \nFROM
      exploration_samples;"
    rendering: null
    execution_time: null
    cell_number: 44
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 50
  fields:
    notebook: 1
    cell_type: 1
    contents: 'The default model is a linear regression, so it has learned from the
      training half of the data that high amounts of perishable goods make for safe
      orders.


      Part 4: Adding more features

      ----------------------------

      We need to add some more features to create a better model. Instead of just
      using the perishable percentage, we can use dollar values as our features, since
      we know criminals want to steal large amounts more than small amounts.'
    rendering: '<article class="markdown-body"><p>The default model is a linear regression,
      so it has learned from the training half of the data that high amounts of perishable
      goods make for safe orders.</p>

      <h2>Part 4: Adding more features</h2>

      <p>We need to add some more features to create a better model. Instead of just
      using the perishable percentage, we can use dollar values as our features, since
      we know criminals want to steal large amounts more than small amounts.</p></article>'
    execution_time: null
    cell_number: 45
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 51
  fields:
    notebook: 1
    cell_type: 3
    contents: "DROP VIEW fraud_samples;\nCREATE VIEW fraud_samples AS\nSELECT \n  SUM(CASE
      WHEN products.perishable THEN (count * price)::NUMERIC::FLOAT ELSE 0.0 END)
      AS perishable_amount, \n  SUM(CASE WHEN NOT products.perishable THEN (count
      * price)::NUMERIC::FLOAT ELSE 0.0 END) AS non_perishable_amount, \n  CASE WHEN
      chargebacks.id IS NOT NULL \n    THEN true \n    ELSE false \n  END AS fraudulent\nFROM
      orders\nLEFT JOIN payments ON payments.order_id = orders.id\nLEFT JOIN chargebacks
      ON chargebacks.payment_id = payments.id\nLEFT JOIN line_items ON line_items.order_id
      = orders.id\nLEFT JOIN products ON products.emoji = line_items.product_emoji\nGROUP
      BY orders.id, chargebacks.id\nORDER BY orders.id;"
    rendering: null
    execution_time: null
    cell_number: 46
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 52
  fields:
    notebook: 1
    cell_type: 1
    contents: And now we retrain a new version of the model, by calling train with
      the same parameters again.
    rendering: <article class="markdown-body"><p>And now we retrain a new version
      of the model, by calling train with the same parameters again.</p></article>
    execution_time: null
    cell_number: 47
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 53
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Our Fraud Classification',
      -- a friendly name we'll use to identify this machine learning project\n  task
      => 'classification', -- we want to classify into true or false\n  relation_name
      => 'fraud_samples', -- our view of the data\n  y_column_name => 'fraudulent',
      -- the \"labels\"\n  test_sampling => 'last',\n  test_size => 0.5 -- use half
      the data for testing rather than the default test size of 25%\n);"
    rendering: null
    execution_time: null
    cell_number: 48
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 54
  fields:
    notebook: 1
    cell_type: 1
    contents: And then we can deploy this most recent version
    rendering: <article class="markdown-body"><p>And then we can deploy this most
      recent version</p></article>
    execution_time: null
    cell_number: 49
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 55
  fields:
    notebook: 1
    cell_type: 3
    contents: SELECT * FROM pgml.deploy('Our Fraud Classification', 'most_recent');
    rendering: null
    execution_time: null
    cell_number: 50
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 56
  fields:
    notebook: 1
    cell_type: 1
    contents: 'And view the input/outputs of this model based on our data:'
    rendering: <article class="markdown-body"><p>And view the input/outputs of this
      model based on our data:</p></article>
    execution_time: null
    cell_number: 51
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 57
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT \n  perishable_amount, \n  non_perishable_amount, \n  fraudulent,
      \n  pgml.predict(\n    'Our Fraud Classification', \n    ARRAY[perishable_amount,
      non_perishable_amount]\n  ) AS predict_fraud \nFROM fraud_samples;"
    rendering: null
    execution_time: null
    cell_number: 52
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 58
  fields:
    notebook: 1
    cell_type: 1
    contents: "This is the basic development cycle for a model. \n  \n1. Add new features.\n2.
      Retrain the new model.\n3. Analyze performance.\n\nEven with a toy schema like
      this, it's possible to create many different features over the data. Examples
      of other statistical features we could add:\n\n- how many orders the customer
      has previously made without chargebacks,\n- what has their total spend been so
      far,\n- how old is this account,\n- what is their average order size,\n- how frequently
      do they typically order,\n- do they typically buy perishable or non perishable
      goods.\n\nWe can create additional VIEWs, subqueries or [Common Table Expressions](https://www.postgresql.org/docs/current/queries-with.html)
      to standardize these features across models or reports.\n\nSubqueries may be
      preferable to Common Table Expressions for generating complex features, because
      CTEs create an optimization gate that prevents the query planner from pushing
      predicates down, which will hurt performance if you intend to reuse this VIEW
      during inference for a single row.\n\nIf you're querying a particular view frequently
      that is expensive to produce, you may consider using a `CREATE MATERIALIZED VIEW`, to
      cache the results.\n\n\n```sql\n-- A subquery\nLEFT JOIN (\n  SELECT DISTINCT
      orders.customer_name, COUNT(*) AS previous_orders FROM ORDERS\n) AS customer_stats\n
      \ ON customer_stats.customer_name = orders.customer_name\n```\n\n```sql\n--
      A view\nCREATE VIEW customer_stats AS \nSELECT DISTINCT orders.customer_name,
      COUNT(*) AS previous_orders FROM ORDERS;\n\n...\nLEFT JOIN customer_stats ON
      customer_stats.customer_name = orders.customer_name\n```\n\n```sql\n-- A Common
      Table Expression\nWITH customer_stats AS ( \n  SELECT DISTINCT orders.customer_name,
      COUNT(*) AS previous_orders FROM ORDERS;\n)\n\n...\nLEFT JOIN customer_stats
      ON customer_stats.customer_name = orders.customer_name\n```"
    rendering: "<article class=\"markdown-body\"><p>This is the basic development
      cycle for a model. </p>\n<ol>\n<li>Add new features.</li>\n<li>Retrain the new
      model.</li>\n<li>Analyze performance.</li>\n</ol>\n<p>Even with a toy schema like
      this, it's possible to create many different features over the data. Examples
      of other statistical features we could add:</p>\n<ul>\n<li>how many orders the
      customer has previously made without chargebacks,</li>\n<li>what has their total
      spend been so far,</li>\n<li>how old is this account,</li>\n<li>what is their
      average order size,</li>\n<li>how frequently do they typically order,</li>\n<li>do
      they typically buy perishable or non perishable goods.</li>\n</ul>\n<p>We can
      create additional VIEWs, subqueries or <a href=\"https://www.postgresql.org/docs/current/queries-with.html\">Common
      Table Expressions</a> to standardize these features across models or reports.</p>\n<p>Subqueries 
      may be preferable to Common Table Expressions for generating complex
      features, because CTEs create an optimization gate that prevents the query planner
      from pushing predicates down, which will hurt performance if you intend to reuse
      this VIEW during inference for a single row.</p>\n<p>If you're querying a particular
      view frequently that is expensive to produce, you may consider using a <code>CREATE
      MATERIALIZED VIEW</code>, to cache the results.</p>\n<pre><code class=\"language-sql\">--
      A subquery\nLEFT JOIN (\n  SELECT DISTINCT orders.customer_name, COUNT(*)
      AS previous_orders FROM ORDERS\n) AS customer_stats\n  ON customer_stats.customer_name
      = orders.customer_name\n</code></pre>\n<pre><code class=\"language-sql\">--
      A view\nCREATE VIEW customer_stats AS \nSELECT DISTINCT orders.customer_name,
      COUNT(*) AS previous_orders FROM ORDERS;\n\n...\nLEFT JOIN customer_stats ON
      customer_stats.customer_name = orders.customer_name\n</code></pre>\n<pre><code
      class=\"language-sql\">-- A Common Table Expression\nWITH customer_stats AS
      ( \n  SELECT DISTINCT orders.customer_name, COUNT(*) AS previous_orders FROM
      ORDERS;\n)\n\n...\nLEFT JOIN customer_stats ON customer_stats.customer_name
      = orders.customer_name\n</code></pre></article>"
    execution_time: null
    cell_number: 53
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 59
  fields:
    notebook: 1
    cell_type: 1
    contents: 'Part 5: Upgrading the Machine Learning Algorithm

      ------------------------------------------


      When you''re out of ideas for features that might help the model distinguish
      orders that are likely to result in chargebacks, you may want to start testing
      different algorithms to see how the performance changes. PostgresML makes algorithm
      selection as easy as passing an additional parameter to `pgml.train`. You may
      want to test them all just to see, but `xgboost` typically gives excellent performance
      in terms of both accuracy and latency.'
    rendering: '<article class="markdown-body"><h2>Part 5: Upgrading the Machine Learning
      Algorithm</h2>

      <p>When you''re out of ideas for features that might help the model distinguish
      orders that are likely to result in chargebacks, you may want to start testing
      different algorithms to see how the performance changes. PostgresML makes algorithm
      selection as easy as passing an additional parameter to <code>pgml.train</code>.
      You may want to test them all just to see, but <code>xgboost</code> typically
      gives excellent performance in terms of both accuracy and latency.</p></article>'
    execution_time: null
    cell_number: 54
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 60
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Our Fraud Classification',
      -- a friendly name we'll use to identify this machine learning project\n  task
      => 'classification', -- we want to classify into true or false\n  relation_name
      => 'fraud_samples', -- our view of the data\n  y_column_name => 'fraudulent',
      -- the \"labels\"\n  algorithm => 'xgboost', -- tree based models like xgboost
      are often the best performers for tabular data at scale\n  test_size => 0.5,
      -- use half the data for testing rather than the default test size of 25%\n
      \ test_sampling => 'last'\n);"
    rendering: null
    execution_time: null
    cell_number: 55
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 61
  fields:
    notebook: 1
    cell_type: 1
    contents: So far we've been training a classifier that gives us a binary 0 or
      1 output to indicate fraud or not. If we'd like to refine our application response
      to the models predictions in a more nuanced way, say high/medium/low risk instead
      of binary, we can use "regression" instead of "classification" to predict a
      likelihood between 0 and 1, instead of binary.
    rendering: <article class="markdown-body"><p>So far we've been training a classifier
      that gives us a binary 0 or 1 output to indicate fraud or not. If we'd like
      to refine our application response to the models predictions in a more nuanced
      way, say high/medium/low risk instead of binary, we can use "regression" instead
      of "classification" to predict a likelihood between 0 and 1, instead of binary.</p></article>
    execution_time: null
    cell_number: 56
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 62
  fields:
    notebook: 1
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Our Fraud Regression',
      -- a friendly name we'll use to identify this machine learning project\n  task
      => 'regression', -- predict the likelihood\n  relation_name => 'fraud_samples',
      -- our view of the data\n  y_column_name => 'fraudulent', -- the \"labels\"\n
      \ algorithm => 'linear', \n  test_size => 0.5, -- use half the data for testing
      rather than the default test size of 25%\n  test_sampling => 'last'\n);"
    rendering: null
    execution_time: null
    cell_number: 57
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 63
  fields:
    notebook: 1
    cell_type: 1
    contents: At this point, the primary limitation of our model is the amount of
      data, the number of examples we have to train it on. Luckily, as time marches
      on, and data accumulates in the database, we can simply retrain this model with
      additional calls to `pgml.train` and watch it adjust as new information becomes
      available.
    rendering: <article class="markdown-body"><p>At this point, the primary limitation
      of our model is the amount of data, the number of examples we have to train
      it on. Luckily, as time marches on, and data accumulates in the database, we
      can simply retrain this model with additional calls to <code>pgml.train</code>
      and watch it adjust as new information becomes available.</p></article>
    execution_time: null
    cell_number: 58
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 64
  fields:
    notebook: 1
    cell_type: 3
    contents: "-- If you'd like to start this tutorial over, you can clear out the
      tables we created.\n-- use Ctrl-/ to comment/uncomment blocks in this editor.\nDROP
      TABLE IF EXISTS products CASCADE; \nDROP TABLE IF EXISTS orders CASCADE; \nDROP
      TABLE IF EXISTS line_items CASCADE; \nDROP TABLE IF EXISTS chargebacks CASCADE;
      \nDROP TABLE IF EXISTS payments CASCADE;"
    rendering: null
    execution_time: null
    cell_number: 59
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 65
  fields:
    notebook: 2
    cell_type: 1
    contents: 'Binary classification means categorizing data into 2 categories. Usually
      these are categories like:


      - `True` or `False`

      - `0` or `1`

      - `hot_dog` or `not_hot_dog`


      These categories divide a population into things we care about, and things we
      can ignore. Binary classification is a common task for machine learning models.
      It can be applied across a broad set of scenarios, once you understand the way
      to structure your problem as a set of example data with labeled outcomes.


      In this tutorial, we''ll train models using various "supervised learning" algorithms
      to classify medical samples as benign or malignant. Supervised learning techniques
      require us to label the sample data for the algorithm to learn how the inputs
      correlate with the labels. After the algorithm has been trained on the labeled
      data set we created, we can present it with new unlabeled data to classify based
      on the most likely outcome.


      As we saw in [Tutorial 1: Real Time Fraud Model](../1/) understanding the structure
      of the data and the labels is a complex and critical step for real world machine
      learning projects. In this example we''ll focus more on the different algorithms,
      and use an academic benchmark dataset that already includes binary labels from
      UCI ML Breast Cancer Wisconsin. Features were computed from a digitized image
      of a fine needle aspirate (FNA) of a breast mass. They describe characteristics
      of the cell nuclei present in the image. The labels are either True for a malignant
      sample of False for a benign sample.


      You can load this dataset into your Postgres database with the following SQL.'
    rendering: '<article class="markdown-body"><p>Binary classification means categorizing
      data into 2 categories. Usually these are categories like:</p>

      <ul>

      <li><code>True</code> or <code>False</code></li>

      <li><code>0</code> or <code>1</code></li>

      <li><code>hot_dog</code> or <code>not_hot_dog</code></li>

      </ul>

      <p>These categories divide a population into things we care about, and things
      we can ignore. Binary classification is a common task for machine learning models.
      It can be applied across a broad set of scenarios, once you understand the way
      to structure your problem as a set of example data with labeled outcomes.</p>

      <p>In this tutorial, we''ll train models using various "supervised learning"
      algorithms to classify medical samples as benign or malignant. Supervised learning
      techniques require us to label the sample data for the algorithm to learn how
      the inputs correlate with the labels. After the algorithm has been trained on
      the labeled data set we created, we can present it with new unlabeled data to
      classify based on the most likely outcome.</p>

      <p>As we saw in <a href="../1/">Tutorial 1: Real Time Fraud Model</a> understanding
      the structure of the data and the labels is a complex and critical step for
      real world machine learning projects. In this example we''ll focus more on the
      different algorithms, and use an academic benchmark dataset that already includes
      binary labels from UCI ML Breast Cancer Wisconsin. Features were computed from
      a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe
      characteristics of the cell nuclei present in the image. The labels are either
      True for a malignant sample of False for a benign sample.</p>

      <p>You can load this dataset into your Postgres database with the following
      SQL.</p></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 66
  fields:
    notebook: 2
    cell_type: 3
    contents: SELECT pgml.load_dataset('breast_cancer');
    rendering: null
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 67
  fields:
    notebook: 2
    cell_type: 1
    contents: This function has created a new table in your database named `pgml.breast_cancer`.
      Let's look at a random sample of the data with some more SQL.
    rendering: <article class="markdown-body"><p>This function has created a new table
      in your database named <code>pgml.breast_cancer</code>. Let's look at a random
      sample of the data with some more SQL.</p></article>
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 68
  fields:
    notebook: 2
    cell_type: 3
    contents: "SELECT * \nFROM pgml.breast_cancer \nORDER BY random()\nLIMIT 10;"
    rendering: null
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 69
  fields:
    notebook: 2
    cell_type: 1
    contents: 'That''s a lot of numeric feature data describing various attributes
      of the cells, but if you scroll all the way to the right above, after running
      the query, you''ll see that each sample set of feature data is labeled `malignant`
      [`True` or `False`]. It would be extremely difficult for a human to study all
      these numbers, and see how they correlate with malignant or not, and then be
      able to make a prediction for new samples, but mathematicians have been working
      on algorithms to do exactly this using computers which happen to be exceptionally
      good at this by now. This is statistical machine learning.


      PostgresML makes it easy to use this data to create a model. It only takes a
      single function call with a few parameters.'
    rendering: '<article class="markdown-body"><p>That''s a lot of numeric feature
      data describing various attributes of the cells, but if you scroll all the way
      to the right above, after running the query, you''ll see that each sample set
      of feature data is labeled <code>malignant</code> [<code>True</code> or <code>False</code>].
      It would be extremely difficult for a human to study all these numbers, and
      see how they correlate with malignant or not, and then be able to make a prediction
      for new samples, but mathematicians have been working on algorithms to do exactly
      this using computers which happen to be exceptionally good at this by now. This
      is statistical machine learning.</p>

      <p>PostgresML makes it easy to use this data to create a model. It only takes
      a single function call with a few parameters.</p></article>'
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 70
  fields:
    notebook: 2
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Breast Cancer Detection',
      \n  task => 'classification', \n  relation_name => 'pgml.breast_cancer', \n
      \ y_column_name => 'malignant'\n);"
    rendering: null
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 71
  fields:
    notebook: 2
    cell_type: 1
    contents: "\U0001F3C1 Congratulations \U0001F3C1\n---------------------\n\nYou've
      just created a machine learning model, tested it's accuracy, and deployed it
      to production. PostgresML orchestrated a bunch of the traditional ML drudgery
      in that couple of seconds to make it as simple as possible for you to get value.
      We'll organize our work on this task under the project name \"Breast Cancer
      Detection\", which you can now see it in your [list of projects](../../projects/).
      You can see that the first model uses the default linear algorithm, and that
      it achieves an [F1 score](https://en.wikipedia.org/wiki/F-score) in the mid
      90's, which is pretty good. A score of 1.0 is perfect, and 0.5 would be as good
      as random guessing. The better the F1 score, the better the algorithm can perform
      on this dataset. \n\nWe can now use this model to make some predictions in real
      time, using the training data as input to the `pgml.predict` function."
    rendering: "<article class=\"markdown-body\"><h2>\U0001F3C1 Congratulations \U0001F3C1</h2>\n<p>You've
      just created a machine learning model, tested it's accuracy, and deployed it
      to production. PostgresML orchestrated a bunch of the traditional ML drudgery
      in that couple of seconds to make it as simple as possible for you to get value.
      We'll organize our work on this task under the project name \"Breast Cancer
      Detection\", which you can now see it in your <a href=\"../../projects/\">list
      of projects</a>. You can see that the first model uses the default linear algorithm,
      and that it achieves an <a href=\"https://en.wikipedia.org/wiki/F-score\">F1
      score</a> in the mid 90's, which is pretty good. A score of 1.0 is perfect,
      and 0.5 would be as good as random guessing. The better the F1 score, the better
      the algorithm can perform on this dataset. </p>\n<p>We can now use this model
      to make some predictions in real time, using the training data as input to the
      <code>pgml.predict</code> function.</p></article>"
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 72
  fields:
    notebook: 2
    cell_type: 3
    contents: "SELECT malignant, pgml.predict(\n    'Breast Cancer Detection', \n
      \   ARRAY[\n        \"mean radius\", \n        \"mean texture\", \n        \"mean
      perimeter\", \n        \"mean area\",\n        \"mean smoothness\",\n        \"mean
      compactness\",\n        \"mean concavity\",\n        \"mean concave points\",\n
      \       \"mean symmetry\",\n        \"mean fractal dimension\",\n        \"radius
      error\",\n        \"texture error\",\n        \"perimeter error\",\n        \"area
      error\",\n        \"smoothness error\",\n        \"compactness error\",\n        \"concavity
      error\",\n        \"concave points error\",\n        \"symmetry error\",\n        \"fractal
      dimension error\",\n        \"worst radius\",\n        \"worst texture\",\n
      \       \"worst perimeter\",\n        \"worst area\",\n        \"worst smoothness\",\n
      \       \"worst compactness\",\n        \"worst concavity\",\n        \"worst
      concave points\",\n        \"worst symmetry\",\n        \"worst fractal dimension\"\n
      \   ]\n) AS prediction\nFROM pgml.breast_cancer\nORDER BY random()\nLIMIT 10;"
    rendering: null
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 73
  fields:
    notebook: 2
    cell_type: 1
    contents: "You can see the model is pretty good at predicting `0` for non malignant
      samples, and `1` for malignant samples. This isn't a great test though, because
      we're using the same data we trained with. We could have just looked up the
      data in the database table if this is all we wanted to do. The point of training
      a machine learning model is to generalize these statistics to data we've never
      seen before. What do you think this model would predict if all the input features
      happened to be 0 or 1? How does that compare to what it's seen before? \n\nIt's
      easy to test the model and see by providing new sample data in real time. There
      are lots of ways we could feed new data to a model in Postgres. We could write
      new samples to a table just like our training data, or we could pass parameters
      directly into a query without recording anything in the database at all. Postgres
      gives us a lot of ways to get data in and out at run time. We'll demonstrate
      with a `VALUES` example for batch prediction."
    rendering: '<article class="markdown-body"><p>You can see the model is pretty
      good at predicting <code>0</code> for non malignant samples, and <code>1</code>
      for malignant samples. This isn''t a great test though, because we''re using
      the same data we trained with. We could have just looked up the data in the
      database table if this is all we wanted to do. The point of training a machine
      learning model is to generalize these statistics to data we''ve never seen
      before. What do you think this model would predict if all the input features
      happened to be 0 or 1? How does that compare to what it''s seen before? </p>

      <p>It''s easy to test the model and see by providing new sample data in real
      time. There are lots of ways we could feed new data to a model in Postgres.
      We could write new samples to a table just like our training data, or we could
      pass parameters directly into a query without recording anything in the database
      at all. Postgres gives us a lot of ways to get data in and out at run time.
      We''ll demonstrate with a <code>VALUES</code> example for batch prediction.</p></article>'
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 74
  fields:
    notebook: 2
    cell_type: 3
    contents: "SELECT sample_name, pgml.predict(\n    'Breast Cancer Detection', \n
      \   ARRAY[\n        \"mean radius\", \n        \"mean texture\", \n        \"mean
      perimeter\", \n        \"mean area\",\n        \"mean smoothness\",\n        \"mean
      compactness\",\n        \"mean concavity\",\n        \"mean concave points\",\n
      \       \"mean symmetry\",\n        \"mean fractal dimension\",\n        \"radius
      error\",\n        \"texture error\",\n        \"perimeter error\",\n        \"area
      error\",\n        \"smoothness error\",\n        \"compactness error\",\n        \"concavity
      error\",\n        \"concave points error\",\n        \"symmetry error\",\n        \"fractal
      dimension error\",\n        \"worst radius\",\n        \"worst texture\",\n
      \       \"worst perimeter\",\n        \"worst area\",\n        \"worst smoothness\",\n
      \       \"worst compactness\",\n        \"worst concavity\",\n        \"worst
      concave points\",\n        \"worst symmetry\",\n        \"worst fractal dimension\"\n
      \   ]\n) AS prediction\nFROM (\n  VALUES \n  \t('all_zeroes',0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0),\n
      \ \t('all_ones',  1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)\n)
      \n  AS t (\n    \"sample_name\",\n    \"mean radius\", \n    \"mean texture\",
      \n    \"mean perimeter\", \n    \"mean area\",\n    \"mean smoothness\",\n    \"mean
      compactness\",\n    \"mean concavity\",\n    \"mean concave points\",\n    \"mean
      symmetry\",\n    \"mean fractal dimension\",\n    \"radius error\",\n    \"texture
      error\",\n    \"perimeter error\",\n    \"area error\",\n    \"smoothness error\",\n
      \   \"compactness error\",\n    \"concavity error\",\n    \"concave points error\",\n
      \   \"symmetry error\",\n    \"fractal dimension error\",\n    \"worst radius\",\n
      \   \"worst texture\",\n    \"worst perimeter\",\n    \"worst area\",\n    \"worst
      smoothness\",\n    \"worst compactness\",\n    \"worst concavity\",\n    \"worst
      concave points\",\n    \"worst symmetry\",\n    \"worst fractal dimension\"\n
      \ );"
    rendering: null
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 75
  fields:
    notebook: 2
    cell_type: 1
    contents: 'Even though the inputs are not data we''ve ever seen before, the model
      is telling us both of these new samples are likely to be benign based on their
      statistical correlations to the training samples we had labeled. As we collect
      new data samples, we could potentially use this model for multiple purposes,
      like screening the samples before doing further more expensive or invasive analysis.


      To demonstrate a more concise call that omits all the feature names (careful
      to get the order right):'
    rendering: '<article class="markdown-body"><p>Even though the inputs are not data
      we''ve ever seen before, the model is telling us both of these new samples are
      likely to be benign based on their statistical correlations to the training samples
      we had labeled. As we collect new data samples, we could potentially use this
      model for multiple purposes, like screening the samples before doing further
      more expensive or invasive analysis.</p>

      <p>To demonstrate a more concise call that omits all the feature names (careful
      to get the order right):</p></article>'
    execution_time: null
    cell_number: 11
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 76
  fields:
    notebook: 2
    cell_type: 3
    contents: "SELECT pgml.predict(\n    'Breast Cancer Detection', \n    ARRAY[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,100000]\n)"
    rendering: null
    execution_time: null
    cell_number: 12
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 77
  fields:
    notebook: 2
    cell_type: 1
    contents: 'Ah hah! We put a really big number into the last feature (worst fractal
      dimension), and got the model to give us a `True` prediction, indicating that
      large values there correlate with a malignant sample all else being equal using
      our default linear algorithm. There are lots of ways we can probe the model
      with test data, but before we spend too much time on this one, it might be informative
      to try other algorithms.


      PostgresML makes it easy to reuse your training data with many of the best algorithms
      available. Why not try them all?'
    rendering: '<article class="markdown-body"><p>Ah hah! We put a really big number
      into the last feature (worst fractal dimension), and got the model to give us
      a <code>True</code> prediction, indicating that large values there correlate
      with a malignant sample all else being equal using our default linear algorithm.
      There are lots of ways we can probe the model with test data, but before we
      spend too much time on this one, it might be informative to try other algorithms.</p>

      <p>PostgresML makes it easy to reuse your training data with many of the best
      algorithms available. Why not try them all?</p></article>'
    execution_time: null
    cell_number: 13
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 78
  fields:
    notebook: 2
    cell_type: 3
    contents: '--

      -- After a project has been trained, omitted parameters will be reused from
      previous training runs

      -- In these examples we''ll reuse the training data snapshots from the initial
      call.

      --


      -- Linear Models

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''ridge'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''stochastic_gradient_descent'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''perceptron'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''passive_aggressive'');


      -- Support Vector Machines

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''svm'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''nu_svm'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''linear_svm'');


      -- Ensembles

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''ada_boost'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''bagging'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''extra_trees'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''gradient_boosting_trees'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''random_forest'',
      hyperparams => ''{"n_estimators": 10}'');


      -- Gradient Boosting

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''xgboost'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''xgboost_random_forest'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Breast Cancer Detection'', algorithm => ''lightgbm'',
      hyperparams => ''{"n_estimators": 1}'');'
    rendering: null
    execution_time: null
    cell_number: 14
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 79
  fields:
    notebook: 2
    cell_type: 1
    contents: "Turns out, computers are pretty fast these days, even with state of
      the art algorithms running on a free tier computation resources. \U0001F60A
      \n\nYou can pop over to the [projects](../../projects/) tab for a visualization
      of the performance of all these algorithms on this dataset, or you can check
      out the artifacts directly in the database."
    rendering: "<article class=\"markdown-body\"><p>Turns out, computers are pretty
      fast these days, even with state of the art algorithms running on a free tier
      computation resources. \U0001F60A </p>\n<p>You can pop over to the <a href=\"../../projects/\">projects</a>
      tab for a visualization of the performance of all these algorithms on this dataset,
      or you can check out the artifacts directly in the database.</p></article>"
    execution_time: null
    cell_number: 15
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 80
  fields:
    notebook: 2
    cell_type: 3
    contents: "SELECT \n  projects.name,\n  models.algorithm_name,\n  round((models.metrics->>'f1')::numeric,
      4) AS f1_score,\n  round((models.metrics->>'precision')::numeric, 4) AS precision,\n
      \ round((models.metrics->>'recall')::numeric, 4) AS recall\nFROM pgml.models\nJOIN
      pgml.projects on projects.id = models.project_id\n  AND projects.name = 'Breast
      Cancer Detection'\nORDER BY models.metrics->>'f1' DESC LIMIT 5;"
    rendering: null
    execution_time: null
    cell_number: 16
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 81
  fields:
    notebook: 2
    cell_type: 1
    contents: Tree based algorithms like `random_forest`, `xgboost` and `lightgbm`
      do well on tabular datasets and frequently lead the pack with A+ level performance
      as measured by the `f1_score`. They are generally sensitive to small changes
      in the inputs, but also robust to outliers. They are also relatively fast algorithms
      that can perform predictions in sub millisecond times, meaning most of the cost
      of inference is in fetching the data they require as inputs. When your inputs
      are already in the database with the model, that time is as fast as possible!
    rendering: <article class="markdown-body"><p>Tree based algorithms like <code>random_forest</code>,
      <code>xgboost</code> and <code>lightgbm</code> do well on tabular datasets and
      frequently lead the pack with A+ level performance as measured by the <code>f1_score</code>.
      They are generally sensitive to small changes in the inputs, but also robust
      to outliers. They are also relatively fast algorithms that can perform predictions
      in sub millisecond times, meaning most of the cost of inference is in fetching
      the data they require as inputs. When your inputs are already in the database
      with the model, that time is as fast as possible!</p></article>
    execution_time: null
    cell_number: 17
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 82
  fields:
    notebook: 4
    cell_type: 1
    contents: 'So far we''ve focussed on Classification tasks which divide the world
      into discrete groups. Sometimes we need to take a more nuanced view when issues
      are not black and white. Sometimes there are no hard boundaries between options,
      or sometimes one sort of classification error might be much more painful than
      another. There are many algorithms that can produce a raw score rather than
      a discrete class for us. These are "Regression" tasks instead of "Classification".


      For this example, we''ll look at several medical indicators that correlate with
      the progression of diabetes one year later. Let''s load up the data and take
      a look'
    rendering: '<article class="markdown-body"><p>So far we''ve focussed on Classification
      tasks which divide the world into discrete groups. Sometimes we need to take
      a more nuanced view when issues are not black and white. Sometimes there are
      no hard boundaries between options, or sometimes one sort of classification
      error might be much more painful than another. There are many algorithms that
      can produce a raw score rather than a discrete class for us. These are "Regression"
      tasks instead of "Classification".</p>

      <p>For this example, we''ll look at several medical indicators that correlate
      with the progression of diabetes one year later. Let''s load up the data and
      take a look</p></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 83
  fields:
    notebook: 4
    cell_type: 3
    contents: SELECT pgml.load_dataset('diabetes');
    rendering: null
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 84
  fields:
    notebook: 3
    cell_type: 1
    contents: "Image classification is a great application of machine learning. In
      this tutorial we'll examine a classic version of this problem, recognizing hand
      written digits to automatically parse zip codes out of addresses. For machine
      learning purposes, we decompose images into their uncompressed pixel values
      as 2D arrays for gray scale images, or 3D arrays for color images. \n\nConvolutional
      Neural Nets and other forms of deep learning, leverage the 2D and 3D adjacency
      of the pixels to get breakthrough state of the art results on difficult image
      classification tasks over thousands of categories, and also for image labeling.
      Postgres has native support for multi dimensional `ARRAY` datatypes, that PostgresML
      can treat accordingly.\n\nLet's load the dataset to start:"
    rendering: '<article class="markdown-body"><p>Image classification is a great
      application of machine learning. In this tutorial we''ll examine a classic version
      of this problem, recognizing hand written digits to automatically parse zip
      codes out of addresses. For machine learning purposes, we decompose images into
      their uncompressed pixel values as 2D arrays for gray scale images, or 3D arrays
      for color images. </p>

      <p>Convolutional Neural Nets and other forms of deep learning, leverage the
      2D and 3D adjacency of the pixels to get breakthrough state of the art results
      on difficult image classification tasks over thousands of categories, and also
      for image labeling. Postgres has native support for multi dimensional <code>ARRAY</code>
      datatypes, that PostgresML can treat accordingly.</p>

      <p>Let''s load the dataset to start:</p></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 85
  fields:
    notebook: 5
    cell_type: 1
    contents: "PostgresML integrates [\U0001F917 Hugging Face Transformers](https://huggingface.co/transformers)
      to bring state-of-the-art models into the data layer. There are tens of thousands
      of pre-trained models with pipelines to turn raw inputs into useful results.
      Many state of the art deep learning architectures have been published and made
      available for download. You will want to browse all the [models](https://huggingface.co/models)
      available to find the perfect solution for your [dataset](https://huggingface.co/dataset)
      and [task](https://huggingface.co/tasks).\n\nWe'll demonstrate some of the tasks
      that are immediately available to users of your database upon installation.\n\n###
      ⚠️ Warning ⚠️\nThese examples take a fair bit of compute. The deep learning
      models themselves can be are several gigabytes, so they may run out of memory
      if you are accessing this notebook on the free tier of our cloud service. If
      you're not using a GPU to accellerate inference, you make expect them to take
      10-20 seconds to execute.\n\n### Examples\nAll of the tasks and models demonstrated
      here can be customized by passing additional arguments to the `Pipeline` initializer
      or call. You'll find additional links to documentation in the examples below.
      \n\nThe Hugging Face [`Pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines)
      API is exposed in Postgres via:\n\n```\npgml.transform(\n    task TEXT OR JSONB,
      \     -- task name or full pipeline initializer arguments\n    call JSONB,              --
      additional call arguments alongside the inputs\n    inputs TEXT[] OR BYTEA[]
      -- inputs for inference\n)\n```\n\nThis is roughly equivalent to the following
      Python:\n\n```\nimport transformers\n\ndef transform(task, call, inputs):\n
      \   return transformers.pipeline(**task)(inputs, **call)\n```"
    rendering: "<article class=\"markdown-body\"><p>PostgresML integrates <a href=\"https://huggingface.co/transformers\">\U0001F917
      Hugging Face Transformers</a> to bring state-of-the-art models into the data
      layer. There are tens of thousands of pre-trained models with pipelines to turn
      raw inputs into useful results. Many state of the art deep learning architectures
      have been published and made available for download. You will want to browse
      all the <a href=\"https://huggingface.co/models\">models</a> available to find
      the perfect solution for your <a href=\"https://huggingface.co/dataset\">dataset</a>
      and <a href=\"https://huggingface.co/tasks\">task</a>.</p>\n<p>We'll demonstrate
      some of the tasks that are immediately available to users of your database upon
      installation.</p>\n<h3>⚠️ Warning ⚠️</h3>\n<p>These examples take a fair bit
      of compute. The deep learning models themselves can be are several gigabytes,
      so they may run out of memory if you are accessing this notebook on the free
      tier of our cloud service. If you're not using a GPU to accellerate inference,
      you make expect them to take 10-20 seconds to execute.</p>\n<h3>Examples</h3>\n<p>All
      of the tasks and models demonstrated here can be customized by passing additional
      arguments to the <code>Pipeline</code> initializer or call. You'll find additional
      links to documentation in the examples below. </p>\n<p>The Hugging Face <a href=\"https://huggingface.co/docs/transformers/main_classes/pipelines\"><code>Pipeline</code></a>
      API is exposed in Postgres via:</p>\n<pre><code>pgml.transform(\n    task TEXT
      OR JSONB,      -- task name or full pipeline initializer arguments\n    call
      JSONB,              -- additional call arguments alongside the inputs\n    inputs
      TEXT[] OR BYTEA[] -- inputs for inference\n)\n</code></pre>\n<p>This is roughly
      equivalent to the following Python:</p>\n<pre><code>import transformers\n\ndef
      transform(task, call, inputs):\n    return transformers.pipeline(**task)(inputs,
      **call)\n</code></pre></article>"
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 86
  fields:
    notebook: 6
    cell_type: 1
    contents: 'PostgresML adds [native vector operations](https://github.com/postgresml/postgresml/tree/master/pgml-extension/sql/install/vectors.sql)
      that can be used in SQL queries. Vector operations are particularly useful for
      dealing with embeddings that have been generated from other machine learning
      algorithms and can provide functions like nearest neighbor calculations using
      the distance functions.


      Emeddings can be a relatively efficient mechanism to leverage the power deep
      learning, without the runtime inference costs. These functions are relatively
      fast and the more expensive distance functions can compute ~100k per second
      for a memory resident dataset on modern hardware.


      The PostgreSQL planner will also [automatically parallelize](https://www.postgresql.org/docs/current/parallel-query.html)
      evalualtion on larger datasets, as configured to take advantage of multiple
      CPU cores when available.'
    rendering: '<article class="markdown-body"><p>PostgresML adds <a href="https://github.com/postgresml/postgresml/tree/master/pgml-extension/sql/install/vectors.sql">native
      vector operations</a> that can be used in SQL queries. Vector operations are
      particularly useful for dealing with embeddings that have been generated from
      other machine learning algorithms and can provide functions like nearest neighbor
      calculations using the distance functions.</p>

      <p>Emeddings can be a relatively efficient mechanism to leverage the power deep
      learning, without the runtime inference costs. These functions are relatively
      fast and the more expensive distance functions can compute ~100k per second
      for a memory resident dataset on modern hardware.</p>

      <p>The PostgreSQL planner will also <a href="https://www.postgresql.org/docs/current/parallel-query.html">automatically
      parallelize</a> evalualtion on larger datasets, as configured to take advantage
      of multiple CPU cores when available.</p></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 87
  fields:
    notebook: 7
    cell_type: 1
    contents: "Models are automatically deployed if their key metric (R2 for regression,
      F1 for classification) is improved over the currently deployed version during
      training. If you want to manage deploys manually, you can always change which
      model is currently responsible for making predictions.\n\n```\npgml.deploy(\n
      \   project_name TEXT,                  -- Human-friendly project name\n    strategy
      TEXT DEFAULT 'best_score', -- 'rollback', 'best_score', or 'most_recent'\n    algorithm_name
      TEXT DEFAULT NULL    -- filter candidates to a particular algorithm, NULL =
      all qualify\n)\n```\n\nThe default behavior allows any algorithm to qualify,
      but deployment candidates can be further restricted to a specific algorithm
      by passing the `algorithm_name`.\n\n## Strategies\nThere are 3 different deployment
      strategies available\n\nstrategy | description\n--- | ---\nmost_recent | The
      most recently trained model for this project\nbest_score | The model that achieved
      the best key metric score\nrollback | The model that was previously deployed
      for this project"
    rendering: "<article class=\"markdown-body\"><p>Models are automatically deployed
      if their key metric (R2 for regression, F1 for classification) is improved over
      the currently deployed version during training. If you want to manage deploys
      manually, you can always change which model is currently responsible for making
      predictions.</p>\n<pre><code>pgml.deploy(\n    project_name TEXT,                  --
      Human-friendly project name\n    strategy TEXT DEFAULT 'best_score', -- 'rollback',
      'best_score', or 'most_recent'\n    algorithm_name TEXT DEFAULT NULL    -- filter
      candidates to a particular algorithm, NULL = all qualify\n)\n</code></pre>\n<p>The
      default behavior allows any algorithm to qualify, but deployment candidates
      can be further restricted to a specific algorithm by passing the <code>algorithm_name</code>.</p>\n<h2>Strategies</h2>\n<p>There
      are 3 different deployment strategies available</p>\n<table>\n<thead>\n<tr>\n<th>strategy</th>\n<th>description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>most_recent</td>\n<td>The
      most recently trained model for this project</td>\n</tr>\n<tr>\n<td>best_score</td>\n<td>The
      model that achieved the best key metric score</td>\n</tr>\n<tr>\n<td>rollback</td>\n<td>The
      model that was previously deployed for this project</td>\n</tr>\n</tbody>\n</table></article>"
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 88
  fields:
    notebook: 7
    cell_type: 1
    contents: ''
    rendering: <article class="markdown-body"></article>
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: 2022-08-22 22:09:15.475779+00:00
- model: app.notebookcell
  pk: 89
  fields:
    notebook: 8
    cell_type: 1
    contents: PostgresML stores all artifacts from training in the database under
      the `pgml` schema. You can manually inspect these tables to further understand
      the inner workings, or to generate additional reporting and analytics across
      your models.
    rendering: <article class="markdown-body"><p>PostgresML stores all artifacts from
      training in the database under the <code>pgml</code> schema. You can manually
      inspect these tables to further understand the inner workings, or to generate
      additional reporting and analytics across your models.</p></article>
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 90
  fields:
    notebook: 3
    cell_type: 3
    contents: SELECT pgml.load_dataset('digits');
    rendering: null
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 91
  fields:
    notebook: 5
    cell_type: 1
    contents: '### Translation

      There are thousands of different pre-trained translation models between language
      pairs. They generally take a single input string in the "from" language, and
      translate it into the "to" language as a result of the call. PostgresML transformations
      provide a batch interface where you can pass an array of `TEXT` to process in
      a single call for efficiency. Not all language pairs have a default task name
      like this example of English to French. In those cases, you''ll need to specify
      [the desired model](https://huggingface.co/models?pipeline_tag=translation)
      by name. Because this is a batch call with 2 inputs, we''ll get 2 outputs in
      the JSONB.


      See [translation documentation](https://huggingface.co/docs/transformers/tasks/translation)
      for more options.


      For a translation from English to French with the default pre-trained model:'
    rendering: '<article class="markdown-body"><h3>Translation</h3>

      <p>There are thousands of different pre-trained translation models between language
      pairs. They generally take a single input string in the "from" language, and
      translate it into the "to" language as a result of the call. PostgresML transformations
      provide a batch interface where you can pass an array of <code>TEXT</code> to
      process in a single call for efficiency. Not all language pairs have a default
      task name like this example of English to French. In those cases, you''ll need
      to specify <a href="https://huggingface.co/models?pipeline_tag=translation">the
      desired model</a> by name. Because this is a batch call with 2 inputs, we''ll
      get 2 outputs in the JSONB.</p>

      <p>See <a href="https://huggingface.co/docs/transformers/tasks/translation">translation
      documentation</a> for more options.</p>

      <p>For a translation from English to French with the default pre-trained model:</p></article>'
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 92
  fields:
    notebook: 5
    cell_type: 3
    contents: "SELECT pgml.transform(\n        'translation_en_to_fr',\n        inputs
      => ARRAY[\n            'Welcome to the future!',\n            'Where have you
      been all this time?'\n        ]\n    ) AS french;"
    rendering: null
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 93
  fields:
    notebook: 5
    cell_type: 1
    contents: '### Sentiment Analysis

      Sentiment analysis is one use of `text-classification`, but there are [many
      others](https://huggingface.co/tasks/text-classification). This model returns
      both a label classification `["POSITIVE", "NEUTRAL", "NEGATIVE"]`, as well as
      the score where 0.0 is perfectly negative, and 1.0 is perfectly positive. This
      example demonstrates specifying the `model` to be used rather than the task.
      The [`roberta-large-mnli`](https://huggingface.co/roberta-large-mnli) model
      specifies the task of `sentiment-analysis` in it''s default configuration, so
      we may omit it from the parameters. Because this is a batch call with 2 inputs,
      we''ll get 2 outputs in the JSONB.


      See [text classification documentation](https://huggingface.co/tasks/text-classification)
      for more options and potential use cases beyond sentiment analysis. You''ll
      notice the outputs are not great in this example. RoBERTa is a breakthrough
      model, that demonstrated just how important each particular hyperparameter is
      for the task and particular dataset regardless of how large your model is. We''ll
      show how to [fine tune](/user_guides/transformers/fine_tuning/) models on your
      data in the next step.'
    rendering: '<article class="markdown-body"><h3>Sentiment Analysis</h3>

      <p>Sentiment analysis is one use of <code>text-classification</code>, but there
      are <a href="https://huggingface.co/tasks/text-classification">many others</a>.
      This model returns both a label classification <code>["POSITIVE", "NEUTRAL",
      "NEGATIVE"]</code>, as well as the score where 0.0 is perfectly negative, and
      1.0 is perfectly positive. This example demonstrates specifying the <code>model</code>
      to be used rather than the task. The <a href="https://huggingface.co/roberta-large-mnli"><code>roberta-large-mnli</code></a>
      model specifies the task of <code>sentiment-analysis</code> in it''s default
      configuration, so we may omit it from the parameters. Because this is a batch
      call with 2 inputs, we''ll get 2 outputs in the JSONB.</p>

      <p>See <a href="https://huggingface.co/tasks/text-classification">text classification
      documentation</a> for more options and potential use cases beyond sentiment
      analysis. You''ll notice the outputs are not great in this example. RoBERTa
      is a breakthrough model, that demonstrated just how important each particular
      hyperparameter is for the task and particular dataset regardless of how large
      your model is. We''ll show how to <a href="/user_guides/transformers/fine_tuning/">fine
      tune</a> models on your data in the next step.</p></article>'
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 94
  fields:
    notebook: 5
    cell_type: 3
    contents: "SELECT pgml.transform(\n        '{\"model\": \"roberta-large-mnli\"}'::JSONB,\n
      \       inputs => ARRAY[\n            'I love how amazingly simple ML has become!',
      \n            'I hate doing mundane and thankless tasks. ☹️'\n        ]\n    )
      AS positivity;"
    rendering: null
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 95
  fields:
    notebook: 6
    cell_type: 1
    contents: '### Elementwise arithmetic w/ constants'
    rendering: <article class="markdown-body"><h3>Elementwise arithmetic w/ constants</h3></article>
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 96
  fields:
    notebook: 6
    cell_type: 3
    contents: ''
    rendering: null
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: 2022-08-22 22:14:31.875531+00:00
- model: app.notebookcell
  pk: 97
  fields:
    notebook: 7
    cell_type: 3
    contents: '-- deploy the "best" model for prediction use

      SELECT * FROM pgml.deploy(''Handwritten Digits'', ''best_score'');'
    rendering: null
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 98
  fields:
    notebook: 8
    cell_type: 1
    contents: '## Models


      Models are an artifact of calls to `pgml.train`.'
    rendering: '<article class="markdown-body"><h2>Models</h2>

      <p>Models are an artifact of calls to <code>pgml.train</code>.</p></article>'
    execution_time: null
    cell_number: 2
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 99
  fields:
    notebook: 8
    cell_type: 3
    contents: select * from pgml.models limit 10;
    rendering: null
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 100
  fields:
    notebook: 8
    cell_type: 1
    contents: '## Projects


      Projects are an artifact of calls to `pgml.train`.'
    rendering: '<article class="markdown-body"><h2>Projects</h2>

      <p>Projects are an artifact of calls to <code>pgml.train</code>.</p></article>'
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 101
  fields:
    notebook: 8
    cell_type: 3
    contents: select * from pgml.projects limit 10;
    rendering: null
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 102
  fields:
    notebook: 8
    cell_type: 1
    contents: '## Snapshots


      Snapshots are an artifcat of calls to `pgml.train` that include a specific `relation_name`
      parameter. A full copy of all data in the relation at training time will be
      saved in a new table named `pgml.snapshot_{id}`. You can retrieve the original
      training data set by inspecting tables like `pgml.snapshot_1`.'
    rendering: '<article class="markdown-body"><h2>Snapshots</h2>

      <p>Snapshots are an artifcat of calls to <code>pgml.train</code> that include
      a specific <code>relation_name</code> parameter. A full copy of all data in
      the relation at training time will be saved in a new table named <code>pgml.snapshot_{id}</code>.
      You can retrieve the original training data set by inspecting tables like <code>pgml.snapshot_1</code>.</p></article>'
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 103
  fields:
    notebook: 3
    cell_type: 1
    contents: We can view a sample of the data with a simple `SELECT`
    rendering: <article class="markdown-body"><p>We can view a sample of the data
      with a simple <code>SELECT</code></p></article>
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 104
  fields:
    notebook: 3
    cell_type: 3
    contents: SELECT target, image FROM pgml.digits LIMIT 10;
    rendering: null
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 105
  fields:
    notebook: 3
    cell_type: 1
    contents: The images are 8x8 gray scale arrays with gray values from 0 (white)
      to 16 (black) pixels. These images have been fairly heavily processed to center
      and crop each one, and the represented digit is labeled in the `target` column.
      By now you should start to have an idea what comes next in this tutorial. We've
      got data, so we train a model with a simple call to PostgresML.
    rendering: <article class="markdown-body"><p>The images are 8x8 gray scale arrays
      with gray values from 0 (white) to 16 (black) pixels. These images have been
      fairly heavily processed to center and crop each one, and the represented digit
      is labeled in the <code>target</code> column. By now you should start to have
      an idea what comes next in this tutorial. We've got data, so we train a model
      with a simple call to PostgresML.</p></article>
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 106
  fields:
    notebook: 3
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Handwritten Digits',
      \n  task => 'classification', \n  relation_name => 'pgml.digits', \n  y_column_name
      => 'target'\n);"
    rendering: null
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 107
  fields:
    notebook: 3
    cell_type: 1
    contents: We can view some of the predictions of the model on the training data.
    rendering: <article class="markdown-body"><p>We can view some of the predictions
      of the model on the training data.</p></article>
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 108
  fields:
    notebook: 3
    cell_type: 3
    contents: "SELECT target, pgml.predict('Handwritten Digits', image) AS prediction\nFROM
      pgml.digits \nLIMIT 10;"
    rendering: null
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 109
  fields:
    notebook: 3
    cell_type: 1
    contents: Hah! Even the default linear classification algorithm performs extremely
      well on such carefully engineered, but real world data. It's a  demonstration
      of how effective feature engineering and clean data can be even with relatively
      simple algorithms. Let's take a look at that models metrics.
    rendering: <article class="markdown-body"><p>Hah! Even the default linear classification
      algorithm performs extremely well on such carefully engineered, but real world
      data. It's a  demonstration of how effective feature engineering and clean data
      can be even with relatively simple algorithms. Let's take a look at that models
      metrics.</p></article>
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 110
  fields:
    notebook: 3
    cell_type: 3
    contents: "SELECT \n  projects.name,\n  models.algorithm_name,\n  round((models.metrics->>'f1')::numeric,
      4) AS f1_score,\n  round((models.metrics->>'precision')::numeric, 4) AS precision,\n
      \ round((models.metrics->>'recall')::numeric, 4) AS recall\nFROM pgml.models\nJOIN
      pgml.projects on projects.id = models.project_id\n  AND projects.name = 'Handwritten
      Digits'\nORDER BY models.created_at DESC LIMIT 5;"
    rendering: null
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 111
  fields:
    notebook: 3
    cell_type: 1
    contents: 'An F1 score in the mid nineties are grade A results, although there
      is room for improvement. We need to keep in mind the cost/benefit tradeoffs
      in the real world. If this algorithm is wrong about a digit 1 out of 20 times,
      it''ll give us the wrong zipe code on every 3rd piece of mail. It might be a
      lot more expensive to re-route 1/3rd of all mail to fix these mistakes than
      it is to hire human''s read and input every zip code manually, so even though
      the results are pretty good, they are not good enough to create real value.


      Luckily, we have the benefit of the last 40 years of some very smart people
      developing a bunch of different algorithms for learning that all have different
      tradeoffs strengths and weaknesses. You could go spend a few years getting a
      degree trying to understand how they all work, or we can just try them all since
      computers are cheaper and more plentiful than engineers.'
    rendering: '<article class="markdown-body"><p>An F1 score in the mid nineties
      are grade A results, although there is room for improvement. We need to keep
      in mind the cost/benefit tradeoffs in the real world. If this algorithm is wrong
      about a digit 1 out of 20 times, it''ll give us the wrong zipe code on every
      3rd piece of mail. It might be a lot more expensive to re-route 1/3rd of all
      mail to fix these mistakes than it is to hire human''s read and input every
      zip code manually, so even though the results are pretty good, they are not
      good enough to create real value.</p>

      <p>Luckily, we have the benefit of the last 40 years of some very smart people
      developing a bunch of different algorithms for learning that all have different
      tradeoffs strengths and weaknesses. You could go spend a few years getting a
      degree trying to understand how they all work, or we can just try them all since
      computers are cheaper and more plentiful than engineers.</p></article>'
    execution_time: null
    cell_number: 11
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 112
  fields:
    notebook: 3
    cell_type: 3
    contents: '--

      -- After a project has been trained, omitted parameters will be reused from
      previous training runs

      -- In these examples we''ll reuse the training data snapshots from the initial
      call.

      --


      -- linear models

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''ridge'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''stochastic_gradient_descent'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''perceptron'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''passive_aggressive'');


      -- support vector machines

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''svm'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''nu_svm'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''linear_svm'');


      -- ensembles

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''ada_boost'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''bagging'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''extra_trees'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''gradient_boosting_trees'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''random_forest'',
      hyperparams => ''{"n_estimators": 10}'');


      -- gradient boosting

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''xgboost'', hyperparams
      => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''xgboost_random_forest'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Handwritten Digits'', algorithm => ''lightgbm'',
      hyperparams => ''{"n_estimators": 1}'');'
    rendering: null
    execution_time: null
    cell_number: 12
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 113
  fields:
    notebook: 3
    cell_type: 1
    contents: In less than 10 seconds, we've thrown a barrage of algorithms at the
      problem and measured how they perform. Now let's take a look at the best one's
      metrics.
    rendering: <article class="markdown-body"><p>In less than 10 seconds, we've thrown
      a barrage of algorithms at the problem and measured how they perform. Now let's
      take a look at the best one's metrics.</p></article>
    execution_time: null
    cell_number: 13
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 114
  fields:
    notebook: 3
    cell_type: 3
    contents: "SELECT \n  projects.name,\n  models.algorithm_name,\n  round((models.metrics->>'f1')::numeric,
      4) AS f1_score,\n  round((models.metrics->>'precision')::numeric, 4) AS precision,\n
      \ round((models.metrics->>'recall')::numeric, 4) AS recall\nFROM pgml.models\nJOIN
      pgml.projects on projects.id = models.project_id\n  AND projects.name = 'Handwritten
      Digits'\nORDER BY models.metrics->>'f1' DESC LIMIT 5;"
    rendering: null
    execution_time: null
    cell_number: 14
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 115
  fields:
    notebook: 3
    cell_type: 1
    contents: "`svm` stands for Support Vector Machines. They do well on this particular
      problem, and can reach A+ F1 scores. Back in our real world performance evaluation
      where they are only wrong 1 out of 100 digits, or 1/14 zip codes, instead of
      our original 1/3rd wrong baseline model. In the real world this means that about
      7% of our mail would end up getting auto-routed to the wrong zip code. Is that
      good enough to start automating? Let's ask the Postmaster general... If he says
      not quite, there is one more thing to try before we break out deep learning.
      \n\nMany algorithm's have a few options we can tweak. These options are called
      hyperparameters. You can find the available ones for SVMs in the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).
      Then we can automatically search all the combinations of the hyperparams to
      see how to tweak the knobs. We don't actually have to have that degree just
      yet..."
    rendering: '<article class="markdown-body"><p><code>svm</code> stands for Support
      Vector Machines. They do well on this particular problem, and can reach A+ F1
      scores. Back in our real world performance evaluation where they are only wrong
      1 out of 100 digits, or 1/14 zip codes, instead of our original 1/3rd wrong
      baseline model. In the real world this means that about 7% of our mail would
      end up getting auto-routed to the wrong zip code. Is that good enough to start
      automating? Let''s ask the Postmaster general... If he says not quite, there
      is one more thing to try before we break out deep learning. </p>

      <p>Many algorithm''s have a few options we can tweak. These options are called
      hyperparameters. You can find the available ones for SVMs in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">docs</a>.
      Then we can automatically search all the combinations of the hyperparams to
      see how to tweak the knobs. We don''t actually have to have that degree just
      yet...</p></article>'
    execution_time: null
    cell_number: 15
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 116
  fields:
    notebook: 3
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n    'Handwritten Digits', \n    algorithm
      => 'svm', \n    hyperparams => '{\"random_state\": 0}',\n    search => 'grid',
      \n    search_params => '{\n        \"kernel\": [\"linear\", \"poly\", \"sigmoid\"],
      \n        \"shrinking\": [true, false]\n    }'\n);"
    rendering: null
    execution_time: null
    cell_number: 16
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 117
  fields:
    notebook: 3
    cell_type: 1
    contents: And then we can peak at the metrics directly with a bit more SQL.
    rendering: <article class="markdown-body"><p>And then we can peak at the metrics
      directly with a bit more SQL.</p></article>
    execution_time: null
    cell_number: 17
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 118
  fields:
    notebook: 3
    cell_type: 3
    contents: 'SELECT metrics

      FROM pgml.models

      ORDER BY created_at DESC

      LIMIT 1;'
    rendering: null
    execution_time: null
    cell_number: 18
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 119
  fields:
    notebook: 3
    cell_type: 1
    contents: It's a bit tough to parse the results of the search in pure SQL, so
      you can hope over to the [Projects](../../projects/) list to see a visualization.
    rendering: <article class="markdown-body"><p>It's a bit tough to parse the results
      of the search in pure SQL, so you can hope over to the <a href="../../projects/">Projects</a>
      list to see a visualization.</p></article>
    execution_time: null
    cell_number: 19
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 120
  fields:
    notebook: 4
    cell_type: 3
    contents: "SELECT * \nFROM pgml.diabetes \nLIMIT 10;"
    rendering: null
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 121
  fields:
    notebook: 4
    cell_type: 1
    contents: In this case, the `target` is a number that represents the severity
      of the disease progression one year later, with larger values indicating worse
      outcomes. Building a Regression model uses the same PostgresML API as Classification,
      just with a different task. You're going to start breezing through these tutorials
      faster and faster.
    rendering: <article class="markdown-body"><p>In this case, the <code>target</code>
      is a number that represents the severity of the disease progression one year
      later, with larger values indicating worse outcomes. Building a Regression model
      uses the same PostgresML API as Classification, just with a different task.
      You're going to start breezing through these tutorials faster and faster.</p></article>
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 122
  fields:
    notebook: 4
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n  project_name => 'Diabetes Progression',
      \n  task => 'regression', \n  relation_name => 'pgml.diabetes', \n  y_column_name
      => 'target'\n);"
    rendering: null
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 123
  fields:
    notebook: 4
    cell_type: 1
    contents: With our baseline model automatically deployed, we can sample some of
      the predictions
    rendering: <article class="markdown-body"><p>With our baseline model automatically
      deployed, we can sample some of the predictions</p></article>
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 124
  fields:
    notebook: 4
    cell_type: 3
    contents: "SELECT target, pgml.predict('Diabetes Progression', ARRAY[age, sex,
      bmi, bp, s1, s2, s3, s4, s5, s6]) AS prediction\nFROM pgml.diabetes \nLIMIT
      10;"
    rendering: null
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 125
  fields:
    notebook: 4
    cell_type: 1
    contents: To get an objective measure of just how far off every single prediction
      is from the target, we can look at the key metrics recorded during training.
    rendering: <article class="markdown-body"><p>To get an objective measure of just
      how far off every single prediction is from the target, we can look at the key
      metrics recorded during training.</p></article>
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 126
  fields:
    notebook: 4
    cell_type: 3
    contents: "SELECT \n  projects.name,\n  models.algorithm_name,\n  round((models.metrics->>'r2')::numeric,
      4) AS r2_score\nFROM pgml.models\nJOIN pgml.projects on projects.id = models.project_id\n
      \ AND projects.name = 'Diabetes Progression'\nORDER BY models.created_at DESC
      LIMIT 5;"
    rendering: null
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 127
  fields:
    notebook: 4
    cell_type: 1
    contents: I like to look at the R2 score, since it is fixed between 0 and 1 it
      can help us compare the performance of different algorithms on our data. Let's
      throw our bag of tricks at the problem and see what sticks.
    rendering: <article class="markdown-body"><p>I like to look at the R2 score, since
      it is fixed between 0 and 1 it can help us compare the performance of different
      algorithms on our data. Let's throw our bag of tricks at the problem and see
      what sticks.</p></article>
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 128
  fields:
    notebook: 4
    cell_type: 3
    contents: '-- linear models

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''ridge'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''lasso'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''elastic_net'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''least_angle'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''lasso_least_angle'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''orthoganl_matching_pursuit'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''bayesian_ridge'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''automatic_relevance_determination'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''stochastic_gradient_descent'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''passive_aggressive'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''ransac'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''theil_sen'',
      hyperparams => ''{"max_iter": 10, "max_subpopulation": 100}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''huber'');


      -- support vector machines

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''svm'', hyperparams
      => ''{"max_iter": 100}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''nu_svm'',
      hyperparams => ''{"max_iter": 10}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''linear_svm'',
      hyperparams => ''{"max_iter": 100}'');


      -- ensembles

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''ada_boost'',
      hyperparams => ''{"n_estimators": 5}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''bagging'',
      hyperparams => ''{"n_estimators": 5}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''extra_trees'',
      hyperparams => ''{"n_estimators": 5}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''gradient_boosting_trees'',
      hyperparams => ''{"n_estimators": 5}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''random_forest'',
      hyperparams => ''{"n_estimators": 5}'');


      -- gradient boosting

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''xgboost'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''xgboost_random_forest'',
      hyperparams => ''{"n_estimators": 10}'');

      SELECT * FROM pgml.train(''Diabetes Progression'', algorithm => ''lightgbm'',
      hyperparams => ''{"n_estimators": 1}'');'
    rendering: null
    execution_time: null
    cell_number: 11
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 129
  fields:
    notebook: 4
    cell_type: 1
    contents: It's that easy, and that fast, to test all the algorithm's in our toolkit
      to see what fares the best, and the best one has automatically been deployed.
      Once we've honed in on a few good candidate algorithms, we can check the docs
      for their hyperparams, and then do another brute force search across all combinations
      to find the best set.
    rendering: <article class="markdown-body"><p>It's that easy, and that fast, to
      test all the algorithm's in our toolkit to see what fares the best, and the
      best one has automatically been deployed. Once we've honed in on a few good
      candidate algorithms, we can check the docs for their hyperparams, and then
      do another brute force search across all combinations to find the best set.</p></article>
    execution_time: null
    cell_number: 12
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 130
  fields:
    notebook: 4
    cell_type: 3
    contents: "SELECT * FROM pgml.train(\n    'Diabetes Progression', \n    algorithm
      => 'xgboost', \n    search => 'grid', \n    search_params => '{\n        \"max_depth\":
      [1, 2], \n        \"n_estimators\": [20, 40],\n        \"learning_rate\": [0.1,
      0.2]\n    }'\n);"
    rendering: null
    execution_time: null
    cell_number: 13
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 131
  fields:
    notebook: 5
    cell_type: 1
    contents: '### Summarization

      Sometimes we need all the nuanced detail, but sometimes it''s nice to get to
      the point. Summarization can reduce a very long and complex document to a few
      sentences. One studied application is reducing legal bills passed by Congress
      into a plain english summary. Hollywood may also need some intelligence to reduce
      a full synopsis down to a pithy blurb for movies like Inception.


      See [summarization documentation](https://huggingface.co/tasks/summarization)
      for more options.'
    rendering: '<article class="markdown-body"><h3>Summarization</h3>

      <p>Sometimes we need all the nuanced detail, but sometimes it''s nice to get
      to the point. Summarization can reduce a very long and complex document to a
      few sentences. One studied application is reducing legal bills passed by Congress
      into a plain english summary. Hollywood may also need some intelligence to reduce
      a full synopsis down to a pithy blurb for movies like Inception.</p>

      <p>See <a href="https://huggingface.co/tasks/summarization">summarization documentation</a>
      for more options.</p></article>'
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 132
  fields:
    notebook: 5
    cell_type: 3
    contents: "SELECT pgml.transform(\n        'summarization',\n        inputs =>
      ARRAY['\n            Dominic Cobb is the foremost practitioner of the artistic
      science \n            of extraction, inserting oneself into a subject''s dreams
      to \n            obtain hidden information without the subject knowing, a concept
      \n            taught to him by his professor father-in-law, Dr. Stephen Miles.
      \n            Dom''s associates are Miles'' former students, who Dom requires
      \n            as he has given up being the dream architect for reasons he \n
      \           won''t disclose. Dom''s primary associate, Arthur, believes it \n
      \           has something to do with Dom''s deceased wife, Mal, who often \n
      \           figures prominently and violently in those dreams, or Dom''s want
      \n            to \"go home\" (get back to his own reality, which includes two
      \n            young children). Dom''s work is generally in corporate espionage.
      \n            As the subjects don''t want the information to get into the wrong
      \n            hands, the clients have zero tolerance for failure. Dom is also
      a \n            wanted man, as many of his past subjects have learned what Dom
      \n            has done to them. One of those subjects, Mr. Saito, offers Dom
      a \n            job he can''t refuse: to take the concept one step further into
      \n            inception, namely planting thoughts into the subject''s dreams
      \n            without them knowing. Inception can fundamentally alter that \n
      \           person as a being. Saito''s target is Robert Michael Fischer, the
      \n            heir to an energy business empire, which has the potential to
      \n            rule the world if continued on the current trajectory. Beyond
      the \n            complex logistics of the dream architecture of the case and
      some \n            unknowns concerning Fischer, the biggest obstacles in success
      for \n            the team become worrying about one aspect of inception which
      Cobb \n            fails to disclose to the other team members prior to the
      job, and \n            Cobb''s newest associate Ariadne''s belief that Cobb''s
      own \n            subconscious, especially as it relates to Mal, may be taking
      over \n            what happens in the dreams.\n        ']\n    ) AS result;"
    rendering: null
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 133
  fields:
    notebook: 5
    cell_type: 1
    contents: '### Question Answering

      Question Answering extracts an answer from a given context. Recent progress
      has enabled models to also specify if the answer is present in the context at
      all. If you were trying to build a general question answering system, you could
      first turn the question into a keyword search against Wikipedia articles, and
      then use a model to retrieve the correct answer from the top hit. Another application
      would provide automated support from a knowledge base, based on the customers
      question.


      See [question answering documentation](https://huggingface.co/tasks/question-answering)
      for more options.'
    rendering: '<article class="markdown-body"><h3>Question Answering</h3>

      <p>Question Answering extracts an answer from a given context. Recent progress
      has enabled models to also specify if the answer is present in the context at
      all. If you were trying to build a general question answering system, you could
      first turn the question into a keyword search against Wikipedia articles, and
      then use a model to retrieve the correct answer from the top hit. Another application
      would provide automated support from a knowledge base, based on the customers
      question.</p>

      <p>See <a href="https://huggingface.co/tasks/question-answering">question answering
      documentation</a> for more options.</p></article>'
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 134
  fields:
    notebook: 5
    cell_type: 3
    contents: "SELECT pgml.transform(\n        'question-answering',\n        inputs
      => ARRAY[\n            '{\n                \"question\": \"Am I dreaming?\",\n
      \               \"context\": \"I got a good nights sleep last night and started
      a simple tutorial over my cup of morning coffee. The capabilities seem unreal,
      compared to what I came to expect from the simple SQL standard I studied so
      long ago. The answer is staring me in the face, and I feel the uncanny call
      from beyond the screen to check the results.\"\n            }'\n        ]\n
      \   ) AS answer;"
    rendering: null
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 135
  fields:
    notebook: 5
    cell_type: 1
    contents: '### Text Generation

      If you need to expand on some thoughts, you can have AI complete your sentences
      for you:'
    rendering: '<article class="markdown-body"><h3>Text Generation</h3>

      <p>If you need to expand on some thoughts, you can have AI complete your sentences
      for you:</p></article>'
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 136
  fields:
    notebook: 5
    cell_type: 3
    contents: "SELECT pgml.transform(\n        'text-generation',\n        '{\"num_return_sequences\":
      2}',\n        ARRAY['Three Rings for the Elven-kings under the sky, Seven for
      the Dwarf-lords in their halls of stone']\n    ) AS result;"
    rendering: null
    execution_time: null
    cell_number: 11
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 137
  fields:
    notebook: 5
    cell_type: 1
    contents: '### More

      There are many different [tasks](https://huggingface.co/tasks) and tens of thousands
      of state-of-the-art [models](https://huggingface.co/models) available for you
      to explore. The possibilities are expanding every day. There can be amazing
      performance improvements in domain specific versions of these general tasks
      by fine tuning published models on your dataset. See the next section for [fine
      tuning](/user_guides/transformers/fine_tuning/) demonstrations.'
    rendering: '<article class="markdown-body"><h3>More</h3>

      <p>There are many different <a href="https://huggingface.co/tasks">tasks</a>
      and tens of thousands of state-of-the-art <a href="https://huggingface.co/models">models</a>
      available for you to explore. The possibilities are expanding every day. There
      can be amazing performance improvements in domain specific versions of these
      general tasks by fine tuning published models on your dataset. See the next
      section for <a href="/user_guides/transformers/fine_tuning/">fine tuning</a>
      demonstrations.</p></article>'
    execution_time: null
    cell_number: 12
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 138
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.add(ARRAY[1.0, 2.0, 3.0], 3);
    rendering: null
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 139
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.subtract(ARRAY[1.0, 2.0, 3.0], 3);
    rendering: null
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 140
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.multiply(ARRAY[1.0, 2.0, 3.0], 3);
    rendering: null
    execution_time: null
    cell_number: 5
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 141
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.divide(ARRAY[1.0, 2.0, 3.0], 100);
    rendering: null
    execution_time: null
    cell_number: 6
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 142
  fields:
    notebook: 6
    cell_type: 1
    contents: '### Pairwise arithmetic'
    rendering: <article class="markdown-body"><h3>Pairwise arithmetic</h3></article>
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 143
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.add(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 144
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.subtract(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 145
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.multiply(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 146
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.divide(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 11
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 147
  fields:
    notebook: 6
    cell_type: 1
    contents: '### Norms'
    rendering: <article class="markdown-body"><h3>Norms</h3></article>
    execution_time: null
    cell_number: 12
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 148
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.norm_l0(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 13
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 149
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.norm_l1(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 14
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 150
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.norm_l2(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 15
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 151
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.norm_max(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 16
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 152
  fields:
    notebook: 6
    cell_type: 1
    contents: '### Normalization'
    rendering: <article class="markdown-body"><h3>Normalization</h3></article>
    execution_time: null
    cell_number: 17
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 153
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.normalize_l1(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 18
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 154
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.normalize_l2(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 19
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 155
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.normalize_max(ARRAY[1.0, 2.0, 3.0]);
    rendering: null
    execution_time: null
    cell_number: 20
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 156
  fields:
    notebook: 6
    cell_type: 1
    contents: '### Comparisons'
    rendering: <article class="markdown-body"><h3>Comparisons</h3></article>
    execution_time: null
    cell_number: 21
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 157
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.distance_l1(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 22
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 158
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.distance_l2(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 23
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 159
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.dot_product(ARRAY[1.0, 2.0, 3.0], ARRAY[4.0, 5.0, 6.0]);
    rendering: null
    execution_time: null
    cell_number: 24
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 160
  fields:
    notebook: 6
    cell_type: 3
    contents: SELECT pgml.cosine_similarity(ARRAY[1.0, 2.0, 3.0], ARRAY[1.0, 2.0,
      3.0]);
    rendering: null
    execution_time: null
    cell_number: 25
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 161
  fields:
    notebook: 6
    cell_type: 1
    contents: '### Generating Random Embeddings


      We can populate a table of embeddings with 10,000 rows that have a 128 dimension
      embedding to demonstrate some vector functionality like nearest neighbor search.'
    rendering: '<article class="markdown-body"><h3>Generating Random Embeddings</h3>

      <p>We can populate a table of embeddings with 10,000 rows that have a 128 dimension
      embedding to demonstrate some vector functionality like nearest neighbor search.</p></article>'
    execution_time: null
    cell_number: 26
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 162
  fields:
    notebook: 6
    cell_type: 3
    contents: "CREATE TABLE embeddings AS\nSELECT id, ARRAY_AGG(rand) AS vector\nFROM
      (\n  SELECT row_number() over () % 10000 + 1 AS id, random()::REAL AS rand\n
      \ FROM generate_series(1, 1280000) AS t\n) series\nGROUP BY id\nORDER BY id;"
    rendering: null
    execution_time: null
    cell_number: 27
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 163
  fields:
    notebook: 6
    cell_type: 3
    contents: "-- Nearest neighbors to e1 using cosine similarity\nSELECT \n    e1.id,
      \n    e2.id,\n    pgml.cosine_similarity(e1.vector, e2.vector) AS distance\nFROM
      embeddings e1\nJOIN embeddings e2 ON 1=1\nWHERE e1.id = 1\nORDER BY distance
      DESC\nLIMIT 10;"
    rendering: null
    execution_time: null
    cell_number: 28
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 164
  fields:
    notebook: 7
    cell_type: 1
    contents: '## Rolling back to a specific algorithm

      Rolling back creates a new deploy for the model that was deployed before the
      current one. Multiple rollbacks in a row will effectively oscilate between the
      two most recently deployed models, making rollbacks a relatively safe operation.'
    rendering: '<article class="markdown-body"><h2>Rolling back to a specific algorithm</h2>

      <p>Rolling back creates a new deploy for the model that was deployed before
      the current one. Multiple rollbacks in a row will effectively oscilate between
      the two most recently deployed models, making rollbacks a relatively safe operation.</p></article>'
    execution_time: null
    cell_number: 3
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 165
  fields:
    notebook: 7
    cell_type: 3
    contents: SELECT * FROM pgml.deploy('Handwritten Digits', 'rollback', 'svm');
    rendering: null
    execution_time: null
    cell_number: 4
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 166
  fields:
    notebook: 8
    cell_type: 3
    contents: select * from pgml.snapshots limit 10;
    rendering: null
    execution_time: null
    cell_number: 7
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 167
  fields:
    notebook: 8
    cell_type: 3
    contents: select * from pgml.snapshot_1 limit 10;
    rendering: null
    execution_time: null
    cell_number: 8
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 168
  fields:
    notebook: 8
    cell_type: 1
    contents: '## Deployments


      Deployments happen automatically if a new project has a better key metric after
      training, or when triggered manually. You can view all deployments.'
    rendering: '<article class="markdown-body"><h2>Deployments</h2>

      <p>Deployments happen automatically if a new project has a better key metric
      after training, or when triggered manually. You can view all deployments.</p></article>'
    execution_time: null
    cell_number: 9
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 169
  fields:
    notebook: 8
    cell_type: 3
    contents: select * from pgml.deployments limit 10;
    rendering: null
    execution_time: null
    cell_number: 10
    version: 1
    deleted_at: null
- model: app.notebookcell
  pk: 170
  fields:
    notebook: 9
    cell_type: 1
    contents: "## Native Installation\n\nA PostgresML deployment consists of two different
      runtimes. The foundational runtime is a Python extension for Postgres ([pgml-extension](https://github.com/postgresml/postgresml/tree/master/pgml-extension/))
      that facilitates the machine learning lifecycle inside the database. Additionally,
      we provide a dashboard ([pgml-dashboard](https://github.com/postgresml/postgresml/tree/master/pgml-dashboard/))
      that can connect to your Postgres server and provide additional management functionality.
      It will also provide visibility into the models you build and data they use.\n\nCheck
      out our documentation for [installation instructions](https://postgresml.org/user_guides/setup/native_installation/)
      in your datacenter.\n\nWe'd also love to hear your feedback. \n\n- Email us
      at team@postgresml.org\n- Start a [discussion on github](https://github.com/postgresml/postgresml/discussions)"
    rendering: '<article class="markdown-body"><h2>Native Installation</h2>

      <p>A PostgresML deployment consists of two different runtimes. The foundational
      runtime is a Python extension for Postgres (<a href="https://github.com/postgresml/postgresml/tree/master/pgml-extension/">pgml-extension</a>)
      that facilitates the machine learning lifecycle inside the database. Additionally,
      we provide a dashboard (<a href="https://github.com/postgresml/postgresml/tree/master/pgml-dashboard/">pgml-dashboard</a>)
      that can connect to your Postgres server and provide additional management functionality.
      It will also provide visibility into the models you build and data they use.</p>

      <p>Check out our documentation for <a href="https://postgresml.org/user_guides/setup/native_installation/">installation
      instructions</a> in your datacenter.</p>

      <p>We''d also love to hear your feedback. </p>

      <ul>

      <li>Email us at team@postgresml.org</li>

      <li>Start a <a href="https://github.com/postgresml/postgresml/discussions">discussion
      on github</a></li>

      </ul></article>'
    execution_time: null
    cell_number: 1
    version: 1
    deleted_at: null
